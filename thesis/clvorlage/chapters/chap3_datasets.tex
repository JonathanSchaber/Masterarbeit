
\newchap{Data Sets}
\label{chap:5_dataset}

\section{Why create an own corpus?}

\section{Corpora}

\subsection{deISEAR}

As \cite{troiano2019crowdsourcing} write in their 

\subsection{MLQA\_V1}

\cite{lewis2019mlqa} compiled

\subsection{PAWS-X}

\cite{yang2019paws}

\subsection{SCARE}

\subsubsection{SCARE normal}

The Sentiment Corpus of App Reviews with Fine-grained Annotations in German \cite{sanger2016scare} is a hand-annotated corpus that asserts so sentiment to German mobile app reviews stemming from the Google Play Store.
Since there are many users of 
In contrast to other data sets, e.g. \citep{socher2013recursive, go2009twitter}, that attributes one sentiment label to a whole text (may it be a review, a tweet, etc.), \cite{sanger2016scare} annotated their data set on a lower textual level:
Not each review gets labelled for a certain polarity --- i.e. \textit{positive}, \textit{negative}, or \textit{neutral} --- but what the authors call \textit{aspects} and \textit{subjective (sub-)phrases}.
An aspect is ``part of an app or related to it'', while a subjective (sub-)phrase ``express opinions and statements of a
personal evaluation regarding the app or a part of it, that are not based on (objective) facts but on individual opinions of the reviewers'' \citep[p.~1116]{sanger2016scare}.
The authors therefore draw a distinction between objective facts regarding an app or parts of it and the sentiment connected to it (``functionality \textit{X} is not working'' $\rightarrow$ negative), and subjective user meanings concerning an app or parts of it (``I really like the color of \textit{X}'' $\rightarrow$ positive).
This fine level of annotations leads often to several annotations per review, the sentiment of which may not always match.
As illustration, consider the following review:

\begin{examples}
	\label{ex:fine-grained-anno}
	\item guter wecker... \textbar\textbar\ vom prinzip her echt gut...aber grade was die sprachausgabe betrifft noch etwas buggy....\footnote{The ``\textbar\textbar'' denotes that the text left of it is the user given ``title'' of the review, and the part on the right is the actual review.}
\end{examples}

There are the following annotations for the several aspects and subjective (sub-)phrases are present in this example:

\begin{multicols}{2}

\begin{itemize}
	\item [Aspects]
	\item Wecker $\rightarrow$ neutral
	\item Prinzip $\rightarrow$ neutral
	\item Sprachausgabe $\rightarrow$ neutral
\end{itemize}

\columnbreak

\begin{itemize}
	\item [Subjectives]
	\item guter $\rightarrow$ positive
	\item echt gut $\rightarrow$ positive
	\item etwas buggy $\rightarrow$ negative
\end{itemize}

\end{multicols}

\subsubsection{SCARE reviews}

Besides their carefully, hand-annotated corpus, the authours also provide a dataset comprising of XXX reviews along with the rating --- one to five stars ---, that were available in German on the Google App XXXXXXXXXXX.




\subsubsection{Preprocessing}

For integrating the SCARE corpus into my GerBLUE corpus, I need to prepare the data, so it can be handled by the model architecture.
Following the original GLUE sentiment task, the model needs only to predict one sentiment label for each example.
Since there exist mostly multiple annotations for each review in this data set, the data needs to be pre-processed in a way, so that there is one review-label per example.

To generate the review-label, I simply carry out an majority class decision:
The label that is most often annotated for a given review, regardless if it is an aspect or a subjective, is then also the review-label.
If there is no majority label, the review-label is set to ``neutral''.
This is also the chosen strategy for 51 reviews that had no labels at all (e.g. ``Ich bin die erfuinderin | | Ich bin die erfunden!!!!!!!!!!!!!!!!!!!!!!!!!'').

\subsection{XNLI}

\cite{conneau2018xnli}

\subsection{XQuAD}

\cite{artetxe2019cross}

% \section{BLEU Scores}
% \label{sec:5_bleuscores}
% 
% Table \ref{bleuresults} shows how to use the predefined tab command to have it listed.
% %\tab{#1: label}{#2: long caption}{#3: the table content}{#4: short caption}
% \tab{bleuresults}{BLEU scores of different MT systems}
% {\begin{tabular}{ll|ccc|c}
% language pair		& ABC	& YYY	\\
% \hline
% EN$\rightarrow$DE	& 20.56	& 32.53 \\
% DE$\rightarrow$EN	& 43.35	& 52.53 \\
% \hline
% \end{tabular}
% }{ABC BLEU scores}
% 
% And we can reference the large table in the appendix as Table \ref{appendixTable}
% 
% \section{Evaluation}
% \label{sec:5_evaluation}
% We saw in section \ref{sec:5_bleuscores} 
% 
% We will see in subsection \ref{subsec:5_moreeval} some more evaluations.
% 
% \subsection{More evaluation}
% \label{subsec:5_moreeval}
% 
% 
% \section{Citations}
% Although BLEU scores should be taken with caution (see \citet{Callison-Burch2006})
% or if you prefer to cite like this: \citep{Callison-Burch2006} \ldots
% 
% to cite: \cite[30-31]{Koehn2005} \\
% to cite within parentheses/brackets: \citep{Koehn2005}, \citep[30-32]{Koehn2005}\\ %\usepackage[square]{natbib} => square brackets
% 
% to cite within the text: \citet{Koehn2005}, \citet[37]{Koehn2005}\\
% only the author(s): \citeauthor{Callison-Burch2006}\\
% only the year: \citeyear{Callison-Burch2006}\\
% 
% \section{Graphics}
% 
% To include a graphic that appears in the list of figures, use the predefined fig command:\\
% %\fig{#1: filename}{#2: label}{#3: long caption}{#4: width}{#5: short caption}
% \fig{images/Rosetta_Stone.jpg}{fig:rosetta}{The Rosetta Stone}{10}{Rosetta}
% 
% %\reffig{#1: label}
% And then reference it as \reffig{fig:rosetta} is easy.
% 
% \section{Some Linguistics}
% 
% (With the package 'covington')\\
% 
% Gloss:
% 
% \begin{examples}
%  \item \gll The cat sits on the table.
% 	    die Katze sitzt auf dem Tisch
% 	\glt 'Die Katze sitzt auf dem Tisch.'
%     \glend
% \end{examples}
% 
% Gloss with morphology:
% 
% \begin{examples}
%  \item \gll La gata duerm -e en la cama.
% 	    Art.Fem.Sg Katze schlaf -3.Sg in Art.Fem.Sg Bett
% 	\glt 'Die Katze schl\"aft im Bett.'
%     \glend
% \end{examples}
% 
