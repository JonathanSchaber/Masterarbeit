
\newchap{Data Sets}
\label{chap:5_dataset}

\section{gliGLUE}

Traditionally in linguistics, language is analyzed into different structural levels, where different tools for describing these levels, or strata, are used.
In most theories, there are are four of these structural levels proposed:
Beginning from the Bottom, there is the level of Phonetics and Phonology, followed by Morphology, then there is the level of Syntax, and the last one is Semantics.\footnote{Sometimes Pragmatics is conceptualized as an additional fifth layer on top, sometimes it is considered to form a field of its own; I follow the latter.}
While the first three levels deal with the form of utterances of human language, semantics is concerned with the meaning of such utterances \citep[p.~4ff.]{kracht2007introduction}.

% \tab{tab:levels-of-lang}{Levels of language analysis and description.}
% {\begin{tabular}{|l|l|l|l|}
% \hline
% \multicolumn{2}{|c|}{Form}     & \multicolumn{2}{c|}{Meaning}     \\ \hline
% 	% a & b & c & d \\ \hline
% 	& \multicolumn{2}{c|}{Level} &                   \\ \hhline{====}
%         Syntax  & \multicolumn{2}{l|}{Sentence} & \multirow{3}{*}{Semantics} \\ \cline{1-3}
%         Morphology & \multicolumn{2}{l|}{Word} &                   \\ \cline{1-3}
%         Phonology & \multicolumn{2}{l|}{Sound} &                   \\ \hline
% \end{tabular}
% }{Levels of language}


Following \cite{wang2018glue}, 

\tab{tab:original-GLUE}{Original GLUE data sets and tasks.}
{\begin{tabular}{l|llll}
	Data Set & NLP Task & ML Task & \# Examples & Splits \\
	\hhline{=|====}
	& \multicolumn{4}{c}{Single-Sentence Tasks} \\
	CoLA &  Acceptability & Binary Classification & 8.5k/1k & train/test \\
	SST-2 & Sentiment Analysis & Binary Classification & 67k/1.8k & train/test \\
	\hline
	& \multicolumn{4}{c}{Two-Sentence Tasks} \\
	MNLI & Natural Language Inference & Multi-Class Classification &  393k/20k & train/test \\
	MRPC & Paraphrase Identification & Binary Classification & 3.7k/1.7k & train/test \\
	QNLI & Question Answering & Binary Classification\footnote{\cite{wang2018glue} reformulate the original SQuAD task CITE of predicting an answer span in the context into a sentence pair binary classification task: They pair each sentence in the context with the question and predict whether or not the context sentence includes the answer span.} &  105k/5.4k & train/test \\
	QQP & Paraphrase Identification & Binary Classification &  364k/391k & train/test \\
	RTE & Natural Language Inference & Binary Classification\footnote{\cite{wang2018glue} combine several data sets into RTE; for data sets that have three labels --- \emph{entailment}, \emph{neutral}, and \emph{contradiction} --- they collapse the latter two into one label \emph{not\_entailment}.} &  2.5k/3k & train/test \\
	STS-B & Sentence Similarity & Regression (1 - 5) & 7k/1.4k & train/test \\
	WNLI & Coreference Resolution & Binary Classification\footnote{In the original Winograd Schema Challenge CITE, the task is to choose the correct referent of a pronoun from a list. \cite{wang2018glue} reformulate this to a sentence pair classification task, where the original sentence is paired with the original sentence with each pronoun substituted from the list and then predicting whether the substituted sentence is entailed by the original one.} &  634/146 & train/test \\
\end{tabular}
}{GLUE}


\tab{tab:overview-data-sets}{gliGLUE data sets and tasks.}
{\begin{tabular}{l|llll}
	Data Set & NLP Task & ML Task & \# Examples & Splits \\
	\hhline{=|====}
	& \multicolumn{4}{c}{Single-Sentence Tasks} \\
	deISEAR &  Emotion Detection & Multi-Class Classification  & 1 001 & - \\
	SCARE & Sentiment Analysis & Multi-Class Classification & 1 760 & - \\
	\hline
	& \multicolumn{4}{c}{Two-Sentence Tasks} \\
	% SCARE Reviews &  Sentiment Analysis & Multi-Class Classification & 802 860 & - \\
	MLQA & Question Answering & Span Prediction & 509/4 499 & dev/test \\
	PAWS-X & Paraphrase Identification & Binary Classification & 14 402/2 000/4 000 & train/dev/test \\
	XNLI & Natural Language Inference & Multi-Class Classification &  2 489/7 498 & dev/test \\
	XQuAD & Question Answering & Span Prediction &  1 192 & - \\
\end{tabular}
}{gliGLUE}

\subsection{General Issues}

There are a few remarks and strategies that apply to all collected corpora:

(1) Most of the data sets are not monolingual, i.e. German, sources, but bi- or multilingual corpora.
To compile a German GLUE corpus I only use the German subset of those corpora.
For example, the MLQA data set provides all 49 combinations of the languages it contains:
Context in Arabic, question in Hindi; context in English, question in Spanish, etc.
Also in this case, I choose only the German-German part of the data set for my corpus.

(2) The data sets I chose for my little GLUE corpus are being provided in different approaches.
While three of the corpora, namely MLQA, PAWS-X, and XNLI, come with a predefined split, the others are made available without splits.
In the latter case, I split the data sets into train, development, and test splits using a 0.7, 0.15, and 0.15 portion, respectively.
Interestingly, the data sets that come with splits, only provide a development and test portion.
To ensure that my results are comparable with those that the authors of the different data sets report, I leave the test split as it is, and split the development set into a train and development set, implementing a 85:15 ratio.

The following differences to the original GLUE corpus must be noted:

(1) While \cite{wang2018glue} reformulate a multitude of tasks into inference tasks, I follow in my implementation \cite{zhang2019semantics} and approach the question answering tasks as \cite{devlin2018bert} in the original BERT implementation; i.e. as span prediction task.

\section{Corpora}

In this section, I give a detailed description of the selected data sets in alphabetical order:
What kind of task is addressed, what is the text variety, how looks the label distribution, etc.

\subsection{deISEAR}

\subsubsection{Task}

This data set addresses the task of Emotion recognition, a sub-task of Sentiment Analysis.
Technically, it is a sequence classification problem: Given a sequence of tokens, predict the correct label from a fixed set of emotions.
Following by the original study ``International Survey on Emotion Antecedents and Reactions'' \citep{scherer1994evidence}, \cite{troiano2019crowdsourcing} constructed their data set for German:
In a first step, the authors presented annotators with one of seven emotions, and asked them to come up with a textual description of an event in which they felt that emotion.
The task was formulated as a sentence completion, so the annotators, which were recruited via an crowdsourcing platform, had to complete sentences having the following structure: ``Ich fühlte {emotion}, als/weil...''.
Seven emotions were given for which the descriptions had to be constructed:
Traurigkeit, Ekel, Schuld, Wut, Angst, Scham, Freude.
For \emph{Traurigkeit} and \emph{Ekel} there are 144 examples in the data set, for the other emotions there are 143.

\begin{examples}
	\label{ex:deisear}
	\item Ich fühlte ..., als mein Laptop kaputt ging und die Garantie schon abgelaufen war.
\end{examples}

The searched emotion is \emph{Traurigkeit} in example \ref{ex:deisear}.

%\fig{images/deISEAR_traindevtest.png}{fig:deISEAR-devtest}{Distribution of emotions in the deISEAR data set.}{15}{deISEAR statistics}

\subsection{MLQA}

\begin{examples}
	\label{ex:mlqa}
	\item Rita Sahatçiu Ora (* 26. November 1990 in Priština, SFR Jugoslawien) ist eine britische Sängerin und Schauspielerin kosovarischer Herkunft. Von 2010 bis 2016 stand sie bei Jay Z und Roc Nation unter Vertrag. Seit 2017 steht sie bei Atlantic Records unter Vertrag.
\end{examples}

\begin{enumerate}
	\item Wann wurde Rita Sahatçiu Ora geboren? $\rightarrow$ 26. November 1990
\end{enumerate}

\cite{lewis2019mlqa} compiled

PROBLEM: 231 out of 5,029 exceed tokenized length of 512 $\rightarrow$ ignore? 4.6\%

stats:

average length train answer: 4.0 (5.6) \\
average length dev answer: 3.7 (5.2) \\
average length test answer: 4.0 (5.6)

average length train question: 9.4 (11.4) \\
average length dev question: 8.6 (10.6) \\
average length test question: 9.1 (11.2)

average length train context: 127.7 (162.7) \\
average length dev context: 125.1 (159.4) \\
average length test context: 129.9 (165.5)


\subsection{PAWS-X}

The PAWS-X corpus \cite{yang2019paws} was compiled to provide a multilingual source for training models that address the problem of paraphrase identification. 
Since most corpora for this task are available only in English the authors compiled this corpus by humanly translate a subset of the original PAWS corpus \cite{zhang2019paws}.

\begin{examples}
	\label{ex:paws-x}
	\item Die Familie zog 1972 nach Camp Hill, wo er die Trinity High School in Harrisburg, Pennsylvania, besuchte.
		
	1972 zog die Familie nach Camp Hill, wo er die Trinity High School in Harrisburg, Pennsylvania, besuchte.
\end{examples}

The label for the sentence pair \ref{ex:paws-x}, of course, would be \emph{true}, since sentence one is a paraphrase of sentence two, and vice versa.


% TODO: new stats figure!
%\fig{}{fig:PAWS-X-devtest}{Distribution of paraphrases (True) versus non-paraphrases (False) in the PAWS-X data set.}{15}{PAWS-X statistics}
stats

\subsubsection{Preprocessing}

During the preprocessing of this data set, the following considerations are taken into account:

In the predefined development and test splits, there are some examples where one or both sentences consist
only of the string ``NS''.
I decided to not include this examples into the data used for training and evaluating my models, since
those examples don't contribute any useful features for the model.\footnote{The authors don't comment on these obscure sentences, so I do not know what was the reasoning behind including these into the data sets.}
Further, some examples consist of empty strings; I treat those the same way as the examples mentioned before.

Further, there are sentences XXXXX

\subsubsection{Statistics}

Since the training data are solely machine-translated while the development and test data are human-translated, there needs to be some clarification as to how differently those sets are.
One measure to capture similarities between sentences is the BLEU score \cite{papineni2002bleu}:
This score measures the overlap of n-grams between two sentences, such that XXX

Train: 0.553 \\
Development: 0.373 \\
Test: 0.384

The training set contains 3,209 sentence pairs (6.6\% of all the sentence pairs) with a BLEU score of 1.0 --- which means they are identical.

\subsection{SCARE}

\subsubsection{SCARE normal}

``Unlike product reviews of other domains, e.g. household appliances, consumer electronics or movies, application reviews offer a couple of peculiarities which deserve special treatment:
The way in which users express their opinion in app reviews is shorter and more concise than in other product reviews.
Moreover, due to the frequent use of colloquial words and a flexible use of grammar, app reviews can be considered to be more similiar [sic] to Twitter messages (“Tweets”) than reviews of products from other domains or platforms \textelp{}.'' \citep[p.~1114]{sanger2016scare}


The Sentiment Corpus of App Reviews with Fine-grained Annotations in German \cite{sanger2016scare} is a hand-annotated corpus that asserts so sentiment to German mobile app reviews stemming from the Google Play Store.
Since there are many users of 
In contrast to other data sets, e.g. \citep{socher2013recursive, go2009twitter}, that attributes one sentiment label to a whole text (may it be a review, a tweet, etc.), \cite{sanger2016scare} annotated their data set on a lower textual level:
Not each review gets labelled for a certain polarity --- i.e. \emph{positive}, \emph{negative}, or \emph{neutral} --- but what the authors call \emph{aspects} and correlating \emph{subjective phrases}.
An aspect is an entity, that is related to the application:
It may be the application itself, parts of the application, a feature request regarding the application, etc.
A subjective phrase ``express[es] opinions and statements of a personal evaluation regarding the app or a part of it, that are not based on (objective) facts but on individual opinions of the reviewers'' \citep[p.~1116]{sanger2016scare}.
In other words, aspects are facts about the App and subjective phrases are user opinions regarding them.
This fine level of annotations leads often to several annotations per review, the sentiment of which may not always match.
As illustration, consider the following review:

\begin{examples}
	\label{ex:fine-grained-anno}
	\item guter wecker... \textbar\textbar\ vom prinzip her echt gut...aber grade was die sprachausgabe betrifft noch etwas buggy....\footnote{The ``\textbar\textbar'' denotes that the text left of it is the user given ``title'' of the review, and the part on the right is the actual review.}
\end{examples}

There are the following annotations for the aspects and their corresponding subjective phrases (aspects are bold, the subjective phrase is italic and the polarity is normal):


\begin{itemize}
	\item \textbf{Wecker}, \textit{guter} $\rightarrow$ positive
	\item \textbf{Prinzip}, \textit{echt gut} $\rightarrow$ positive
	\item \textbf{Sprachausgabe}, \textit{etwas buggy} $\rightarrow$ negative
\end{itemize}

As is clear from this example, in a given review there may be several aspects with a corresponding subjective phrase per review.
It is well possible, as in the provided example, that the sentiment of these is not always the same.

Example from .csv file:

\tab{tab:scare-csv-example}{An example from the alarm\_clocks.csv file.}
{\begin{tabular}{llllllll}
	Class & ID & Left & Right & Text & Aspect- / Subj-ID & Polarity & Relation  \\
	\hline
	subjective & 7000 & 0 & 15 & Alles wieder ok & 7000-subjective2 & Positive & Related \\
	aspect & 7000 & 21 & 27 & Update & 7000-aspect1 & Neutral & Related \\
	subjective & 7000 & 28 & 40 & funktioniert & 7000-subjective1 & Positive & Related \\
	subjective & 7001 & 0 & 10 & Echt super & 7001-subjective5 & Positive & Related \\
	subjective & 7001 & 15 & 22 & Schönes & 7001-subjective4 & Positive & Related \\
	subjective & 7001 & 38 & 51 & einzigartiges & 7001-subjective3 & Positive & Related \\
	aspect & 7001 & 52 & 61 & interface & 7001-aspect2 & Neutral & Related \\
	subjective & 7001 & 63 & 78 & wirklich klasse & 7001-subjective2 & Positive & Related \\
	subjective & 7001 & 80 & 90 & Schön wäre & 7001-subjective1 & Negative & Related \\
	aspect & 7001 & 113 & 135 & lieder als klingeltöne & 7001-aspect1 & Neutral & Foreign \\
\end{tabular}
}{Example SCARE .csv}

Corresponding .rel file:

\tab{tab:scare-csv-example}{An example from the alarm\_clocks.rel file.}
{\begin{tabular}{lllll}
	Relation-ID & Aspect-ID & Subj-ID & Aspect-String & Subj-String \\
	\hline
	7000 & 7000-aspect1 & 7000-subjective1 & Update & funktioniert \\
	7001 & 7001-aspect2 & 7001-subjective4 & interface & Schönes \\
	7001 & 7001-aspect2 & 7001-subjective3 & interface & einzigartiges \\
	7001 & 7001-aspect1 & 7001-subjective1 & lieder als klingeltöne & Schön wäre \\
\end{tabular}
}{Example SCARE .rel}


stats: there are 1,760 fine-grained annotated reviews

\subsubsection{SCARE reviews}

Besides their carefully, hand-annotated corpus, the authors also provide a dataset comprising of 802,860 reviews along with the rating --- one to five stars ---, that were available in German on the Google Play Store.
This data set is much larger than the annotated one: Due to the great expenses of generating those fine-grained annotations, the authors were able to annotate only 0.22\% of all reviews available.

\fig{images/SCARE_star_stats.png}{fig:SCARE-starstats}{Overview of percentage of stars given. Clearly, there is an imbalance towards giving the full amount of stars possible}{12}{SCARE stars statistics}

\subsubsection{Preprocessing}

For integrating the SCARE corpus into my GerBLUE corpus, I need to prepare the data, so it can be handled by the model architecture.
Following the original GLUE sentiment task, the model needs only to predict one sentiment label for each example.
Since there exist mostly multiple annotations for each review in this data set, the data needs to be pre-processed in a way, so that there is one review-label per example.

To generate the review-label, I simply carry out an majority class decision:
The label that is most often annotated for a given review, regardless if it is an aspect or a subjective, is then also the review-label.
If there is no majority label, the review-label is set to ``neutral''.
This is also the chosen strategy for 51 reviews that had no labels at all; an example of such a review is the following one: 

\begin{examples}
	\item ``Ich bin die erfuinderin \textbar \textbar\ Ich bin die erfunden!!!!!!!!!!!!!!!!!!!!!!!!!''.
\end{examples}

\fig{images/SCARE_label_stats.png}{fig:SCARE-labelstats}{Statistics of label generation. For most of the examples, there was a clear majority decision as to which label should be chosen. \emph{Close Majority} means the majority vote was off by 1. The reddish portions in the graph were labelled \emph{neutral} by default, while the blueish ones were labelled accoring to the majority vote decision.}{12}{SCARE label statistics}
2.9\% of reviews had no labels at all 

3.0\% of votes were non-majority

13.8\% of votes were close (label difference of 1)

\subsection{XNLI}

\cite{conneau2018xnli}

% TODO: new stats figure!
%\fig{}{fig:XNLI-stats}{Distribution of labels in the XNLI data set.}{15}{XNLI label statistics}

\begin{examples}
	\label{ex:xnli}
	\item Ich wusste nicht was ich vorhatte oder so, ich musste mich an einen bestimmten Ort in Washington melden.

        Ich war noch nie in Washington, deshalb habe ich mich auf der Suche nach dem Ort verirrt, als ich dahin entsandt wurde.
\end{examples}

The label for example \ref{ex:xnli} is \emph{neutral} since the second sentence does not follow necessarily from the first and it also does not contradict it, either.
number of examples= 7,500

\subsection{XQuAD}

\begin{examples}
	\label{ex:xquad}
	\item Aristoteles lieferte eine philosophische Diskussion über das Konzept einer Kraft als integraler Bestandteil der aristotelischen Kosmologie. Nach Ansicht von Aristoteles enthält die irdische Sphäre vier Elemente, die an verschiedenen „natürlichen Orten“ darin zur Ruhe kommen. Aristoteles glaubte, dass bewegungslose Objekte auf der Erde, die hauptsächlich aus den Elementen Erde und Wasser bestehen, an ihrem natürlichen Ort auf dem Boden liegen und dass sie so bleiben würden, wenn man sie in Ruhe lässt. Er unterschied zwischen der angeborenen Tendenz von Objekten, ihren „natürlichen Ort“ zu finden (z. B. dass schwere Körper fallen), was eine „natürliche Bewegung“ darstellt und unnatürlichen oder erzwungenen Bewegungen, die den fortwährenden Einsatz einer Kraft erfordern. Diese Theorie, die auf der alltäglichen Erfahrung basiert, wie sich Objekte bewegen, wie z. B. die ständige Anwendung einer Kraft, die erforderlich ist, um einen Wagen in Bewegung zu halten, hatte konzeptionelle Schwierigkeiten, das Verhalten von Projektilen, wie beispielsweise den Flug von Pfeilen, zu erklären. Der Ort, an dem der Bogenschütze den Pfeil bewegt, liegt am Anfang des Fluges und während der Pfeil durch die Luft gleitet, wirkt keine erkennbare effiziente Ursache darauf ein. Aristoteles war sich dieses Problems bewusst und vermutete, dass die durch den Flugweg des Projektils verdrängte Luft das Projektil zu seinem Ziel trägt. Diese Erklärung erfordert ein Kontinuum wie Luft zur Veränderung des Ortes im Allgemeinen.
\end{examples}

The questions and corresponding answer spans for paragraph \ref{ex:xquad} in the data set are the following:

\begin{enumerate}
	\item Wer leitete eine philosophische Diskussion über Kraft? $\rightarrow$ Aristoteles
	\item Wovon war das Konzept der Kraft ein integraler Bestandteil? $\rightarrow$ aristotelischen Kosmologie
	\item Aus wie vielen Elementen besteht die irdische Sphäre nach Ansicht des Aristoteles? $\rightarrow$ vier
	\item Wo vermutete Aristoteles den natürlichen Ort für Erd- und Wasserelemente? $\rightarrow$ auf dem Boden
	\item Was bezeichnete Aristoteles als erzwungene Bewegung? $\rightarrow$ unnatürlichen
\end{enumerate}

\cite{artetxe2019cross}

stats:

average length train answer: 3.2 (4.7) \\
average length dev answer:  3.3 (5.2) \\
average length test answer: 3.6 (5.7)

average length train question: 11.3 (14.3)  \\
average length dev question: 11.5 (14.3)  \\
average length test question: 11.4 (14.5)

average length train context: 151.3 (191.7)  \\
average length dev context: 149.5 (190.7) \\
average length test context: 144.3 (187.3) 

\subsection{Overview}

\tab{tab:data-set-overview}{Overview over collected data sets and tasks.}
{\begin{tabular}{l|llll}
	Data Set & NLP Task  & ML Task  & \# Examples & Splits \\
	\hline
	deISEAR &  Emotion Detection & Sequence Classification  & XYZ & - \\
	MLQA & Question Answering & Span Prediction & XYZ & dev/test \\
	PAWS-X & Paraphrase Identification & Sequence Classification & XYZ & train/dev/test \\
	SCARE & Sentiment Analysis & Sequence Classifiaction & XYZ & - \\
	SCARE Rev. &  Sentiment Analysis & Sequence Classification & XYZ & - \\
	XNLI & Natural Language Inference & Sequence Classification &  XYZ & dev/test \\
	XQuAD & Question Answering & Span Prediction & XYZ & - \\
\end{tabular}
}{Overview data sets}

% \section{BLEU Scores}
% \label{sec:5_bleuscores}
% 
% Table \ref{bleuresults} shows how to use the predefined tab command to have it listed.
% %\tab{#1: label}{#2: long caption}{#3: the table content}{#4: short caption}
% \tab{bleuresults}{BLEU scores of different MT systems}
% {\begin{tabular}{ll|ccc|c}
% language pair		& ABC	& YYY	\\
% \hline
% EN$\rightarrow$DE	& 20.56	& 32.53 \\
% DE$\rightarrow$EN	& 43.35	& 52.53 \\
% \hline
% \end{tabular}
% }{ABC BLEU scores}
% 
% And we can reference the large table in the appendix as Table \ref{appendixTable}
% 
% \section{Evaluation}
% \label{sec:5_evaluation}
% We saw in section \ref{sec:5_bleuscores} 
% 
% We will see in subsection \ref{subsec:5_moreeval} some more evaluations.
% 
% \subsection{More evaluation}
% \label{subsec:5_moreeval}
% 
% 
% \section{Citations}
% Although BLEU scores should be taken with caution (see \citet{Callison-Burch2006})
% or if you prefer to cite like this: \citep{Callison-Burch2006} \ldots
% 
% to cite: \cite[30-31]{Koehn2005} \\
% to cite within parentheses/brackets: \citep{Koehn2005}, \citep[30-32]{Koehn2005}\\ %\usepackage[square]{natbib} => square brackets
% 
% to cite within the text: \citet{Koehn2005}, \citet[37]{Koehn2005}\\
% only the author(s): \citeauthor{Callison-Burch2006}\\
% only the year: \citeyear{Callison-Burch2006}\\
% 
% \section{Graphics}
% 
% To include a graphic that appears in the list of figures, use the predefined fig command:\\
% %\fig{#1: filename}{#2: label}{#3: long caption}{#4: width}{#5: short caption}
% \fig{images/Rosetta_Stone.jpg}{fig:rosetta}{The Rosetta Stone}{10}{Rosetta}
% 
% %\reffig{#1: label}
% And then reference it as \reffig{fig:rosetta} is easy.
% 
% \section{Some Linguistics}
% 
% (With the package 'covington')\\
% 
% Gloss:
% 
% \begin{examples}
%  \item \gll The cat sits on the table.
% 	    die Katze sitzt auf dem Tisch
% 	\glt 'Die Katze sitzt auf dem Tisch.'
%     \glend
% \end{examples}
% 
% Gloss with morphology:
% 
% \begin{examples}
%  \item \gll La gata duerm -e en la cama.
% 	    Art.Fem.Sg Katze schlaf -3.Sg in Art.Fem.Sg Bett
% 	\glt 'Die Katze schl\"aft im Bett.'
%     \glend
% \end{examples}
% 
