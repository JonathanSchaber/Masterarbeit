
\newchap{Data Sets}
\label{chap:5_dataset}

\section{Why create an own corpus?}

\section{Corpora}

\subsection{deISEAR}

As \cite{troiano2019crowdsourcing} write in their 

\begin{examples}
	\label{ex:deisear}
	\item Ich fühlte ..., als mein Laptop kaputt ging und die Garantie schon abgelaufen war.
\end{examples}

The searched emotion is \textit{Traurigkeit} in example \ref{ex:deisear}.

\subsection{MLQA\_V1}

\begin{examples}
	\label{ex:mlqa}
	\item Rita Sahatçiu Ora (* 26. November 1990 in Priština, SFR Jugoslawien) ist eine britische Sängerin und Schauspielerin kosovarischer Herkunft. Von 2010 bis 2016 stand sie bei Jay Z und Roc Nation unter Vertrag. Seit 2017 steht sie bei Atlantic Records unter Vertrag.
\end{examples}

\begin{enumerate}
	\item Wann wurde Rita Sahatçiu Ora geboren? $\rightarrow$ 26. November 1990
\end{enumerate}

\cite{lewis2019mlqa} compiled

\subsection{PAWS-X}

The PAWS-X corpus \cite{yang2019paws} was compiled to provide a multilingual source for training models that address the problem of paraphrase identification. 
Since most corpora for this task are available only in English, the authors compiled this corpus, by humanly translate a subset of the original PAWS corpus \cite{zhang2019paws}.

\begin{examples}
	\label{ex:paws-x}
	\item Die Familie zog 1972 nach Camp Hill, wo er die Trinity High School in Harrisburg, Pennsylvania, besuchte.
		
	1972 zog die Familie nach Camp Hill, wo er die Trinity High School in Harrisburg, Pennsylvania, besuchte.
\end{examples}

The label for the sentence pair \ref{ex:paws-x}, of course, would be \textit{true}, since sentence one is a paraphrase of sentence two, and vice versa.


stats

4,000 examples in German (human translated)

49,402 examples in German (machine translated)



\subsection{SCARE}

\subsubsection{SCARE normal}

The Sentiment Corpus of App Reviews with Fine-grained Annotations in German \cite{sanger2016scare} is a hand-annotated corpus that asserts so sentiment to German mobile app reviews stemming from the Google Play Store.
Since there are many users of 
In contrast to other data sets, e.g. \citep{socher2013recursive, go2009twitter}, that attributes one sentiment label to a whole text (may it be a review, a tweet, etc.), \cite{sanger2016scare} annotated their data set on a lower textual level:
Not each review gets labelled for a certain polarity --- i.e. \textit{positive}, \textit{negative}, or \textit{neutral} --- but what the authors call \textit{aspects} and \textit{subjective (sub-)phrases}.
An aspect is ``part of an app or related to it'', while a subjective (sub-)phrase ``express opinions and statements of a
personal evaluation regarding the app or a part of it, that are not based on (objective) facts but on individual opinions of the reviewers'' \citep[p.~1116]{sanger2016scare}.
The authors therefore draw a distinction between objective facts regarding an app or parts of it and the sentiment connected to it (``functionality \textit{X} is not working'' $\rightarrow$ negative), and subjective user meanings concerning an app or parts of it (``I really like the color of \textit{X}'' $\rightarrow$ positive).
This fine level of annotations leads often to several annotations per review, the sentiment of which may not always match.
As illustration, consider the following review:

\begin{examples}
	\label{ex:fine-grained-anno}
	\item guter wecker... \textbar\textbar\ vom prinzip her echt gut...aber grade was die sprachausgabe betrifft noch etwas buggy....\footnote{The ``\textbar\textbar'' denotes that the text left of it is the user given ``title'' of the review, and the part on the right is the actual review.}
\end{examples}

There are the following annotations for the several aspects and subjective (sub-)phrases are present in this example:

\begin{multicols}{2}

\begin{itemize}
	\item [Aspects]
	\item Wecker $\rightarrow$ neutral
	\item Prinzip $\rightarrow$ neutral
	\item Sprachausgabe $\rightarrow$ neutral
\end{itemize}

\columnbreak

\begin{itemize}
	\item [Subjectives]
	\item guter $\rightarrow$ positive
	\item echt gut $\rightarrow$ positive
	\item etwas buggy $\rightarrow$ negative
\end{itemize}

\end{multicols}

As is clear from this example, in a given review there may be several claims or remarks concerning functionalities of the product, or personal views about an app in general.
It is well possible, as in the provided example, that the sentiment of this ``micro-stances'' is not always the same; while all aspects in the example above are \textit{neutral}, there are two \textit{positive} and one \textit{negative} subjectives.


stats: there are 1,760 fine-grained annotated reviews

\subsubsection{SCARE reviews}

Besides their carefully, hand-annotated corpus, the authors also provide a dataset comprising of 802,860 reviews along with the rating --- one to five stars ---, that were available in German on the Google Play Store.
This data set is much larger than the annotated one: Due to the great expenses of generating those fine-grained annotations, the authors were able to annotate only 0.22\% of all reviews available.



\subsubsection{Preprocessing}

For integrating the SCARE corpus into my GerBLUE corpus, I need to prepare the data, so it can be handled by the model architecture.
Following the original GLUE sentiment task, the model needs only to predict one sentiment label for each example.
Since there exist mostly multiple annotations for each review in this data set, the data needs to be pre-processed in a way, so that there is one review-label per example.

To generate the review-label, I simply carry out an majority class decision:
The label that is most often annotated for a given review, regardless if it is an aspect or a subjective, is then also the review-label.
If there is no majority label, the review-label is set to ``neutral''.
This is also the chosen strategy for 51 reviews that had no labels at all (e.g. ``Ich bin die erfuinderin \textbar \textbar\ Ich bin die erfunden!!!!!!!!!!!!!!!!!!!!!!!!!'').

2.9\% of reviews had no labels at all 

3.0\% of votes were non-majority

13.8\% of votes were close (label difference of 1)

\subsection{XNLI}

\cite{conneau2018xnli}

\begin{examples}
	\label{ex:xnli}
	\item Ich wusste nicht was ich vorhatte oder so, ich musste mich an einen bestimmten Ort in Washington melden.

        Ich war noch nie in Washington, deshalb habe ich mich auf der Suche nach dem Ort verirrt, als ich dahin entsandt wurde.
\end{examples}

The label for example \ref{ex:xnli} is \textit{neutral} since the second sentence does not follow necessarily from the first and it also does not contradict it, either.
number of examples= 7,500

\subsection{XQuAD}

\begin{examples}
	\label{ex:xquad}
	\item Aristoteles lieferte eine philosophische Diskussion über das Konzept einer Kraft als integraler Bestandteil der aristotelischen Kosmologie. Nach Ansicht von Aristoteles enthält die irdische Sphäre vier Elemente, die an verschiedenen „natürlichen Orten“ darin zur Ruhe kommen. Aristoteles glaubte, dass bewegungslose Objekte auf der Erde, die hauptsächlich aus den Elementen Erde und Wasser bestehen, an ihrem natürlichen Ort auf dem Boden liegen und dass sie so bleiben würden, wenn man sie in Ruhe lässt. Er unterschied zwischen der angeborenen Tendenz von Objekten, ihren „natürlichen Ort“ zu finden (z. B. dass schwere Körper fallen), was eine „natürliche Bewegung“ darstellt und unnatürlichen oder erzwungenen Bewegungen, die den fortwährenden Einsatz einer Kraft erfordern. Diese Theorie, die auf der alltäglichen Erfahrung basiert, wie sich Objekte bewegen, wie z. B. die ständige Anwendung einer Kraft, die erforderlich ist, um einen Wagen in Bewegung zu halten, hatte konzeptionelle Schwierigkeiten, das Verhalten von Projektilen, wie beispielsweise den Flug von Pfeilen, zu erklären. Der Ort, an dem der Bogenschütze den Pfeil bewegt, liegt am Anfang des Fluges und während der Pfeil durch die Luft gleitet, wirkt keine erkennbare effiziente Ursache darauf ein. Aristoteles war sich dieses Problems bewusst und vermutete, dass die durch den Flugweg des Projektils verdrängte Luft das Projektil zu seinem Ziel trägt. Diese Erklärung erfordert ein Kontinuum wie Luft zur Veränderung des Ortes im Allgemeinen.
\end{examples}

The questions and corresponding answer spans for paragraph \ref{ex:xquad} in the data set are the following:

\begin{enumerate}
	\item Wer leitete eine philosophische Diskussion über Kraft? $\rightarrow$ Aristoteles
	\item Wovon war das Konzept der Kraft ein integraler Bestandteil? $\rightarrow$ aristotelischen Kosmologie
	\item Aus wie vielen Elementen besteht die irdische Sphäre nach Ansicht des Aristoteles? $\rightarrow$ vier
	\item Wo vermutete Aristoteles den natürlichen Ort für Erd- und Wasserelemente? $\rightarrow$ auf dem Boden
	\item Was bezeichnete Aristoteles als erzwungene Bewegung? $\rightarrow$ unnatürlichen
\end{enumerate}

\cite{artetxe2019cross}

% \section{BLEU Scores}
% \label{sec:5_bleuscores}
% 
% Table \ref{bleuresults} shows how to use the predefined tab command to have it listed.
% %\tab{#1: label}{#2: long caption}{#3: the table content}{#4: short caption}
% \tab{bleuresults}{BLEU scores of different MT systems}
% {\begin{tabular}{ll|ccc|c}
% language pair		& ABC	& YYY	\\
% \hline
% EN$\rightarrow$DE	& 20.56	& 32.53 \\
% DE$\rightarrow$EN	& 43.35	& 52.53 \\
% \hline
% \end{tabular}
% }{ABC BLEU scores}
% 
% And we can reference the large table in the appendix as Table \ref{appendixTable}
% 
% \section{Evaluation}
% \label{sec:5_evaluation}
% We saw in section \ref{sec:5_bleuscores} 
% 
% We will see in subsection \ref{subsec:5_moreeval} some more evaluations.
% 
% \subsection{More evaluation}
% \label{subsec:5_moreeval}
% 
% 
% \section{Citations}
% Although BLEU scores should be taken with caution (see \citet{Callison-Burch2006})
% or if you prefer to cite like this: \citep{Callison-Burch2006} \ldots
% 
% to cite: \cite[30-31]{Koehn2005} \\
% to cite within parentheses/brackets: \citep{Koehn2005}, \citep[30-32]{Koehn2005}\\ %\usepackage[square]{natbib} => square brackets
% 
% to cite within the text: \citet{Koehn2005}, \citet[37]{Koehn2005}\\
% only the author(s): \citeauthor{Callison-Burch2006}\\
% only the year: \citeyear{Callison-Burch2006}\\
% 
% \section{Graphics}
% 
% To include a graphic that appears in the list of figures, use the predefined fig command:\\
% %\fig{#1: filename}{#2: label}{#3: long caption}{#4: width}{#5: short caption}
% \fig{images/Rosetta_Stone.jpg}{fig:rosetta}{The Rosetta Stone}{10}{Rosetta}
% 
% %\reffig{#1: label}
% And then reference it as \reffig{fig:rosetta} is easy.
% 
% \section{Some Linguistics}
% 
% (With the package 'covington')\\
% 
% Gloss:
% 
% \begin{examples}
%  \item \gll The cat sits on the table.
% 	    die Katze sitzt auf dem Tisch
% 	\glt 'Die Katze sitzt auf dem Tisch.'
%     \glend
% \end{examples}
% 
% Gloss with morphology:
% 
% \begin{examples}
%  \item \gll La gata duerm -e en la cama.
% 	    Art.Fem.Sg Katze schlaf -3.Sg in Art.Fem.Sg Bett
% 	\glt 'Die Katze schl\"aft im Bett.'
%     \glend
% \end{examples}
% 
