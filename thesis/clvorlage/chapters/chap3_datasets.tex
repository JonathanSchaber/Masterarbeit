\newchap{Data Sets}
\label{chap:3_datasets}

% \epigraph{ascascascascascasc}{\textit{unknown}}

\section{GerGLUE}

Because semantics is such a fuzzy, hard to formalize property of language, it is not easy to
assess the capabilities of an architecture designed at solving problems related to meaning. In
the data-driven NLP community today, it is common practice to measure the {\color{red} power of
a model} by measuring it's performance on some standardized data set. However, a model aiming at
capturing semantics of human langauge ``must be able to process language in a way that is not
exclusive to a single task, genre, or dataset'', as \cite{wang2018glue} correctly point out.

To provide a standardized collection of datasets for the NLP community to compare different NLU-targeted
models, \citeauthor{wang2018glue} compiled the General Language Understanding Evaluation benchmark, in short
GLUE. It consists of nine data sets addressing different NLU problems; from acceptability
tasks (is the phrase ``Saw the man the dog.'' an acceptable English sentence?) to detecting
textual entailment (is the meaning of ``A boy is at the beach'' entailed by the sentence
``Two kids are building a sandcastle at the beach''?). See table \ref{tab:original-GLUE} for
a list of all GLUE data sets, their tasks and further characteristics.

% Therefore, ``it
% must be able to process language in a way that is not exclusive to a single task,
% genre, or dataset'', as \cite{wang2018glue}
% To assess that an architecture designed for tackling NLU problems surpasses another technology indeed,
% it is not probably not enough

\tab{tab:original-GLUE}{Original GLUE data sets and tasks (following the table from \cite{wang2018glue}).}{
  \resizebox{\textwidth}{!}{
    \begin{tabular}{l|llll}
      Data Set & NLP Task                   & ML Task                    & \# Examples & Splits \\ \hline\hline
      & \multicolumn{4}{g}{\textit{Single-Sentence Tasks}} \\
      CoLA     &  Acceptability             & Binary Classification      & 8.5k/1k & train/test \\
      SST-2    & Sentiment Analysis         & Binary Classification      & 67k/1.8k & train/test \\
      \hline
      & \multicolumn{4}{g}{\textit{Sentence Pair Tasks}} \\
      MNLI     & Natural Language Inference & Multi-Class Classification &  393k/20k & train/test \\
      MRPC     & Paraphrase Identification  & Binary Classification      & 3.7k/1.7k & train/test \\
      QNLI     & Question Answering         & Binary Classification\myfootnote{\cite{wang2018glue} reformulate the original SQuAD task CITE of predicting an answer span in the context into a sentence pair binary classification task: They pair each sentence in the context with the question and predict whether or not the context sentence includes the answer span.} &  105k/5.4k & train/test \\
      QQP      & Paraphrase Identification  & Binary Classification      &  364k/391k & train/test \\
      RTE      & Natural Language Inference & Binary Classification\myfootnote{\cite{wang2018glue} combine several data sets into RTE; for data sets that have three labels --- \emph{entailment}, \emph{neutral}, and \emph{contradiction} --- they collapse the latter two into one label \emph{not\_entailment}.} &  2.5k/3k & train/test \\
      STS-B    & Sentence Similarity        & Regression (1 - 5)         & 7k/1.4k & train/test \\
      WNLI     & Coreference Resolution     & Binary Classification\myfootnote{In the original Winograd Schema Challenge CITE, the task is to choose the correct referent of a pronoun from a list. \cite{wang2018glue} reformulate this to a sentence pair classification task, where the original sentence is paired with the original sentence with each pronoun substituted from the list and then predicting whether the substituted sentence is entailed by the original one.} &  634/146 & train/test
    \end{tabular}
  }
}{GLUE}


Following \cite{wang2018glue}, I compile a NLU dataset collection for German, encompassing several
tasks. Unfortunately, the data availability for German is not as extensive as for English, so that
not for all tasks in the original GLUE there was a German counter part; e.g. I couldn't find a sentence
similarity dataset.

\tab{tab:overview-data-sets}{GerGLUE data sets and tasks.}{
  \resizebox{\textwidth}{!}{
    \begin{tabular}{l|llll}
      Data Set & NLP Task                   & ML Task                    & \# Examples        & Splits \\ \hline\hline
      & \multicolumn{4}{g}{\textit{Single-Sentence Tasks}} \\
      deISEAR  & Emotion Detection          & Multi-Class Classification & 1,001              & - \\
      SCARE    & Sentiment Analysis         & Multi-Class Classification & 1,760              & - \\
      \hline
      & \multicolumn{4}{g}{\textit{Sentence Pair Tasks}} \\
      % SCARE Reviews &  Sentiment Analysis & Multi-Class Classification & 802 860 & - \\
      MLQA     & Question Answering         & Span Prediction            & 509/4,499          & dev/test \\
      PAWS-X   & Paraphrase Identification  & Binary Classification      & 49,402/2,001/2,001 & train/dev/test \\
      XNLI     & Natural Language Inference & Multi-Class Classification & 2,489/5,009        & dev/test \\
      XQuAD    & Question Answering         & Span Prediction            & 1,179              & -
    \end{tabular}
  }
}{GerGLUE}

\subsection{General Issues}

There are a few remarks and strategic decisions that apply to all collected dataset in GerGLUE:

(1) All of the datasets except for deISEAR are not monolingual, i.e. German, sources, but bi-
or multilingual corpora. To compile a German GLUE corpus I only use the German subset of
those corpora. For example, the MLQA data set provides all 49 combinations of the languages
it contains: Context in Arabic, question in Hindi; context in English, question in Spanish,
etc. Also in this case, I choose only the German-German part of the data set for my corpus.

(2) The data sets I chose for my GerGLUE corpus are being provided in different modes: While
three of the corpora, namely MLQA, PAWS-X, and XNLI, come with predefined splits, the others
are made available in one set, without defining training, development, and test parts. In
the latter case, I split the data sets into train, development, and test splits using a 0.7,
0.15, and 0.15 portion, respectively. Interestingly, the data sets that come with splits,
mostly provide only a development and test portion. To ensure that my results are comparable
with those that the authors of the different data sets report, I leave the test split as it
is, and split the development set into a train and development set, implementing a 85:15
ratio. The only dataset having explicitly predefined training, development, and test splits
is PAWS-X.

(3) Several of the datasets were constructed by translating existing monolingual English sources
(semi-)automatically into the different target languages. As I show in chapter \ref{chap:5_results},
this does not come without introducing noise into the data.

% The following differences to the original GLUE corpus must be noted:

% (1) While \cite{wang2018glue} reformulate a multitude of tasks into inference tasks, I follow in
% my implementation \cite{zhang2019semantics} and approach the question answering tasks as
% \cite{devlin2018bert} in the original BERT implementation; i.e. as span prediction task.

% (2) I tried to combine a multitude of different tasks into my GLUE dataset (single sentence tasks
% vs bi- or multiple sentence tasks, classification vs. span detection, different semantic problems
% such as emotion detection, question answering etc.), I could not compile all tasks that appear in
% GLUE into my semantic dataset compilation.
% For example, there are data sets that concern linguistic acceptability in the original GLUE
% corpus, such as  e.g. CoLA \cite{warstadt2019neural}, or XXX .
% To disregard this task was not an intentional decision, but due to fact that there are simply not
% as many datasets available for German and apparently there are no datasets addressing linguistic
% acceptability in German.


\section{Corpora}

In this section, I give a detailed description of the selected data sets in alphabetical order:
What kind of task is addressed, what is the text variety, and report some statistical measures,
e.g. the average length of examples in the different sub-sets (Training, Development, Test). I
also document the SOTA benchmark results that the authors specify on their datasets; anyhow,
the results of GLiBERT will not be compared directly with them since my focus is not on dataset
SOTAs but differences between GliBERT with and without SRL information. Nevertheless, for most
datasets, the GliBERT performance is on par with what the authors report.



\subsection{deISEAR}

This data set addresses the task of Emotion Recognition, a sub-task of Sentiment Analysis
\citep{cambria2017affective}. Technically, it is a sequence classification problem: Given a
sequence of tokens $x_1 \dotso x_n$, predict the correct label $y$ from a fixed set of emotions
$Y$. Or, in a more natural way of speaking, to automatically determine what emotion a certain
statement expresses. Following the original study ``International Survey on Emotion Antecedents
and Reactions'' \citep{scherer1994evidence}, \cite{troiano2019crowdsourcing} constructed the
deISEAR data set for German:

In a first step, the authors presented annotators with one of seven emotions, and asked them
to come up with a textual description of an event in which they felt that emotion. The task
was formulated as a sentence completion problem, so the annotators which were recruited via
a crowdsourcing platform, had to complete sentences having the following structure: ``Ich
f√ºhlte \emph{emotion}, als/weil/dass ...''. The seven emotions that were given for which
the descriptions had to be constructed were: Traurigkeit, Ekel, Schuld, Wut, Angst, Scham,
Freude.\myfootnote{Interestingly, out of these seven emotions, six represent rather negative
emotions --- only \emph{Freude} is a clearly positive sensation. Maybe negative emotions are
more lucidly detactable (by humans and/or machines) than positive ones and therefore the study
focused on it.}

\begin{figure}
  \begin{minipage}{0.45\linewidth}
  \vspace{0pt}
    \includegraphics[width=0.9\linewidth]{images/deISEAR_subtokenized.pdf}
  \end{minipage}
  \hfill
  \begin{minipage}{0.45\linewidth}
  \vspace{0pt}
    \includegraphics[width=0.9\linewidth]{images/deISEAR_label_percentages.pdf}
  \end{minipage}
  \stepcounter{myfigure}
  \caption[XNLI Lengths]{\textbf{Left}: Length of subtokenized deISEAR sentences. Note that one extreme outlier in the
                          development set comprising 300 BERT-subtokens is not included in the plot. \textbf{Right}: Label distributions of deISEAR datasets.}
  \label{fig:deisear-stats}
\end{figure}

The second phase of the data generation process comprised of re-labeling the generated sentences
such that five annotators each re-annotate every sentence. The emotion word was omitted, and
the annotators had the list of seven emotions at hand. \citeauthor{troiano2019crowdsourcing}
report that for approximately half of all sentences, the inter-annotator agreement was perfect;
i.e. each of the five annotators attributed the same emotion. However, some emotions seem to
be prone for not being clearly separable: Shame, for example, gets confused with guilt and vice versa
in 17\% and 19\% of the cases, respectively. Remarkably, this pattern is not visible for the GLiBERT
predictions, as is exemplified for one exnsemble in table \ref{tab:confusion-deISEAR}: Shame and guilt
are reliably told apart by the model, however, it seems to have more general problems in recognizing
sadness and anger.

\tab{tab:confusion-deisear}{Confusion matrix for the best deISEAR ensemble ($\alpha$ +SRL subtokenized zeros FFNN head). Interestingly, the observed confounding
                            of Shame and Guilt in the human re-annotations is not present for the GliBERT predictions. However, the overall most difficult, i.e. most
                            often confused emotions for the reported GliBERT head seem to be sadness and anger.}{
  \begin{tabular}{|ll|ccccccc|}
    \hline
                                                    &             & \multicolumn{7}{c|}{Predicted} \\
                                                    &             & Angst    & Ekel    & Freude   & Scham    & Schuld   & Traurigkeit & Wut       \\ \hline
    \multirow{7}{*}{\rotatebox[origin=c]{90}{True}} & Angst       & 17       & 0       & 0        & 0        & 4        & 0           & 2         \\
                                                    & Ekel        & 0        & 18      & 0        & 0        & 2        & 0           & 2         \\
                                                    & Freude      & 1        & 0       & 22       & 0        & 1        & 2           & 2         \\
                                                    & Scham       & 0        & 4       & 2        & 10       & 1        & 0           & 0         \\
                                                    & Schuld      & 0        & 2       & 1        & 3        & 17       & 0           & 0         \\
                                                    & Traurigkeit & 2        & 1       & 0        & 2        & 2        & 13          & 1         \\
                                                    & Wut         & 2        & 1       & 1        & 1        & 2        & 2           & 8         \\ \hline
  \end{tabular}
}{Confusion matrix for one deISEAR +SRL ensemble}

The authors do not report any metric regarding the reliability of their labels based
on the human re-classifications, but since they provided the complete results of
phase 2, I was abe to compute the Fleiss' $\kappa$, a standard metric for estimating
annotator agreement, and thus, the reliability of these labels; the computed value
equals to 0.66, which corresponds to ``substantial'' agreement in the interpretation
scale for th variable, proposed by \cite{landis1977measurement}.


Following are seven example sentence randomly picked out of the deISEAR corpus, one for each emotion.

\begin{examples}
  \item Ich f√ºhlte \textbf{[Traurigkeit]}, als mein Laptop kaputt ging und die Garantie schon abgelaufen war.
  \item Ich f√ºhlte \textbf{[Scham]}, weil mir mal beim Urlaub das Geld ausging.
  \item Ich f√ºhlte \textbf{[Freude]}, als ich mit meinen Arbeitskollegen ohne Ende Witze gerissen habe.
  \item Ich f√ºhlte \textbf{[Angst]}, als der Chef sagte dass Mitarbeiter gek√ºndigt werden m√ºssen.
  \item Ich f√ºhlte \textbf{[Wut]}, als ich die Nachricht gelesen habe, dass der VfB Stuttgart nicht in neue Spieler investieren wird.
  \item Ich f√ºhlte \textbf{[Ekel]}, als ich verschimmeltes Essen im K√ºhlschrank gefunden habe.
  \item Ich f√ºhlte \textbf{[Schuld]}, dass ich meinen besten Kumpel versetzt habe.
\end{examples}

Now it is up to you: Here are four sentences with masked emotions --- try to assign what you thin is the correct one.
The possible emotions are \textbf{Angst, Ekel, Freude, Scham, Schuld, Traurigkeit, Wut}.\myfootnote{\rotatebox{180}{\ref{itm:deisear1}: Freude, \ref{itm:deisear2}: Scham, \ref{itm:deisear3}: Wut, \ref{itm:deisear4}: Angst}}


\begin{examples}
  \item \label{itm:deisear1} Ich f√ºhlte \textbf{[?]}, als ich meine kleine Tochter zum Schwimmen abgeholt habe.
  \item \label{itm:deisear2} Ich f√ºhlte \textbf{[?]}, als meine Mutter mich zur Schule begleiten musste als ich die schule geschw√§nzt hatte
  \item \label{itm:deisear3} Ich f√ºhlte \textbf{[?]}, als die √Ñrzte im KH bei meiner im sterben liegenden Gro√ümutter einen k√ºnstlichen Zugang legen wollten um die Schilddr√ºsenmedikamente zu verabreichen.
  \item \label{itm:deisear4} Ich f√ºhlte \textbf{[?]}, als eine Feuerwerksrakete in Richtung meiner Kinder abgefeuert wurde und mein kleiner weinend davon lief.
\end{examples}

%The searched emotion is \emph{Traurigkeit} in example \ref{ex:deisear}.

%\fig{images/deISEAR_traindevtest.png}{fig:deISEAR-devtest}{Distribution of emotions in the deISEAR data set.}{15}{deISEAR statistics}

% \subsubsection{Statistics}

deISEAR is one of the data sets that are made available without any pre-defined
training/development/test splits. Therefore I shuffle all 1,001 sentences and
split with a 70:15:15 ratio, resulting in a training set of 700, a development
set of 150 and a test set of 151 sentences.

In figure \ref{fig:deisear-stats}, the length of the sentences and the label distributions in the three data sets are
plotted. While the data sets were created randomly, there are some peculiarities observable: The lengths of the sentences
in the training set show a greater variation compared to the development and test set than one would except.
% Further, the one extremely long example present in the whole data set ended also up in the training set --- although this is not
% too unexpected.

% number of subtokens in deISEAR: 18,679
% number of sharps: 2251
% nubmer of [UNK]s: 0

% \textbf{merged} \\
% average length train: 15.9 (sigma 6.6) \\
% average length dev: 17.9 (sigma 19.9) \\
% average length test: 17.1 (sigma 7.4)

% \textbf{subtokenized} \\
% average length train: 18.1 (sigma 7.9) \\
% average length dev: 20.7 (sigma 24.1) \\
% average length test: 19.5 (sigma 8.8)

% \fig{images/deISEAR_subtokenized.pdf}{fig:deISEAR-context}{deISEAR: Length of subtokenized Sentences.}{8}{deISEAR-Length}
% \fig{images/deISEAR_subtokenized_all.pdf}{fig:out-deISEAR-context}{deISEAR: Length of subtokenized Sentences (with one extreme outlier).}{8}{deISEAR-Length}


% \subsubsection{SOTA}

\cite{troiano2019crowdsourcing} train a maximum entropy classifier with L2 regularization with
boolean unigram features on the original ISEAR corpus (7,665 instances).
Since the original ISEAR study and data collection was carried out in English, they then machine
translate the 1,001 deISEAR examples and evaluate on them.
Using this strategy, the authors accomplish an average micro F$_1$ of 47. (Note: micro F$_1$ in settings where each example gets
exactly one label assigned is the same as accuracy)

\subsection{MLQA}

The term question answering subsumes several related tasks or problems: In it's most
general form, question answering refers to the ability to give a meaningful answer
to any possible inquiry. This variety is normally termed \emph{open-domain question
answering} and models addressing this task require a whole pipeline of algorithms in
the background to produce acceptable results (cf. \cite{chen-yih-2020-open}). Another
form is so called \emph{multi-choice question answering}, where the task for the
model is to select the correct answer out of a list of options given the answer (cf.
\cite{welbl2017crowdsourcing}). The subsort of question answering MLQA addresses is
so called \emph{span prediction question answering}. The goal here is to extract the
correct answer span out of a context text given the question.

\cite{lewis2019mlqa} compiled the MLQA data set using Wikipedia articles. First, they
``automatically identify sentences from \textelp{} articles which have the same or
similar meaning in multiple languages.''\myfootnote{However, taking the sometimes huge
contexts into account, I think the better formulation would have been ``paragraphs''
with similar meaning, instead of ``sentences''.} Secondly, they crowdsourced questions
for the English paragraphs, let them humanly translate into the target languages,
and finally annotate the answer spans in the corresponding paragraphs.\myfootnote{If
this was done manually or by implementing some other techniques is, unfortunately,
not reported. The presence of sometimes strange offsets (included commas, missing
prepositions as in example \ref{itm:mlqa-strange-answer-span} etc.) seem to indicate a
not fully hand-made annotation --- at least to me.}

\begin{figure}
  \begin{minipage}{0.45\linewidth}
  \vspace{0pt}
    \includegraphics[width=0.9\linewidth]{images/MLQA_context.pdf}
  \end{minipage}
  \hfill
  \begin{minipage}{0.45\linewidth}
  \vspace{0pt}
    \includegraphics[width=0.9\linewidth]{images/MLQA_answer.pdf}
  \end{minipage}
  \stepcounter{myfigure}
  \caption[MLQA Lengths]{\textbf{Left}: Length of subtokenized MLQA contexts.
                         \textbf{Right}: Length of subtokenized MLQA answers. Note the difference in y-axis scaling between the two plots:
                         The contexts are longer by orders of magnitude to the answer spans. In fact, most answer spans consist of only a handful of
                         (subtokenized) words.}
  \label{fig:mlqa-stats}
\end{figure}

Following are five random examples out of the MLQA data set:

\begin{examples}
  \item \textbf{Context}:
        Rita Sahat√ßiu Ora (* 26. November 1990 in Pri≈°tina, SFR Jugoslawien) ist
        eine britische S√§ngerin und Schauspielerin kosovarischer Herkunft. Von 2010 bis 2016 stand
        sie bei Jay Z und Roc Nation unter Vertrag. Seit 2017 steht sie bei Atlantic Records unter Vertrag.

        \textbf{Question}: Wann wurde Rita Sahat√ßiu Ora geboren?\\
        \textbf{Answer}: 26. November 1990
  \item \textbf{Context}:
        W√§hrend Somalia an milit√§rischer St√§rke gewonnen hatte, wurde √Ñthiopien aufgrund innenpolitischer
        Umst√§nde geschw√§cht. 1974 hatte die Derg-Milit√§rjunta den abessinischen Kaiser Haile Selassie
        gest√ºrzt, sich aber bald in interne Machtk√§mpfe verstrickt, woraufhin es zu Unruhen kam. In
        verschiedenen Landesteilen waren Derg-feindliche und separatistische Kr√§fte aktiv. Das regionale
        Machtgleichgewicht hatte sich zugunsten Somalias verschoben.

      \textbf{Question}: Zu wessen Gunsten verlagerte sich die Balance of Power?\\
      \textbf{Answer}: Somalia
  \item \label{itm:mlqa-strange-answer-span} \textbf{Context}:
        Das Johnston-Atoll verlassend, drehte John nach Nordwesten ab und begann sich erneut zu
        intensivieren, als die Windscherung nachlie√ü. Am 27. August Ortszeit erreichte John einen
        sekund√§ren H√∂hepunkt mit Windgeschwindigkeiten von 210 km/h. Kurz darauf √ºberquerte John
        die Datumsgrenze bei etwa 22¬∞ n√∂rdlicher Breite und gelangte in das Beobachtungsgebiet
        des Joint Typhoon Warning Center (JTWC) auf Guam. Durch seinen Aufenthalt im westlichen
        Pazifischen Ozean wurde Hurrikan John zum Taifun John. Kurz nach dem √ºberschreiten
        der Datumsgrenze schw√§chte sich John wieder ab und die Vorw√§rtsbewegung kam fast zum
        Stillstand. Am 1. September hatte sich Taifun John zum tropischen Sturm abgeschw√§cht und
        ver√§nderte seine Position knapp westlich der Datumsgrenze kaum. Dort blieb der Sturm die
        n√§chsten sechs Tage, w√§hrend der John eine mehrere Tage andauernde Schleife entgegen dem
        Uhrzeigersinn zog, bis am 7. September ein Trog in die Gegend gelangte und John schnell
        nach Nordosten abzog. Am 8. September √ºberquerte John die Datumslinie wieder nach Osten
        und gelangte erneut in den Zentralpazifik.Dort angelangt erreichte John seinen terti√§ren
        H√∂hepunkt mit Windgeschwindigkeiten von 145 km/h als starker Kategorie-1-Hurrikan, ein
        gutes St√ºck n√∂rdlich der Midwayinseln. Der Trog nahm die Struktur von John auseinander
        und das kalte Wasser des n√∂rdlichen Zentralpazifiks tat sein √úbriges. Am 10. September
        wurde die 120. Sturmwarnung zu John ausgegeben, mit der das System als au√üertropisch
        erkl√§rt wurde, etwa 1600 km s√ºdlich von Unalaska.

       \textbf{Question}: Wo wurde John zum Taifun?\\
       \textbf{Answer}: westlichen Pazifischen Ozean
\end{examples}

To demonstrate that this is by no means a trivial task --- at least for us humans ---, try to
identify the correct answer span\myfootnote{\rotatebox{180}{√ºberwiegend antibritisch
und prorussisch}} for the following context-question pair:

\begin{examples}
    \item \textbf{Context}:
        Das britische Parlament genehmigte K√∂nigin Victoria, ihrer Tochter
        als Mitgift 40.000 Britische Pfund (in heutiger Kaufkraft 3.662.803 Pfund) zu
        zahlen und legte die j√§hrliche Apanage der Prinzessin auf 8000 Pfund fest.
        K√∂nig Friedrich Wilhelm IV. gew√§hrte seinem Neffen ein j√§hrliches Einkommen
        von 9000 Talern. Das Einkommen des Prinzen war damit nicht ausreichend, um
        die Kosten eines standesgem√§√üen Haushaltes zu decken, und einen Teil der
        Haushaltskosten w√ºrde zuk√ºnftig Prinzessin Victoria aus ihrem Verm√∂gen tragen
        m√ºssen. Der zuk√ºnftige Hofstaat des jungen Paares wurde von der preu√üischen
        K√∂nigin und der zuk√ºnftigen Schwiegermutter Prinzessin Auguste ausgew√§hlt. Die
        beiden Frauen entschieden sich √ºberwiegend f√ºr Personen, die bereits l√§nger
        im Hofdienst standen und damit deutlich √§lter als das prinzliche Paar waren.
        Prinz Alberts Bitte, seiner Tochter doch wenigstens zwei gleichaltrige und
        britische Hofdamen zu gew√§hren, wurde nicht entsprochen. Als Kompromiss wurden
        mit den Komtessen Walburga von Hohenthal und Marie zu Lynar zwei Hofdamen
        gew√§hlt, die Prinzessin Victoria wenigstens altersm√§√üig entsprachen. Immerhin
        konnte Prinz Albert Ernst von Stockmar, den Sohn seines jahrelangen Beraters
        Christian Friedrich von Stockmar, als pers√∂nlichen Sekret√§r der Prinzessin
        durchsetzen. Prinz Albert, der √ºberzeugt davon war, dass der preu√üische Hof
        die Einheirat einer britischen Prinzessin als Bereicherung und Ehre ans√§he,
        bestand au√üerdem darauf, dass Prinzessin Victoria den Titel einer Princess
        Royal of the United Kingdom of Great Britain and Ireland beibehielt. An dem
        √ºberwiegend antibritisch und prorussisch eingestellten preu√üischen Hof l√∂ste
        dieser Schritt allerdings nur Ver√§rgerung aus.Der Hochzeitsort war Anlass
        f√ºr weitere Meinungsverschiedenheiten. F√ºr das preu√üische K√∂nigshaus war es
        selbstverst√§ndlich, dass ein Prinz, der als zweiter in der Thronfolge stand,
        in Berlin heiratete. Letztlich konnte sich aber K√∂nigin Victoria durchsetzen,
        die als regierende Monarchin f√ºr sich in Anspruch nahm, ihre √§lteste Tochter
        in ihrem Land zu verm√§hlen. Das Paar trat schlie√ülich am 25. Januar 1858 in
        der Kapelle des St James‚Äôs Palace in London vor den Traualtar.

        \textbf{Question}: Was war die Position der Berliner Gerichts gegen√ºber Gro√übritannien und Russland?\\
\end{examples}



% \subsubsection{Statistics}

MLQA is a set that comes with pre-defined development and test splits. The test set
contains 4,499 examples, while the dev set is made up of 509 examples. As for all the
other pre-splitted data sets, a training set is not part of the splits, this has to do
with the approach how \cite{lewis2019mlqa} intend to use their data set, which will
be explained a little further down. In a first set up, I treat the MLQA development
set as training set and split off a 15\% portion of it as development set, resulting
in 432 training instances and 77 development examples. In a second set up I shuffle
the pre-defined splits and create a more suiting 70:15:15 set ratio; resulting in the
following numbers of examples per set: Training 3,506, development 751, test 751.

As can bee seen in the left figure of \ref{fig:mlqa-stats}, MLQA comprises quite a high number
of examples --- 868, to be precise --- which do not fit subtokenized into a normal BERT model.
However, those examples where the answer span lies inside the maximum BERT sequence are simply
cut off after the maximum length and are kept in the data set for the experiments. Only the 22
examples where the answer span lies partially or fully outside the maximum length were dropped.

% all subs in MLQA: 883,394
% sharps: 188,445
% unks: 8,492

% \textbf{merged} \\
% average length train answer: 4.0 (sigma 4.9) \\
% average length dev answer: 3.7 (sigma 5.4) \\
% average length test answer: 4.0 (sigma 5.1)

% average length train question: 9.4 (sigma 3.7) \\
% average length dev question: 8.6 (sigma 3.4) \\
% average length test question: 9.1 (sigma 3.4)

% average length train context: 127.7 (sigma 110.0) \\
% average length dev context: 125.1 (sigma 116.7) \\
% average length test context: 129.9 (sigma 123.1)

% \textbf{subtokenized} \\
% average length train answer: 5.6 (sigma 6.6) \\
% average length dev answer: 5.2 (sigma 6.7) \\
% average length test answer: 5.6 (sigma 7.0)

% average length train question: 11.4 (sigma 4.5) \\
% average length dev question: 10.6 (sigma 4.3) \\
% average length test question: 11.2 (sigma 4.3)

% average length train context: 162.7 (sigma 139.0) \\
% average length dev context: 159.4 (sigma 145.6) \\
% average length test context: 165.5 (sigma 156.7)

% \subsubsection{SOTA}
\label{chap:mlqa-sota}

\citeauthor{lewis2019mlqa} define two tasks tey use to evaluate model performance on
MLQA: The first one, cross-lingual transfer (XLT), means training models in English,
conducting model selection on the development set, and evaluating on the test sets for
the various languages. The second case, generalized cross-lingual transfer (G-XLT), is
not of interest to my thesis, since it involves a mixing of languages: the context is
presented in one language and the question in another. The authors include two models
for the zero-shot transfer approach: multilingual BERT and XLM and use SQuAD (100,00
instances) as training set.

The best results for German in the XLT mode \citeauthor{lewis2019mlqa} reoprt, is a 47.6\% exact
match accuracy, achieved by XLM. However, as the training procedure and amount of traning data
differs quite drastically from my set up, it's not possible to directly compare their benchmark
with my results. Detailed GliBERT results on the GerGLUE question answering tasks are reported
in chapter \ref{chap:5_results}, section \ref{sec:qa-results}.

% The total of all instances in all languages in MLQA is 46,444.



\subsection{PAWS-X}

\begin{wrapfigure}[21]{l}{0.45\linewidth}
  \begin{center}
    \includegraphics[width=0.9\linewidth]{images/PAWS-X_BLEU_scores.pdf}
  \end{center}
  \stepcounter{myfigure}
  \caption[PAWS-X BLEU]{BLEU scores of the PAWS-X data sets. Clearly visible is the diffence between the training set on the one hand,
                      and the development and test set on the other:}
\end{wrapfigure}

The PAWS-X corpus \cite{yang2019paws} was compiled to provide a multilingual source for
training models that address the NLU problem of paraphrase identification. This task
is typically formalized as a binary classification task: given two sentences $S_1$ and
$S_2$, the model must determine whether they convey the same meaning --- performing well
on such a dataset is often taken as an indicator of reliably capturing the semantics
of an utterance \citep{mckeown1980paraphrasing}. Since most corpora for this task are
available only in English the authors compiled this corpus by humanly translate a subset
of the original English PAWS corpus \cite{zhang2019paws}.

Concretely, \citeauthor{yang2019paws} ``translate the Wikipedia portion of the original PAWS
corpus from English'' to six target languages. Out of these, I include the German part into
GerGLUE. These corpora were created by hiring human translators to translate the original
PAWS development and test sets into the six languages, while for the training set a ``neural
machine translation'' system was employed. The following examples are randomly drawn out of
the whole German part of PAWS-X:

\begin{examples}
  \label{ex:paws-x}
  \item Die Familie zog 1972 nach Camp Hill, wo er die Trinity High School in Harrisburg, Pennsylvania, besuchte.

  1972 zog die Familie nach Camp Hill, wo er die Trinity High School in Harrisburg, Pennsylvania, besuchte.

  \textbf{True}
  \item Prestige geh√∂rt der verheirateten Kiribati-Frau an, sie steht jedoch betr√§chtlich unter der Autorit√§t ihres Mannes.

  Die verheiratete Kiribati-Frau ist ein inh√§rentes Prestige, aber sie steht unter der Autorit√§t ihres Mannes.

  \textbf{True}
  \item Die √∂sterreichische Schule geht davon aus, dass die subjektive Entscheidung des Einzelnen, einschlie√ülich des individuellen Wissens, der Zeit, der Erwartungen und anderer subjektiver Faktoren, alle wirtschaftlichen Ph√§nomene verursacht.

  Die √∂sterreichische Schule geht davon aus, dass die subjektive Entscheidung des Einzelnen, einschlie√ülich des subjektiven Wissens, der Zeit, der Erwartung und anderer individueller Faktoren, alle wirtschaftlichen Ph√§nomene verursacht.

  \textbf{False}
  \item "Es ist der vierte Track und die dritte Single aus ihrem Durchbruch ""Smash"" (1994)."

  Es ist der vierte Track und die dritte Single von ihrem Durchbruchalbum `` Smash '' (1994).

  \textbf{True}
  \item Die Mannschaft reagierte auf die √Ñnderungen im n√§chsten Spiel am selben Abend am 19. Februar.

  Die Mannschaft reagierte auf die √Ñnderungen im selben Spiel am n√§chsten Abend des 19. Februars.

  \textbf{False}
\end{examples}


% TODO: new stats figure!
%\fig{}{fig:PAWS-X-devtest}{Distribution of paraphrases (True) versus non-paraphrases (False) in the PAWS-X data set.}{15}{PAWS-X statistics}

You are invited to test yourself and predict if the following sentence pairs are in fact rephrasings
of the same semantic content. The gold labels are in this footnote\myfootnote{\rotatebox{180}{\ref{itm:paws-x1}:
False, \ref{itm:paws-x2}: True}}.


\begin{examples}
  \item \label{itm:paws-x1} Die Single wurde am 12. Oktober 2012 im italienischen Radio Airplay gespielt und am 3. Dezember 2012 weltweit verschickt.

  Die Single wurde am 12. Oktober 2012 nach Italien zu Radio Airplay geschickt und am 3. Dezember 2012 weltweit ver√∂ffentlicht.
  \item \label{itm:paws-x2} Lloyd gr√ºndete und leitete sein Unternehmen, um mit dem Verkauf von Spielzeug und Geschenken zu beginnen, und er erweiterte das House of Lloyd, mit Sitz in Grandview, Missouri, w√§hrend das Gesch√§ft mit den Geschenken wuchs.

  Lloyd gr√ºndete und leitete sein Unternehmen zum Verkauf von Spiel- und Geschenkwaren und erweiterte das in Grandview, Missouri, liegende House of Lloyd mit dem Wachstum des Marktes f√ºr Geschenkwaren.
\end{examples}

% \subsubsection{Preprocessing}

% \subsubsection{Statistics}

The average lengths of the sentences is very regularly distributed with averages of 27.5
($\sigma$ 8.6), 27.7 ($\sigma$ 8.4), and 28.1 ($\sigma$ 8.4) for the subtokenized training,
development, and test sets. Also, the lenght differences between the first and second
sentence in each example is virtually nonexistent, which is not surprising if the creation
of the original PAWS dataset is taken into account: ``Our automatic generation method is
based on two ideas. The first swaps words to generate a sentence pair with the same BOW,
controlled by a language model. The second uses back translation to generate paraphrases
with high BOW overlap but different word order'' \citep{zhang2019paws}.

Really striking about PAWS-X is the enormous difference in examples per set
as depicted in figure \ref{fig:pawsx-stat} --- the training set encompasses
twelve times as many examples as the development and test set combined.

Since the training data are solely machine-translated while the development and test data are
human-translated, there needs to be some clarification as to how differently those sets are.
One measure to capture similarities between sentences is the BLEU score \cite{papineni2002bleu}:
This score measures the overlap of n-grams between two sentences, such that XXX
The BLEU score is a value between 0 (no n-gram overlaps) to 1 (perfect n-gram overlaps), where a
BLEU score of 1 means that the two sentences are identical. As for other measures, like accuracy
e.g., the value is sometimes multiplied by 100 for better readability, which I will also do here.

% \begin{figure}
%   \label{fig:pawsx-stat}
%   \begin{minipage}{0.45\linewidth}
%   \vspace{0pt}
%     \includegraphics[width=0.9\linewidth]{images/PAWS-X_subtokenized_sent1.pdf}
%   \end{minipage}
%   \hfill
%   \begin{minipage}{0.45\linewidth}
%   \vspace{0pt}
%     \includegraphics[width=0.9\linewidth]{images/PAWS-X_subtokenized_sent2.pdf}
%   \end{minipage}
%   \stepcounter{myfigure}
%   \caption[PAWS-X Lengths]{\textbf{Left}: Length of subtokenized PAWS-X first sentences. Note that one extreme outlier in the
%                            traning set comprising 863 (\textit{sic}) BERT-subtokens is not included in the plot.
%                            \textbf{Right}: Length of subtokenized PAWS-X second sentences.}
% \end{figure}

\begin{figure}
  \begin{minipage}{0.45\linewidth}
  \vspace{0pt}
    \includegraphics[width=0.9\linewidth]{images/PAWS-X_num_examples.pdf}
  \end{minipage}
  \hfill
  \begin{minipage}{0.45\linewidth}
  \vspace{0pt}
    \includegraphics[width=0.9\linewidth]{images/PAWS-X_labels.pdf}
  \end{minipage}
  \stepcounter{myfigure}
  \caption[PAWS-X Set Size/Labels]{\textbf{Left}: Number of examples in the PAWS-X split. The automatically translated
                           trainin set contains way more examples than the humanly translated development and test set.
                           \textbf{Right}: Distribution of paraphrase (True) and non-paraphrase (False) examples in the
                           PAWS-X sets.}
  \label{fig:pawsx-stat}
\end{figure}

% all subs in PAWS-X: 2,906,717
% all sharps: 684,155
% all unks: 9,880

% Number of instances: \\
% Train: 48,977 \\
% Dev: 1,932 \\
% Test: 1,967

% \textbf{merged} \\
% average lenght sentence 1 train: 21.0 (sigma 6.5) \\
% average length sentence 2 train: 21.0 (sigma 5.8)

% average length sentence 1 dev: 21.1 (sigma 6.0) \\
% average length sentence 2 dev: 21.1 (sigma 6.0)

% average length sentence 1 test: 21.4 (sigma 5.9) \\
% average length sentence 2 test: 21.3 (sigma 5.9)

% \textbf{subtokenized} \\
% average length sentence 1 train: 27.5 (sigma 9.0) \\
% average length sentence 2 train: 27.4 (sigma 8.2)

% average length sentence 1 dev: 27.6 (sigma 8.4) \\
% average length sentence 2 dev: 27.7 (sigma 8.4)

% average length sentence 1 test: 28.1 (sigma 8.4) \\
% average length sentence 2 test: 28.0 (sigma 8.4)


% identical sentence pairs:

% Train: 3209, wrong labelled: 84

% Dev: 38, wrong labelled: 4

% Test: 27, wrong labelled: 0


The BLEU scores indicate that the sentence pairs in the training set are in tendency much
more similar to each other than in the development and test set. Taken into account how
the data sets were generated, this makes actually sense, however: While the development
and test sets were translated from English to German by humans, the huge training set was
automatically translated. Since the original differences in the sentence pair might well
have been rather subtle, it is no surprise that an algorithm might exhibit difficulties in
grasping those differences; resulting in similar translations for two similar sentences.
Note that due to the difficulties mentioned before, the automatic translation resulted in
3,209 sentence pairs (6.6\% of all the sentence pairs) with a BLEU score of 100.00 in the
training set --- which means they are identical.\myfootnote{I reported this to the authors
of the corpus, but didn't receive an answer from them.}


% \subsubsection{SOTA}

\cite{yang2019paws} achieve their best result --- 89.2\% accuracy for German --- employing the
following model architecture:
They train a multilingual BERT on all languages, including the original English pairs and the
machine-translated data in all other languages and evaluate on the individual languages.

During the preprocessing of this data set, the following considerations are taken into account: In
the predefined development and test splits, there are some examples where one or both sentences
consist only of the string ``NS''. I decided to not include this examples into the data used for
training and evaluating my models, since those examples don't contribute any useful features for
the model.\myfootnote{The authors don't comment on these obscure sentences, so I do not know what
was the reasoning behind including these into the data sets.} Further, some examples consist of
empty strings; I treat those the same way as the examples mentioned before. Upon non-systematic
manual inspection, there were indications of problematic data in PAWS-X, which will be examined in
more detail rigor in chapter \ref{chap:5_results}, section \ref{sec:glinoise}.


\subsection{SCARE}

\begin{wrapfigure}[24]{r}{0.45\linewidth}
  \centering
    \includegraphics[width=0.9\linewidth]{images/SCARE_subtokenized.pdf}
  \stepcounter{myfigure}
  \label{fig:scare-subtokenized}
  \caption[SCARE Lengths]{Length of subtokenized SCARE reviews. Mostly, the reviews are rather
    short, with an average of 25.29 subtokens over all sets, but there is quite a number of
    outliers --- indicating further that it is quite a heterogeneous data set, also concerning
    this aspect. Note that there is one extreme outlier in the train set, comprising 452 BERT
    subtokens, which is not included in the figure.}
\end{wrapfigure}

SCARE is a dataset assembled for sentiment analysis, i.e. the task of assessing the
transmitted emotion of an utterance. The data collected for the \textbf{S}entiment
\textbf{C}orpus of \textbf{A}pp \textbf{Re}views stems, as the name suggests, from
product reviews in the Google Play Store in German. \cite{sanger2016scare} describe
the language contained in the dataset the following way:

\begin{quote}
  Unlike product reviews of other domains, e.g. household appliances, consumer electronics or
  movies, application reviews offer a couple of peculiarities which deserve special treatment:
  The way in which users express their opinion in app reviews is shorter and more concise than
  in other product reviews. Moreover, due to the frequent use of colloquial words and a flexible
  use of grammar, app reviews can be considered to be more similiar [sic] to Twitter messages
  (‚ÄúTweets‚Äù) than reviews of products from other domains or platforms \textelp{}.
\end{quote}

In contrast to other datasets, e.g. \citep{socher2013recursive, go2009twitter},
that attributes one sentiment label to a whole text (may it be a review, a tweet,
etc.), \cite{sanger2016scare} annotated their dataset on an aspect-based manner:
Not each review gets labelled for a certain polarity --- i.e. \emph{positive},
\emph{negative}, or \emph{neutral} --- but what the authors call \emph{aspects} and
correlating \emph{subjective phrases}. An aspect is a category, feature, or
topic that is being talked about: It may be the application itself, parts of the
application, a feature request regarding the application, etc. A subjective phrase
``express[es] opinions and statements of a personal evaluation regarding the app or
a part of it, that are not based on (objective) facts but on individual opinions of
the reviewers'' \citep[p.~1116]{sanger2016scare}. In other words, aspects are facts
about the App and subjective phrases are user opinions regarding them.

This fine level of annotations leads often to several annotations per review, the sentiment of
which may not always match.
As illustration, consider the following review:

\begin{examples}
  \label{ex:fine-grained-anno}
  \item guter wecker... \textbar\textbar\ vom prinzip her echt gut...aber grade was die sprachausgabe betrifft noch etwas buggy....\myfootnote{The ``\textbar\textbar'' denotes that the text left of it is the user given ``title'' of the review, and the part on the right is the actual review.}
\end{examples}

There are the following annotations for the aspects and their corresponding subjective
phrases annotated in SCARE (aspects are bold, the subjective phrase is italic and the
polarity is normal):

\begin{itemize}
  \item \textbf{Wecker}, \textit{guter} $\rightarrow$ positive
  \item \textbf{Prinzip}, \textit{echt gut} $\rightarrow$ positive
  \item \textbf{Sprachausgabe}, \textit{etwas buggy} $\rightarrow$ negative
\end{itemize}

As is clear from this example, in a given review there may be several aspects with a corresponding
subjective phrase per review. It is well possible, as in the provided example, that the sentiment
of these is not always the same.

% Example from .txt file:

% \tab{tab:scare-txt-example}{An example from the alarm\_clocks.txt file.}{
%     \begin{tabularx}{\textwidth}{lX}
%       ID       & Text \\
%       \hline
%       7000     & Alles wieder ok, das Update funktioniert wieder \\
%       7001     & Echt super. \textbar\textbar\ Sch√∂nes, und vor allem einzigartiges interface, wirklich klasse. Sch√∂n w√§re noch, wenn man eigene lieder als klingelt√∂ne einstellen k√∂nnte. \\
%       7002     & Ein sicherer Start in den Tag \\
%       7003     & timely wecker \textbar\textbar\ Einfach nur top \\
%       7004     & Super, aber \textbar\textbar\ √§ndert klingeltonlautst√§rke. Nexus S Android 4.0.3 \\
%       7005     & Wecker \textbar\textbar\ Wirklich gelungene app, tadellos! \\
%       7006     & Sehr sch√∂ne UI und Optik... \textbar\textbar\ Eine Bereicherung auf voller L√§nge... 5-Sterne ***** und da√ü gerne. \\
%       7007     & Akkuverbrauch zu hoch \textbar\textbar\ Wenn die app l√§uft dann ist der akku meines Note3 in ein paar stunden leer. \\
%       7008     & NSA APP? \textbar\textbar\ Innerhalb 2 Wochen 150MB an Hintergrunddaten?! Was wird da gesendet??? \\
%       7009     & Ist halt n Wecker
%     \end{tabularx}
% }{Example SCARE .txt}



% Example from .csv file:

% \tab{tab:scare-csv-example}{An example from the alarm\_clocks.csv file.}{
%   % or better \scalebox since landscape ?
%   % \resizebox{\textwidth}{!}{
%     \begin{tabular}{llllllll}
%       Class      & ID   & Left & Right & Text                   & Aspect- / Subj-ID & Polarity & Relation  \\
%       \hline
%       subjective & 7000 & 0    & 15    & Alles wieder ok        & 7000-subjective2  & Positive & Related \\
%       aspect     & 7000 & 21   & 27    & Update                 & 7000-aspect1      & Neutral  & Related \\
%       subjective & 7000 & 28   & 40    & funktioniert           & 7000-subjective1  & Positive & Related \\
%       subjective & 7001 & 0    & 10    & Echt super             & 7001-subjective5  & Positive & Related \\
%       subjective & 7001 & 15   & 22    & Sch√∂nes                & 7001-subjective4  & Positive & Related \\
%       subjective & 7001 & 38   & 51    & einzigartiges          & 7001-subjective3  & Positive & Related \\
%       aspect     & 7001 & 52   & 61    & interface              & 7001-aspect2      & Neutral  & Related \\
%       subjective & 7001 & 63   & 78    & wirklich klasse        & 7001-subjective2  & Positive & Related \\
%       subjective & 7001 & 80   & 90    & Sch√∂n w√§re             & 7001-subjective1  & Negative & Related \\
%       aspect     & 7001 & 113  & 135   & lieder als klingelt√∂ne & 7001-aspect1      & Neutral  & Foreign
%     \end{tabular}
%   % }
% }{Example SCARE .csv}

% Corresponding .rel file:

% \tab{tab:rel-scare-rel-example}{An example from the alarm\_clocks.rel file.}
% {\begin{tabular}{lllll}
%   Relation-ID & Aspect-ID    & Subj-ID          & Aspect-String          & Subj-String \\
%   \hline
%   7000        & 7000-aspect1 & 7000-subjective1 & Update                 & funktioniert \\
%   7001        & 7001-aspect2 & 7001-subjective4 & interface              & Sch√∂nes \\
%   7001        & 7001-aspect2 & 7001-subjective3 & interface              & einzigartiges \\
%   7001        & 7001-aspect1 & 7001-subjective1 & lieder als klingelt√∂ne & Sch√∂n w√§re
% \end{tabular}
% }{Example SCARE .rel}


% stats: there are 1,760 fine-grained annotated reviews

% Baseline concerning imbalance labels: Always predicting majority class (``Positive'') results
% in accuracy of 59.09\%.

% all subtokens in SCARE: 44,511
% all sharps: 9,163
% all unks: 92

% \subsubsection{SCARE reviews}

% Besides their carefully, hand-annotated corpus, the authors also provide a dataset comprising of
% 802,860 reviews along with the rating --- one to five stars ---, that were available in German on
% the Google Play Store.
% This data set is much larger than the annotated one: Due to the great expenses of generating those
% fine-grained annotations, the authors were able to annotate only 0.22\% of all reviews available.

For integrating the SCARE corpus into my GerBLUE corpus, I need to prepare the data, so it
can be handled by the model architecture; following the original GLUE sentiment task, the
model needs only to predict one sentiment label for each example. Since there exist mostly
multiple annotations for each review in this data set, the data needs to be pre-processed
in a way, so that there is one review-label per example.

To generate the review-label, I carry out a majority class decision:
The polarity that is most often annotated for a given review is also the review-label; in the
example above comprising three aspect-subjective polarities, two of them positive, one negative,
the whole example would be labelled as positive.
If there is no majority label, the review-label is set to ``neutral''.
This is also the chosen strategy for 51 reviews that had no labels at all; an example of such a
review is the following one:

\begin{examples}
  \item Ich bin die erfuinderin \textbar \textbar\ Ich bin die erfunden!!!!!!!!!!!!!!!!!!!!!!!!!
\end{examples}

Figure \ref{fig:scare-fig:scare-stats} shows the outcome of this preprocessing and
the confidence of the such generated labels for the examples. Following are 5 sentences with their
generated labels --- as you can see, at least for a human judge, the labels seem to capture the
overall sentiment of the sentences quite good.


\begin{examples}
  \item Ganz okay \textbar\textbar\ Hatte ein Problem mit der APP aber die updates neu installiert und jetzt gehts wieder vorl√§ufig mal Und Ordner w√§ren sch√∂n wenn man diese erstellen kann

        \textbf{Neutral}
  \item Ssssereeehhhr gut

        \textbf{Positive}
  \item Wie kann man so eine gute app machen und dann nicht auf wvga anpassen. Weg mit den matschtexturen und vor allem dem Icon x-(

        \textbf{Negative}
  \item spitze \textbar\textbar\ Daran sollte sich MS ein Beispiel nehmen!

        \textbf{Positive}
  \item L√§uft nicht auf dem Acer A500 \textbar\textbar\ St√ºrzt leider immer beim Abspielen eines Videos ab. Honeycomb 3.2

        \textbf{Negative}
\end{examples}

Following are three examples to test for yourself with the gold labels in this
footnote\myfootnote{\rotatebox{180}{\ref{itm:scare1}: Negative, \ref{itm:scare2}:
Positive, \ref{itm:scare3}: Positive}}.

\begin{examples}
  \item \label{itm:scare1} F√ºr android einfach schwach || Ich habe die app, weil ich selbst im verein spiele!app st√ºrzt sehr oft ab,ob jetzt kurz nach starten der app oder mitten beim suchen kommt die meldung fussball.de angehalten und man bekommt die m√∂glichkeit ein bericht zu senden oder auf ok zu dr√ºcken um nochmal die app zu starten einfach schwach.die 3 sterne sind eigentlich auch zu viel aber weil wenigstens kleine updates kommen,die zwar nicht was √§ndern wollen wir mal nicht so sein.
  \item \label{itm:scare2} For Free is ok || Es fehlen Hausnummern...also scheint die Genauigkeit der Karten nicht absolut zu sein...weiteres nach l√§ngerer Testzeit
  \item \label{itm:scare3} Alle Funktionen, die .... || ... ich als Langschl√§fer brauche.
\end{examples}

Analyzing the label distributions after turning the original aspect-based SCARE into an emotion detection dataset
reveals that the positive class amasses the most examples on itself (1,071), while the neutral class only comprises
193 items out of a total of 1,760 examples (cf. figure \ref{fig:scare-stats}). However, as the analysis of the
GliBERT predictions in chapter \ref{chap:results} discloses, the models do not simply favour the majority class.

\begin{figure}
  \begin{minipage}{0.45\linewidth}
  \vspace{0pt}
    \includegraphics[width=0.9\linewidth]{images/SCARE_labels.pdf}
  \end{minipage}
  \hfill
  \begin{minipage}{0.45\linewidth}
  \vspace{0pt}
    \includegraphics[width=0.9\linewidth]{images/SCARE_label_stats.pdf}
  \end{minipage}
  \stepcounter{myfigure}
  \caption[Accumulated Gains and Losses.]{\textbf{Left}: Number of examples per label after heuristically computing them in the SCARE dataset. {\color{red} WRITE MORE ABOUT IMBALANCE, WHAT TO DO ABOUT IT, COMPUTE F1, ETC} \textbf{Right}: Statistics of label generation. For most of the examples, there was a clear majority decision as to which label should be chosen. \emph{Close Majority} means the majority vote was off by 1. The \emph{No Majority}/\emph{No Labels} portions in the graph were labelled \emph{neutral} by default, while \emph{Clear Majority}/\emph{Close Majority}  were labelled according to the majority vote decision.}
  \label{fig:scare-stats}
\end{figure}


One could argue that the \texttt{title \textbar \textbar\ comment} structure of the reviews could be encoded
as sentence pair, in the style of PAWS-X and XNLI. But by examining the data, a lot of reviews seemed to
be of the following structure:

\begin{examples}
  \item Erkennt Drucker nicht... \textbar \textbar\ ...,wenn er als Netzwerkdrucker an der FritzBox h√§ngt, und eine manuelle Konfiguration √ºber IP-Adresse ist nicht m√∂glich.
\end{examples}

i.e. that the title and the text of the review form one continuous sequence. Nevertheless, the
separator symbol arguably ``\textbar \textbar'' introduces some noise, especially for the ParZu
parser, which needs to parse the sequence.


% \subsubsection{Statistics}

SCARE is a rather small dataset with a total of 1,760 examples. Since the dataset comes
without predefined splits, I split it regularly into a 70\% training, 15\% development,
and 15\% test set. As \citeauthor{sanger2016scare} stated in the passage quoted at the
beginning of the description of this section, online reviews tend to be rather short,
colloquial text snippets, which is confirmed by plotting the number of BERT subtokens in
figure \ref{fig:fig:scare-subtokenized}. However, since the texts contain many typos,
doublings of characters, and other distortions, the number of ``actual'' words is
considerably lower on average than the number of BERT subtokens: Take a sentence such
as ``Unterirdisch, laaaaangsaaaaam mit WLAN seid neuestem nur'' which would be subtokenized by German BERT as:

[``Unter'', ``\#\#ird'', ``\#\#isch'', ``,'', ``la'', ``\#\#aa'', ``\#\#aa'', ``\#\#n'', ``\#\#gs'', ``\#\#aa'', ``\#\#aa'', ``\#\#am'', ``mit'', ``W'', ``\#\#LA'', ``\#\#N'', ``sei'', ``\#\#d'', ``neues'', ``\#\#tem'', ``nur'']

--- a list of 21 subtokens compared to the de facto 8 tokens present in the sentence.

% Number of examples: \\
% Train: 1,232 \\
% Dev: 264 \\
% Test: 264

% \textbf{merged} \\
% average length train: 20.2 (sigma 21.6)

% average length dev: 19.2 (sigma 19.1)

% average length test: 20.6 (sigma 20.0)

% \textbf{subtokenized} \\

% average length train: 25.4 (sigma 28.2)

% average length dev: 24.0 (sigma 23.1)

% average length test: 26.1 (sigma 25.9)

% \subsubsection{SOTA}

\cite{sanger2016scare} don't predict a sentiment for each instance, but predict fine-grained
aspect and subjective phrase spans using a CRF-based model.
They report results for exact matches as well as partial matches.
For the aspects, they achieve an exact match F1 score of 62\% and 63\% for subjective phrases, respectively.
Since predicting fine-grained aspect and subjective phrase spans is much more difficult than
extrapolating an overall sentiment of the same utterance, a comparison between the outcomes of the
two tasks are not really comparable.
% Furthermore,


\subsection{XNLI}

The name XNLI indicates already what kind of NLU problem this dataset is targeted at:
\textbf{N}atural \textbf{L}anguage \textbf{I}nference is the task of determining whether
a hypothesis $h$ can reasonably be inferred from a premise $p$, i.e. if $h$ ``follows''
from $p$. It must be emphasized that although the wording suggests it, NLI is quite
different from formal logic deduction in that its emphasis ``is on informal reasoning,
lexical semantic knowledge, and variability of linguistic expression, rather than on
long chains of formal reasoning'' \citep{maccartney2009natural}.

\cite{conneau2018xnli} built the XNLI corpus by employing professional translators to translate
7,500 English sentence pairs from the Multi-Genre Natural Language Inference (MultiNLI) corpus
\cite{williams2017broad} into fifteen languages. First, they randomly sample 750 examples from each
of the ten text source used in MultiNLI, which is in English, and then let the same MultiNLI worker
pool generate three hypothesis for each sentence, one for each possible label (\emph{entailment},
\emph{contradicion}, \emph{neutral}). Each sentence pair was then assigned a gold label that was
retrieved by carrying out a majority vote between the label that was assigned by the person who
created the hypothesis and the labels that were assigned independently to the sentence pair by
four other people. Finally, all the sentence pairs were translated into the different languages by
translators. In addition, \cite{conneau2018xnli} carry out some tests to verify that the original
gold label still holds in the translated sentences: They recruited two bilingual annotators to
reevaluate 100 examples in English and French, i.e. they had to re-assign the labels given the
sentence pairs. For the English examples, \citeauthor{conneau2018xnli} find a 85\% consensus on the
gold labels, and for French a corresponding 83\%, from which the authors conclude that the overall
semantic relationship between the two languages has been preserved.

\begin{figure}
  \begin{minipage}{0.45\linewidth}
  \vspace{0pt}
    \includegraphics[width=0.9\linewidth]{images/XNLI_premise_subtokenized.pdf}
  \end{minipage}
  \hfill
  \begin{minipage}{0.45\linewidth}
  \vspace{0pt}
    \includegraphics[width=0.9\linewidth]{images/XNLI_hypo_subtokenized.pdf}
  \end{minipage}
  \stepcounter{myfigure}
  \caption[XNLI Lengths]{\textbf{Left}: Length of subtokenized XNLI premises. \textbf{Right}:
    Length of subtokenized XNLI hypotheses. While the lengths between the sets do not vary greatly,
    the hypotheses are significantly shorter than the premises.}
  \label{fig:xnli-subtokenized}
\end{figure}

Here are some randomly sampled German XNLI hypothesis-premis sentence pairs with the
corresponding gold labels.

\begin{examples}
  \label{ex:xnli}
  \item Ich wusste nicht was ich vorhatte oder so, ich musste mich an einen bestimmten Ort in Washington melden.

        Ich war noch nie in Washington, deshalb habe ich mich auf der Suche nach dem Ort verirrt, als ich dahin entsandt wurde.

        \textbf{Neutral}
  \item Nat√ºrlich haben sie mich dort gefragt, warum ich ging.

        Sie fragten, warum ich in den Laden ging.

        \textbf{Neutral}
  \item Und ich dachte OK und das war es dann!

        Nachdem ich ja gesagt hatte, endete es.

        \textbf{Entailment}
  \item John Burke (Alabama) √ºberpr√ºft und analysiert andere zeitgen√∂ssische Konten und findet, dass Boswells nicht nur der genaueste ist, sondern er nutzt es, um Johnsons Charakter zu demonstrieren, wobei andere es lediglich als literarischen Geschw√§tz abstempeln.

        John Burke ignoriert Aussagen.

        \textbf{Contradiction}
  \item Die √∂ffentliche Bibliothek in Greenlee County, Arizona, zeigt die finanziellen und technologischen Probleme von l√§ndlichen Einrichtungen auf.

        Greenlee County hat eine √∂ffentliche Bibliothek.

        \textbf{Entailment}
\end{examples}

As for the datasets before, here are some examples to test for yourself with the answers in
this footnote\myfootnote{\rotatebox{180}{\ref{itm:xnli1}: Neutral, \ref{itm:xnli2} Entailment,
\ref{itm:xnli3} Neutral}}.

\begin{examples}
  \item \label{itm:xnli1} Er kommt aus Griechenland und er kommt aus einem kleinen Dorf in Griechenland namens Tokaleka und er kam nach Amerika und ich glaube es war 1969 oder 1970 und er heiratete kurz darauf.

        Er ist ein griechischer Mann, der kein Englisch spricht.
  \item \label{itm:xnli2} Suchen Sie nach Emily Dickinson's kommendem Gedicht, alles was ich wirklich von Poesie wissen muss habe ich bein Microsoft gelernt.

        Dickinson schrieb Gedichte.
  \item \label{itm:xnli3} ‚ÄûSo hat es auch in den kubanischen Tropen vor Kurzem einen Tag gegeben, der so sch√∂n wie Ruhm und kalt wie ein Grabstein war.‚Äú

        Es ist immer √ºber 80 in Kuba.
\end{examples}
%The label for example \ref{ex:xnli} is \emph{neutral} since the second sentence does not follow necessarily from the first and it also does not contradict it, either.

% \subsubsection{Statistics}

XNLI is a medium sized dataset in the context of GerGLUE: It comprises a total of 7,498 examples,
predefined in a development and test set of 2,489 and 5,009 examples, respectively. From the
development set, there are a further 2,115 examples split off as training set, leading to a very
small dev set of only 374 examples. As will be seen afterwards, this is of course not an ideal set
up --- therefore I resample the complete XNLI data set, split it into normal sets (5,248, 1,125,
and 1,125 examples, respectively), compute a second round of experiments on this splits and
obtain indeed better results.

\begin{wrapfigure}[20]{r}{0.45\linewidth}
  \begin{center}
    \includegraphics[width=1.0\linewidth]{images/XNLI_labels.pdf}
  \end{center}
  \stepcounter{myfigure}
  \caption[XNLI labels]{Label distributions of the XNLI data set; apparently, the three classes are very well balanced.}
\end{wrapfigure}

The lenghts of premises and hypothesis reported in figure \ref{fig:xnli-subtokenized} confirm the
impression from the sample sentences that hypothesis are significantly shorter than premises;
this is not such a surprise considering the construction of the dataset where people were asked
to produce hypotheses given a certain premise --- if given such a task one would come
up naturally with a comparatively short, concise question.


% Number of Examples: \\
% Train: 2,115 \\
% Dev: 374 \\
% Test: 5,009

% all subs: 290,826
% sharps: 52,457
% unks: 115

% \textbf{merged} \\
% average length premise train: 20.8 (sigma 9.4) \\
% average length hypothesis train: 10.5 (sigma 4.0)

% average length premise dev: 20.9 (sigma 9.1) \\
% average length hypothesis dev: 11.9 (sigma 4.9)

% average length premise test: 21.2 (sigma 9.6) \\
% average length hypothesis test: 10.7 (sigma 4.1)


% \textbf{subtokenized} \\
% average length premise train: 25.8 (sigma 11.9) \\
% average length hypothesis train: 12.4 (sigma 4.7)

% average length premise dev: 26.3 (sigma 12.1) \\
% average length hypothesis dev: 14.3 (sigma 6.0)

% average length premise test: 26.1 (sigma 12.0) \\
% average length hypothesis test: 12.8 (sigma 5.0)


% \fig{images/XNLI_premise_subtokenized.pdf}{fig:XNLI-premise-length}{XNLI: Length of subtokenized premises.}{8}{XNLI-Length}

% In contrary to the above described PAWS-X corpus, there are no identical sentence pairs in XNLI.

% \subsubsection{SOTA}

The best system \cite{conneau2018xnli} report for German on their XNLI data set is a model that
relies heavily on translation: They train a BiLSTM on the MultiNLI data (432,702 instances) and
translate the test set of the given language to English and predict on this data. Employing this
strategy, the authors obtain an accuracy on the German test set of 68.7\%.



\subsection{XQuAD}

Like MLQA, XQuAD \citep{artetxe2019cross} is a question answering dataset, where an answer span
needs to be identified given a context and a corresponding question. In contrast to MLQA, where the
contexts are text sequences from Wikipedia articles, XQuAD was constructed by humanly translate 240
paragraphs and 1190 question-answer pairs from SQuAD 1.1 \citep{rajpurkar2016squad}.

% ``XQuAD consists of a subset of 240 paragraphs and 1190 question-answer pairs from the development
% set of SQuAD v1.1 together with their translations into ten languages \textelp{} In order to
% facilitate easy annotations of answer spans, we choose the most frequent answer for each question
% and mark its beginning and end in the context paragraph using special symbols, instructing
% translators to keep these symbols in the relevant positions in their translations''
% \cite{artetxe2019cross}.

\begin{examples}
  \label{ex:xquad}
  \item \textbf{Context}:
        Aristoteles lieferte eine philosophische Diskussion √ºber das Konzept einer Kraft
        als integraler Bestandteil der aristotelischen Kosmologie. Nach Ansicht von Aristoteles enth√§lt
        die irdische Sph√§re vier Elemente, die an verschiedenen ‚Äûnat√ºrlichen Orten‚Äú darin zur Ruhe
        kommen. Aristoteles glaubte, dass bewegungslose Objekte auf der Erde, die haupts√§chlich aus den
        Elementen Erde und Wasser bestehen, an ihrem nat√ºrlichen Ort auf dem Boden liegen und dass sie so
        bleiben w√ºrden, wenn man sie in Ruhe l√§sst. Er unterschied zwischen der angeborenen Tendenz von
        Objekten, ihren ‚Äûnat√ºrlichen Ort‚Äú zu finden (z. B. dass schwere K√∂rper fallen), was eine
        ‚Äûnat√ºrliche Bewegung‚Äú darstellt und unnat√ºrlichen oder erzwungenen Bewegungen, die den
        fortw√§hrenden Einsatz einer Kraft erfordern. Diese Theorie, die auf der allt√§glichen Erfahrung
        basiert, wie sich Objekte bewegen, wie z. B. die st√§ndige Anwendung einer Kraft, die erforderlich
        ist, um einen Wagen in Bewegung zu halten, hatte konzeptionelle Schwierigkeiten, das Verhalten von
        Projektilen, wie beispielsweise den Flug von Pfeilen, zu erkl√§ren. Der Ort, an dem der
        Bogensch√ºtze den Pfeil bewegt, liegt am Anfang des Fluges und w√§hrend der Pfeil durch die Luft
        gleitet, wirkt keine erkennbare effiziente Ursache darauf ein. Aristoteles war sich dieses
        Problems bewusst und vermutete, dass die durch den Flugweg des Projektils verdr√§ngte Luft das
        Projektil zu seinem Ziel tr√§gt. Diese Erkl√§rung erfordert ein Kontinuum wie Luft zur
        Ver√§nderung des Ortes im Allgemeinen.

        \textbf{Question}: Wer leitete eine philosophische Diskussion √ºber Kraft?\\
        \textbf{Answer}: Aristoteles

        \textbf{Question}: Wovon war das Konzept der Kraft ein integraler Bestandteil?\\
        \textbf{Answer}: aristotelischen Kosmologie

        \textbf{Question}: Aus wie vielen Elementen besteht die irdische Sph√§re nach Ansicht des Aristoteles?\\
        \textbf{Answer}: vier

        \textbf{Question}: Wo vermutete Aristoteles den nat√ºrlichen Ort f√ºr Erd- und Wasserelemente?\\
        \textbf{Answer}: auf dem Boden

        \textbf{Question}: Was bezeichnete Aristoteles als erzwungene Bewegung?\\
        \textbf{Answer}: unnat√ºrlichen
\end{examples}

% \subsubsection{Statistics}

XQuAD is also one of the smaller datasets in the GerGLUE corpus --- a total
of 1,179 examples, without predefined splits, which results in training, development, and test
splits of 820, 181, and 178 examples each. As for MLQA, the answer spans often consists of
only a handful of subtokens, due to the fact of many questions asking for only a certain
name or place mentioned in the context.

% Nxamples: \\
% Train: 820
% Dev: 181
% Test: 178

% all subs in XQuAD: 238,944
% sharps: 51,154
% unks: 2,143

\begin{figure}
  \begin{minipage}{0.45\linewidth}
  \vspace{0pt}
    \includegraphics[width=0.9\linewidth]{images/XQuAD_context.pdf}
  \end{minipage}
  \hfill
  \begin{minipage}{0.45\linewidth}
  \vspace{0pt}
    \includegraphics[width=0.9\linewidth]{images/XQuAD_answer.pdf}
  \end{minipage}
  \stepcounter{myfigure}
  \caption[XQuAD Lengths]{\textbf{Left}: Length of subtokenized XQuAD contexts.
                         \textbf{Right}: Length of subtokenized XQuAD answers. Note the difference in y-axis scaling between the two plots.}
  \label{fig:xquad-stats}
\end{figure}

Again,you can test you own performance on this dataset by answering the following questions relating
to the given context:

\begin{examples}
  \item \label{itm:xquad} \textbf{Context}: Da sowohl Pr√§sident Kenyatta als auch Vizepr√§sident William Ruto
        2013 Gerichtstermine vor dem Internationalen Strafgerichtshof in Verbindung mit den
        Auswirkungen der Wahl von 2007 hatten, entschied sich US-Pr√§sident Barack Obama,
        das Land w√§hrend seiner Afrikareise Mitte 2013 nicht zu besuchen. Sp√§ter in diesem
        Sommer besuchte Kenyatta auf Einladung von Pr√§sident Xi Jinping China, nachdem er
        einen Zwischenstopp in Russland eingelegt und die USA als Pr√§sident nicht besucht
        hatte. Im Juli 2015 besuchte Obama Kenia als erster amerikanischer Pr√§sident, der
        das Land w√§hrend seiner Regierungszeit bereiste.

        \textbf{Question}: Welches Land besuchte Kenyatta auf Einladung des Pr√§sidenten?\myfootnote{\rotatebox{180}{China}}

        \textbf{Question}: Wann besuchte Obama Kenia schlie√ülich?\myfootnote{\rotatebox{180}{Juli 2015}}

        \textbf{Question}: Wer entschied sich, das Land 2013 nicht zu besuchen?\myfootnote{\rotatebox{180}{US-Pr√§sident Barack Obama}}

        \textbf{Question}: Was war das Ergebnis der Wahl von 2007?\myfootnote{\rotatebox{180}{Gerichtstermine vor dem Internationalen Strafgerichtshof}}
\end{examples}

% \textbf{merged}
% average length train answer: 3.3 (sigma 3.2) \\
% average length dev answer: 3.4 (sigma 3.4) \\
% average length test answer: 3.0 (sigma 3.3)

% average length train context: 147.3 (sigma 68.7) \\
% average length dev context: 151.7 (sigma 74.1) \\
% average length test context: 162.0 (94.8)

% average length train question: 11.2 (sigma 3.8) \\
% average length dev question: 11.9 (sigma 4.3) \\
% average length test question: 11.0 (sigma 4.0)

% \textbf{subtokenized}
% average length train answer: 5.0 (sigma 4.5) \\
% average length dev answer: 5.0 (sigma 4.8) \\
% average length test answer: 4.5 (sigma 4.4)

% average length train context: 187.9 (85.8) \\
% average length dev context: 192.8 (sigma 90.1) \\
% average length test context: 205.1 (sigma 113.0)

% average length train question: 14.1 (sigma 4.9) \\
% average length dev question: 15.3 (sigma 5.3) \\
% average length test question: 13.9 (sigma 4.9)


% \subsubsection{SOTA}

As benchamrk results on XQuAD, \citeauthor{artetxe2019cross} employ a very peculiar architecture
that consists in re-training a monolingual English BERT model on Wikipedia and transfer it to the
target languages, following these steps:

\begin{enumerate}
    \item Pre-train a monolingual BERT in English with original pretraining objectives
    \item Transfer model to new language $L_2$, but learn only token embeddings new (transformer body is frozen) with original pretraining objectives
    \item Fine-tune transformer for downstream task in English (transformer body is freezed)
    \item Zero-shot transfer this model to $L_2$ by swapping the English token embeddings with the $L_2$ embeddings
\end{enumerate}

The authors report the following results for the German part of XQuAD:
F1: 73.6
Accuracy (exact match): 57.6\%

\subsection{Summary}

In the following table \ref{tab:gerglue-overview} an overview of all the datasets is given;
the NLP and corresponding ML tasks the dataset is designed for, the numbers of examples of
the splits and nature of the splits (predefined or not), the style or register of the text
present in the dataset, as well as some important distinctiveness of each dataset. Note that
the ordering is not strictly alphabetical, but follows the following grouping: First, come the
single sentence classification tasks, then follow the sentence pair classification task,
and lastly come the question answering tasks. This order --- classification tasks together,
question answering tasks together --- will due conceptual reasons also be adhered to in the
rest of the thesis.

% In conclusion, I will point the reader to the following properties of GerGLUE datasets to keep in mind:



\begin{landscape}

\tab{tab:gerglue-overview}{Overview of \textit{GerGLUE} data sets and tasks. The number of examples represent the size of the
                            set splits after preparing the data sets for the experiment; therefore all datasets have all splits.
                            Note that during the preprocessing some examples had to be excluded (see chapter \ref{chap:3_datasets}
                            for more details), that's why some numbers do not coincide with those of table \ref{tab:overview-data-sets}.}
{\begin{tabularx}{\linewidth}{ll|lllllX}
  \multicolumn{8}{c}{\large \textbf{GerGLUE}}\\ \\
  \multicolumn{2}{c}{}                                                            & \multirowcell{2}{NLP Task}  & \multirowcell{2}{ML Task}   & \multirowcell{2}{\# Examples\\{\tiny Train/Dev/Test}} & \multirowcell{2}{Predefined Splits} & \multirowcell{2}{Register\\{\tiny coll. $<$ mix. $<$ form.}} & \multirowcell{2}{Remarks}                                    \\ \\ \toprule
  \multirow{5}{*}{\rotatebox[origin=c]{90}{\textit{Single}}} &                    & \multicolumn{6}{g}{\textit{Classification}}\\
                                                             &  deISEAR           & Emotion Detection           & Multi-Class                 & 700/150/151                                          & -                                   & \multicolumn{1}{c}{mixed}                                    & Boilerplate text structures (``Ich f√ºhlte [?], als ...'')    \\
                                                             & SCARE              & Sentiment Analysis          & Multi-Class                 & 1,232/264/264                                         & -                                   & \multicolumn{1}{c}{colloquial}                               & Very informal, ungrammatical, and often short text snippets            \\ \cline{1-2}
  \multirow{8}{*}{\rotatebox[origin=c]{90}{\textit{Pair}}}   & PAWS-X             & Paraphrase Identification   & Binary                      & 48,977/1,932/1,967                                    & Train/Dev/Test                      & \multicolumn{1}{c}{formal}                                   & Translation artifact noise, dev/test splits OOD to train     \\
                                                             & XNLI               & Natural Language Inference  & Multi-Class                 & 2,115/374/5,009                                       & Dev/Test                            & \multicolumn{1}{c}{mixed}                                    & Translation artifact noise, language from different domains  \\ \cline{3-8}
                                                             &                    & \multicolumn{6}{g}{\textit{Question Answering}}\\
                                                             & MLQA               & Question Answering          & Span Prediction             & 432/77/4,499                                          & Dev/Test                            & \multicolumn{1}{c}{formal}                                   & Highly imbalanced splits regarding number of examples        \\
                                                             & XQuAD              & Question Answering          & Span Prediction             & 820/181/178                                           & -                                   & \multicolumn{1}{c}{formal}                                   & Small data set
\end{tabularx}
}{Summary GerGLUE}

\end{landscape}

