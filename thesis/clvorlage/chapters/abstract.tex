\newpage
\phantomsection % to get the hyperlinks (and bookmarks in PDF) right for index, list of files, bibliography, etc.
\addcontentsline{toc}{chapter}{Abstract}
\begin{abstract}

\section*{Abstract}

In recent years, employing pretrained word embeddings from large language models as input
representations in NLP has become state-of-the-art in many applications. Taking the general
trend of outsourcing linguistic analysis to machine learning to the extreme, BERT computes
these embeddings through self-supervised pre-training, completely lacking any linguistic
framework.
Despite this, BERT embeddings have proven to transfer remarkably well to Natural Language Understanding
tasks. However, quickly there were flaws and short comings detected, suggesting that BERT fails in certain
--- often trivial --- contexts reliably recognizing the semantic content of a sentence.
The results of this thesis indicate that providing BERT with additional linguistic, semanticity
providing, information leads to a performance improvement on such tasks.
However, significant gain relies on two key factors: First, the generation of this
linguistic information is paramount --- in this thesis, this process is automated,
leading to modest quality of this semantic mark-up, which in turn is reflected in
noise pruning the generalization capabilities of a model relying on it. Second,
the suitability of the core data on which the model is trained for specific tasks
similarly stands and falls with its quality --- e.g. a lot of non-English datasets
are created by automatically translating English corpora, thereby introducing translation
artifacts which circumvent the learning of a clear objective function.




\selectlanguage{ngerman}
\section*{Zusammenfassung}

Und hier sollte die Zusammenfassung auf Deutsch erscheinen.

\selectlanguage{english}
\end{abstract}
\newpage
