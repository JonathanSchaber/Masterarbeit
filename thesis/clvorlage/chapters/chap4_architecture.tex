
\newchap{Architecture}
\label{chap:4_architecture}

In this chapter, I describe the different parts of the GliBERT model, how they interact and
what a priori decisions were taken during the design of its architecture, such as the number
of predicate-argument structures considered, etc.

To eliminate possible misunderstandings and establish a ``standard'' vocabulary discussing the GliBERT
architecture, I define the following terms which will be used in the specified sense throughout
this thesis:

\begin{itemize}
  \item Following \cite{devlin2018bert}, in this thesis ``a `sentence' can be an arbitrary span of contiguous text, rather
    than an actual linguistic sentence.''
  \item A ``model'' or ``system'' denotes any algorithm that is tailored at a specific task and designed
    to handle natural language as input.
  \item As mentioned in the introduction, natural language can be communicated through different channels (speech, singing, text)
    --- I am, however, only concerned with textual representations of language.
  \item Following \cite{goldberg2017neural}, I use uppercase letters $W, X, Z$ to represent matrices and lowercase
    letters $b, x, y$ to represent vectors. Moreover, keeping consistent with \citeauthor{goldberg2017neural},
    vectors are assumed to be row vectors, leading to right-side matrix multiplication: $xW + b$.
  \item I do assume the reader to be fond of the basic mathematical and theoretical concepts of machine learning,
    keeping mathematical demonstration to a minimum. For a sound introduction into concepts, terms, and mathematical
    reasoning behind deep learning in NLP, I can highly recommend ``Neural Network Methods for Natural Language Processing'' \citep{goldberg2017neural}.
\end{itemize}


% CONTINUATION:
% A `sequence' refers to the input token sequence to BERT,
% which may be a single sentence or two sentences packed together.''

\section{Overview}

GliBERT is an architecture that combines different, pre-existing models and
tools to solve the classification or question answering task at hand. The
general way an input sequence is processed by GliBERT is depicted in figure
\ref{fig:architecture}:


\fig{images/architecture_grob}{fig:architecture}{General architecture of GliBERT, exemplified on a deISEAR dataset instance.}{14}{GliBERT Architecture}

An input sentence gets processed in parallel by two systems, the vanilla German BERT
and the SRL producing and encoding module, to produce two numerical representations
of its tokens. These representations get combined and are sent through a head module
which produces the actual predictions. The core parts of the model are the following:

\begin{description}
	\item[\textbf{BERT module}] This is the vanilla BERT base model: It tokenizes the input sequence, sends it through its twelve transformer layers and outputs the final hidden states of each (sub-)token.
  \item[\textbf{SRL module}] This modules actually consists of three submodules: First, the sequence is processed by the \href{https://github.com/rsennrich/ParZu}{ParZu} \citep{sennrich2009new} parser to identify predicates.
	Secondly, the sequence along the information about which tokens are predicates is handed to the \href{https://liir.cs.kuleuven.be/software_pages/damesrl.php}{DAMESRL} model \citep{do2018flexible} which predicts actual SRLs.
	To ensure there are no tokenization mix-ups between BERT and DAMESRL (because these differences are not reversible as will be seen later), the sequence gets tokenized BERT-style and is passed as this list of tokens to DAMESRL. In a last step,
	the SRL squence gets numerically encoded, using a bi-directional two-layer GRU.
  \item[\textbf{combination}] In this step the BERT and SRL representation get combined: to do this, the embeddings need to be processed, i.e. splittet or merged, respectively, so that they can be concatenated.
	For this, there exist two approaches:
	(A) Fuse the subtokens of BERT back to tokens, or (B) split the SRLs according to the subtokens of BERT.
  \item[\textbf{Head module}] At last, the combined token representations of the input is fed through the final network that transforms it to predict task-dependent output. Several architectures can potentially be applied here: FFNNs, GRUs, CNNs, etc.
\end{description}

\begin{landscape}\centering
  % \vspace*{\fill}
  \fig{images/architecture.png}{fig:architecture-big}{
    Detailed architecture of GliBERT: On the left, the input sentence is passed through two paths:
    On top, through the German BERT, with the optional subtoken fusion module on top. On the bottom,
    through ParZu and DAMESRL, with subsequent embedding via a GRU model; after that an optional
    split token module follows. The bold arrows on the right side show information flow, if BERT
    subtokens are fusioned for appending with SRLs. The dotted arrows represent the information flow
    if the SRLs are split to match with the BERT subtokens.}{24}{GliBERT Architecture detail}
  % \vfill
\end{landscape}


\section{BERT Module}

Since its publishing three years ago, BERT \citep{devlin2018bert} has
often been viewed as a turning-point in NLP: The embeddings it computes
by implementing massive self-supervised pre-training proved to be very
potent representations of language and were successfully implemented in
a wide array of applications addressing downstream tasks via transfer
learning (see chapter \ref{chap:2_approach}). Pretrained models and
APIs for BERT are by now vastly available for a multitude of languages
--- I chose to use the \texttt{bert-base-german-cased} model from
\href{https://deepset.ai/german-bert}{deepset} which is available
in \texttt{pyTorch} through the hugging face's \texttt{transformer}
library \cite{wolf2019transformers}.

While the original BERT was presented in two different sized variants --- \emph{base}
and \emph{large} --- deepset only provides a BERT base model which has the following
specifications according to it's configuration file:

\tab{tab:bert-hyperconfigs}{German BERT hyperparameter configuration.}{
  \scalebox{0.9}{
    \begin{tabular}{ll}
      Transformer Blocks         & 12             \\
      hidden Size                & 768            \\
      hidden activation function & GeLu           \\
      hidden dropout probability & 0.1            \\
      Attention Heads            & 12             \\
      Vocabulary size            & 30,000 (cased) \\
      Total Parameters           & 110 million    \\
    \end{tabular}
  }
}{German BERT hyperparameter configuration}

The handling of the BERT model is straightforward through huggingface's \texttt{transformer}
library: With a simple function call \texttt{BertModel.from\_pretrained()} one loads the
pretrained BERT, and with another function, \texttt{BertTokenizer.from\_pretrained()},
one instantiates the BERT tokenizer.
After encoding a sentence using the tokenizer's method \texttt{.encode\_plus()}, the
encoded sentence is sent through BERT via its \texttt{.forward()}-method --- or
called implicitly, by passing the sentence to the model --- which returns the
vectors for all input tokens, which can then be used in downstream tasks. Fine-tuning
is carried out by passing the computed loss to the specified optimizer funtion (I use the
AdamW optimizer \citep{loshchilov2019decoupled}, a modification of the well-known
Adam (Adaptive Moment Estimation) optimizing function \citep{kingma2014adam}, implementing
additional weight decay), which updates BERT's weight matrices.\myfootnote{After wiring
all the different parts together, GliBERT is one big model having one loss function, which means that all
weights of all layers in the system get update by the AdamW optimizer, not just BERT.}

In the \href{https://github.com/JonathanSchaber/Masterarbeit}{GitHub} repository, in
the file \texttt{load\_data.py}, the data gets tokenized and loaded, and in
the file \texttt{gli\_bert.py}, the forward pass and weight-updating is
defined in the \texttt{fine\_tune\_BERT()} function.

\begin{tcolorbox}[
  colback=blue!5!white,
  colframe=blue!75!black,
  title={\centering Code}]

  In the configuration file \texttt{config.json} you are able to control if the vanilla BERT
  embeddings should be combined with SRL information via the \texttt{combine\_SRLs} attribute:
  If it is set to \texttt{true}, the SRL module is activated and the BERT+SRL combined
  representations are used for the task, if it is set to \texttt{false}, only the vanilla
  BERT embeddings are used.

\end{tcolorbox}




\section{SRL Module}
\label{sec:srl-module}

A Semantic Role Labeller (SRL) is a system, thaltl assigns automatically Semantic Roles to a
given input text.\myfootnote{This may be one or multiple sentences.}

State-of-the-art SRLs are end-to-end models, in the sense that there is no need of
complex pre-processing of the input sentence, such as POS-tagging, syntax parsing, etc.
in advance before the actual labeling takes place. However, as is the case for the model
I employ, some pre-processing remains. For GliBERT, I implement the DAMESRL, a model
presented by \cite{do2018flexible}. I use their pre-trained German Character-Attention
configuration which, according to the authors, achieved an F1 score of 73.5\% on the
CoNLL'09 task \citep{hajivc2009conll}. Despite being characterized as an end-to-end
model by the authors, their SRL needs as input not only the tokenized sentence, but also
``its predicate $w_p$ as input'' \citep{do2018flexible}. In other words, DAMESRL expects
as input a sentence $s$ as list of tokens $[ t_1, t_2, \dotsc t_{n-1}, t_n ]$, where for
each token there is an attribute defined whether it functions as predicate in $s$ or
not.

% ``A major advantage of dependency grammars is their ability to deal with languages that are
% morphologically rich and have a relatively free word order.'' \citep[p.~274]{jurafsky2019speech}
For extracting predicates, I rely on the dependency parse tree the ParZu parser
\cite{sennrich2013exploiting} generates for a German sentence. Given the parsed
sentence, I need to decide which tokens in it are predicates, and which are not.
While this may seem like a straightforward task --- just find the verb as in a
simple sentence like ``He \emph{ate} the apple.'' ---, there are actually a few
caveats that need to be considered, e.g. (predicates are emphasized): (1) There
may be no predicates at all: ``What a day!''. (2) There might be more than one
predicate: ``We \emph{saw} her \emph{leave} the room''. (3) Not all verbs might
be predicates, e.g. because they play grammatically the role of a light verb: ``I
can \emph{hear} you''. In the following section, I will describe how I tackle
these problems by making use of the parse tree information of ParZu.


\subsection{ParZu}

% It is a known problem in the analysis of semantic roles that a proper procedure for predicate
% identification is a problem hard to tackle, consider e.g. the discussion concerning so called
% light verbs: \cite{wittenberg2016light}.

Before analyzing which semantic roles are present in a given sentence, one
must determine the predicates in this sentence: ``First, the predicates which
assign semantic roles to the constituents are identified prior to semantic
role labeling proper. They are usually identified as the main verbs which
head clauses'' \citep[p.~74]{samardzic2013dynamics}. In a dependency framework
like the Universal Stanford Dependencies (USD) \citep{de2014universal},
which explicitly sets the content verb as root,\myfootnote{Note that is not
undisputed:``The parsing scheme that USD advocates takes the division between
function word and content word as its guiding principle. One major difficulty
with doing this is that the dividing line between function word and content
word is often not clear'' \cite{gross2015dependency}.} identification of the
relevant predicate is straightforward: One needs just to look at the dependency
parse tree of a given sentence and select the verbal heads --- i.e. roots ---
of the clauses.

However, the ParZu parser \citep{sennrich2009new} which I employ models not content verbs
as heads but function verbs.\myfootnote{This follows general dependency frameworks proposed
for German, e.g. \cite{gerdes2001word, gross2015dependency}.} In other words, in the sentence
``He was hit by a ball.'', ``hit'' would be the predicate that assigns the semantic roles of
proto-patient ``he'' and theme, or instrument ``a ball''. The dependency parse tree produced
by implementing the USD framework would analyze the word ``hit'' as being the root of this
sentence, making it easy to forward the such annotated sentence to a semantic role labeler
which accepts as input a list of tokens, and marked which ones are the predicates.


Since the parse tree of the German equivalent ``Er wurde von einem Ball getroffen'' produced
by ParZu analyzes the word ``wurde'' as root of the tree, it does not make sense
to forward this as predicate to DAMESRL.

% (interestingly, this stands in contrast to the Pro3Gres parser \citep{schneider2008hybrid} which

% ``In a constituency parse, the finite verb is the head of a verb phrase or rather sentence.
% A dependency parse, on the other hand, does not consider auxiliaries as heads and therefore
% finite verbs are usually not the head of the sentence.  Hence, the head of a sentence typically
% is the verb containing the meaning.  In that sense, dependency structures are closer to the
% semantics of a sentence.'' \citep[p.~6f.]{aepli2018parsing}

% According to the USD, function words are subordinated to content words, which means that
% in a sentence ``He was hit by a ball.'', the infinite participle \textit{hit} would be
% analysed as root, not the finitely inflected \textit{was}.  This is an accordance with the
% view that XXXXXXXXXX However, there is a ``substantial amount of evidence [that] delivers a
% strong argument for the \textelp{} approach, which subordinates full verbs to auxiliaries''
% \cite{gross2015dependency}.

I propose the following algorithm \ref{alg:find-predicates} deciding whether a verb in a
ParZu-parsed sentence is or is not a predicate using a heuristic, relying on the token's
POS tag that the parser predicts. The ParZu parser's default output follows the CoNLL
scheme \citep{buchholz2006conll} which means that there are two levels of POS tagging:
coarse-grained (CPOSTAG) and fine-grained (POSTAG), where the POSTAG corresponds to the
token's STTS tag \citep{schiller1999guidelines}.

\begin{algorithm}
\caption{Predicate finding algorithm}
\label{alg:find-predicates}
	\begin{algorithmic}[1]
	\FORALL{token $t \in$ sentence}
		\IF{CPOSTAG $t \neq$ 'V'}
			\STATE $t \leftarrow$ NOT\_PRED
		\ELSE
			\IF{POSTAG $t =$ 'VVFIN'}
				\STATE $t \leftarrow$ PRED
			\ELSE
				\STATE FLAG $\leftarrow True$
				\FORALL{token $u \neq t \in$ subclause \textbf{where} $t \in$ subclause}
					\IF{CPOSTAG $u =$ 'V' $\land$ $u$ dependent on $t$}
						\STATE $t \leftarrow$ NOT\_PRED
						\STATE FLAG $\leftarrow False$
						\STATE \textbf{break}
					\ENDIF
				\ENDFOR
				\IF{FLAG $= True$}
					\STATE $t \leftarrow$ PRED
				\ENDIF
			\ENDIF
		\ENDIF
	\ENDFOR
\end{algorithmic}
\end{algorithm}


Letting this algorithm run on the dependence parse tree depicted in figure \ref{fig:example-parzu}
of the sentence

\begin{examples}
	\item \label{ex:multiple-predicates} Die Klage wurde abgewiesen, was als Sieg beschrieben werden kann.
\end{examples}

leads to the correct identification of ``abgewiesen'' and ``beschrieben'' as predicates,
disregarding the light and modal verbs ``wurde'', ``werden'', and ``kann'.

%\fig{#1: filename}{#2: label}{#3: long caption}{#4: width}{#5: short caption}
\fig{images/exampleParzu2.png}{fig:example-parzu}{
  Example ParZu dependency parse tree for a sentence with two predicates out of several verbal
  forms. Different Information per word is displayed: The normal word form, the CPOSTAG, the
  POSTAG, the lemma of the word, morphological information, and the index.
}{16}{Multiple Predicates Dependency Parse Tree}

Basically, the algorithm only takes tokens into account which are verbals (have the CPOSTAG
\emph{V}) ; if the POSTAG is \emph{VVFIN}, i.e. if it is a finitely inflected verb form,
it is right away considered to be a predicate. Otherwise, if it is a verbal form but not
finitely inflected, it is checked whether it is dependent on another verbal element of its
subclause, if this is the case, it's not labeled as predicate, otherwise it is. This leads
e.g. to the correct selection of ``beschrieben'' as predicate in the subclause of sentence
\ref{ex:multiple-predicates}, since it forms the ``lowest'' verbal element in this clause,
only pointing to a light verb which modifies its grammatical function, which in turn points
to another light verb which is the head of the subclause.


% The condition on line 9, that only tokens in the respective subclause are considered, is
% ensured by making sure that if a token \textit{u}'s POS is ``V'' and it points to its head
% \textit{t}, that it is not itself the head of a subclause --- i.e. its dependency relation
% is e.g. ``relative clause''. If that is the case the token \textit{u} is considered to
% belong to another subclause and therefore not preventing token \textit{t} from getting
% labelled as a predicate. Consider again the example \ref{ex:multiple-predicates}: Let's say
% we are in the for-loop at the token \textit{weitergeleitet}. Because it is a verb but not
% a finite full-verb, we enter the else-clause on line 7. If we were now to loop through all
% token of sentence \ref{ex:multiple-predicates} we would find that token \textit{führt} is a
% verb that points to our primary token. Without the above outlined constraint that only verbs
% in the same subclause pointing to our original verb are preventing it from being labelled
% a predicate, \textit{weitergeleitet} would be labelled as non-predicate. This is obviously
% false. Taking into account the above considerations, we see that although \textit{führt}
% points to \textit{weitergeleitet}, its edge label is \textit{rel} --- which means that it's
% the head of a relative subclause --- therefore it is not anymore in the same subclause and
% \textit{weitergeleitet} gets labelled as predicate.

\subsection{Ensuring Tokenization Equivalence}

Another difficulty I faced was the tokenization differences between different parsers; which can
lead to sitautions where it is actually impossible to correctly automatically align the tokens
which two parsers produce for the same sequence.
% The consequence of this is that for some
% sentences the SRL tokens cannot be properly combined with their corresponding BERT token.
% The DAMESRL system implements the tokenizer provided by the Natural Language
% Toolkit (NLTK)\myfootnote{\url{https://www.nltk.org/}} which implements a linguistically motivated
% tokenizing. {\color{red} explain what that means}
For GliBERT, this problem occurs at the interface where the embeddings of the BERT
module and the embeddings for the SRLs, which are based on the ParZu tokenization,
need to be aligned. As shown in figure \ref{fig:architecture}, a sentence passes
through two pipelines which both apply tokenization to it: The ParZu/DAMESRL/GRU
and the BERT pipeline. After both have computed numerical representation for the
tokens, these must be combined. However, the tokenization of ParZu (which would
get passed on to DAMESRL and then to the SRL-encoder) and the tokenization of BERT
differ. ParZu implements the \href{http://www.statmt.org/moses/}{Moses tokenizer}
\citep{koehn2007moses}, while BERT, in contrast, utilizes an approach called
``WordPieces'', which is a rather information processing motivated approach,
rather then the linguistically motivated Moses: ``Using wordpieces gives a good
balance between the flexibility of single characters and the efficiency of full
words for decoding, and also sidesteps the need for special treatment of unknown
words.'' \citep[p.~2]{wu2016google}. As a consequence, aligning the corresponding
tokens to guarantee that the correct information pieces get combined is necessary.

To illustrate this, consider the following made-up but not unplausible text snippet:

\begin{examples}
	\item \label{itm:tok-sentence} Anstiege um 4° zwischen 1990-2010
\end{examples}

The tokenizations produced by ParZu, i.e. Moses, and BERT are
depicted side-by-side (note that the BERT tokenization has been
merged for better readability):

% \begin{tabularx}{\textwidth}{@{}l<{}@{\ }l<{}@{\ }X@{}}
\tab{tab:tokenization}{Tokenization aligning difficulties displayed on example sentence
\ref{itm:tok-sentence}. Note that the BERT subtokens are already merged back to tokens for
better readability (``Anstieg'', ``\#\#e'' = ``Anstiege''). In this case both tokenization
sequences would need duplication (duplicated tokens are highlighted in blue).
Achieving this reliably for all potential edge cases is nearly impossible.}
{\begin{tabularx}{\textwidth}{m{3.4cm}m{3.4cm}|m{3.4cm}m{3.4cm}}
  \multicolumn{2}{c}{\textbf{normal}} & \multicolumn{2}{c}{\textbf{aligend}} \\
  \texttt{BERT}  & \texttt{Moses}     & \texttt{BERT}                    & \texttt{Moses}                        \\ \hline
  Anstiege       & Anstiege           & \customcolorbox{Anstiege}{white} & \customcolorbox{Anstiege}{white}      \\
  um             & um                 & \customcolorbox{um}{white}       & \customcolorbox{um}{white}            \\
  {[}UNK{]}      & 4                  & \customcolorbox{[UNK]}{white}    & \customcolorbox{4}{white}             \\
  zwischen       & \textdegree{}      & \customcolorbox{[UNK]}{blue}     & \customcolorbox{\textdegree{}}{white} \\
  1990           & zwischen           & \customcolorbox{zwischen}{white} & \customcolorbox{zwischen}{white}      \\
  -              & 1990-2021          & \customcolorbox{1990}{white}     & \customcolorbox{1990-2021}{white}     \\
  2021           &                    & \customcolorbox{-}{white}        & \customcolorbox{1990-2021}{blue}      \\
                 &                    & \customcolorbox{2021}{white}     & \customcolorbox{1990-2021}{blue}      \\
\end{tabularx}
}{Tonekization Alignment}

In the beginning, I attempted to device an algorithm which would duplicate the respective
tokens in the Moses tokenization to lign up with the BERT (sub-)tokenization. However, it soon
became clear that this was an endeavor too error prone and meticulous: It is e.g. not a priori
clear which tokenization is the shorter one and therefore needs to be duplicated (sometimes, as
in table \ref{tab:tokenization}, both sequences need duplication); BERT tokenizes tokens which
are out-of-vocabulary (OOV) or which contain OOV subtokens as [UNK] further excessively complicates the picture etc.
Therefore, I chose the following strategy which avoids all these problems:

(1) Take the ParZu-tokenized and parsed sentence, apply the predicate-finding algorithm to it.
Then, (2) tokenize the sentence using the BERT tokenizer, merge subtokenized tokens back to
regular tokens, identify the afore detected predicates\myfootnote{Since predicates are normally
verbal forms without any special characters in them, a string comparison search suffices to
achieve this.} and hand the BERT tokenized, predicate marked token sequence to DAMESRL.



% The first question that arises is: which tokenization should be mapped onto wich? In other
% words: should we try to align the BERT tokens with the corresponding NLTK tokens or vice
% versa? Let's assume we decide to align the tokenization $T$ with fewer items to the one
% with more items --- in this case this would mean aligning $T_{NLTK}$ with $T_{BERT}$. So,
% the first five tokens are no problem, we can align them by simply doing an exact match and
% confirm that the elements correspond.
% But when we reach the sixth token, the exact match fails. To decide whether the token
% $t_{T_{BERT}}$ or the token $t_{T_{NLTK}}$ was split up --- i.e. to determine which token
% must be copied to ensure tokenization equality ---, we need to do a mutual substring match.
% Doing this, we eoudl find out that ``-'' is a substring of ``-222''. In consequence,
% we align the two, duplicate ``-222'' and compare it with token number 7 in $T_{BERT}$. Since
% ``222'' is a substring of ``-222'', so we align the two of them.

% While it is theoretically possible to align tokens that were differently tokenized by the two
% algorithms, it is nevertheless quite cumbersome. The main problem, however, arises due to the
% [UNK] token BERT introduces for characters --- or character sequences --- which lie out of its
% vocabulary. Since there is obviously no more (sub-)string comparison possible, the process
% gets even more complicated: Suppose you have duplicted the ``-222'' in the NLTK column and are
% now on line 7. In the BERT tokenization you see the ``[UNK]'' token, while in the NLTK you see
% a ``\textdegree{}C''. To find out, what all is containt in the ``[UNK]'', you need to look at
% the token before and after it in the BERT tokenization and compare it with the respective NLTK
% tokens. since the the and so on....
% % \myfootnote{Of course, one could heuristically {\color{red}
% % go on until no [UNK] token is encountered align up in between.}}
% % Eventually, I decided to simply feed into DAMESRL
% the BERT-tokenized sequences, to get around this issue.


% \begin{tabularx}{\textwidth}{@{}l<{}@{\ }X@{}}
%   \textbf{BERT} (merged) & \textbf{NLTK} \\
%   Die                                        & Die \\
%   mittlere                                   & mittlere \\
%   Oberflächentemperatur                      & Oberflächentemperatur \\
%   wird                                       & wird \\
%   auf                                        & auf \\
%   -                                          & -222 \\
%   222                                        & \textcolor{blue}{-222}\\
%   {[}UNK{]}                                  & \textdegree{}C \\
%   (                                          & ( \\
%   {\fontfamily{ptm}\selectfont\texttildelow} & {\fontfamily{ptm}\selectfont\texttildelow}51 \\
%   51                                         & \textcolor{blue}{{\fontfamily{ptm}\selectfont\texttildelow}51} \\
%   K                                          & K \\
%   )                                          & ) \\
%   geschätzt                                  & geschätzt \\
%   .                                          & .
% \end{tabularx}


\subsection{DAMESRL}

There are only a few end-to-end SOTA SRL frameworks available for German that come with a pre-trained model,
especially such ones that can be conveniently integrated in a pipeline of a bigger system.

\cite{do2018flexible} fill exactly this hole: They introduce DAMESRL, an SRL framework
that implements SOTA architecture, namely self-attention mechanisms, similar to BERT's.
They report an F1 score of 73.5 for their best model configuration on the German dataset
of CoNLL '09. This best configuration is based on word embeddings as well as character
embeddings, self-attention and a softmax layer on top.

The DAMSRL predictor receives the BERT-tokenized sentence along with the information which tokens
in it are predicates (zero or more). For each token labelled as predicate in a sequence it predicts
for each other token in the sequence its SRL.

\fig{images/num_predarg_structs.pdf}{fig:num-predarg-structs}{Number of predicate-argument structures in all datasets.
                                                              Due to it's boilerplate template form, deISEAR shows a peculiar distribution:
                                                              Since it's examples always begin with ``Ich [\textsubscript{PRED} fühlte] $X$, als ....'',
                                                              it's guaranteed that at least one predicate is identifed, and it is very probable,
                                                              because of this sentence structure, that another will occur.
                                                              The other curious pattern exhibits SCARE: In no other
                                                              dataset the amount of sentences where ParZu couldn't detect any predicates
                                                              is that high.

                                                              Besides the noted peculiarities, it is safe to say that setting the maximum number of
                                                              predicate-argument structures to three does probably not lead to much information loss;
                                                              on average over all datasets, 92.73\% of all sentences possess three or less such structures.}{14}{Predicate-Argument Structures}


In most cases, a sentences contains not exactly one predicate which distributes semantic roles,
but several, especially in longer sentences, --- or even none, especially in colloquial, short
sentences. Research by \citeauthor{zhang2019semantics} suggests that fixing the number of
predicate-argument structures to three yields the best results; so I adopt this number. In
other words, if a sentence has more than three argument-predicate structures, I only care about
the first three predicates identified (if proceeding from left to right through the sequence),
and disregard the others.

\begingroup
\begin{srl}[!h]
  \begin{minipage}{0.45\linewidth}
  \vspace{0pt}
    \begin{BVerbatim}[commandchars=\\\{\}, fontsize=\footnotesize]
            Slot 1   Slot 2   Slot 3

Wir         B-A0     0        0
wollten     O        0        0
eine        B-A1     0        0
Sache       I-A1     0        0
mehr        I-A1     0        0
retten      B-V      0        0
als         B-C-A1   0        0
die         I-C-A1   0        0
Restlichen  I-C-A1   0        0
.           O        0        0
      \end{BVerbatim}
  \end{minipage}
  % \hfill
  \begin{minipage}{0.45\linewidth}
  \vspace{0pt}
    \begin{BVerbatim}[commandchars=\\\{\}, fontsize=\footnotesize]
                    Slot 1   Slot 2   Slot 3

        Wir         B-A0     B-A0     B-A0
        wollten     O        O        O
        eine        B-A1     B-A1     B-A1
        Sache       I-A1     I-A1     I-A1
        mehr        I-A1     I-A1     I-A1
        retten      B-V      B-V      B-V
        als         B-C-A1   B-C-A1   B-C-A1
        die         I-C-A1   I-C-A1   I-C-A1
        Restlichen  I-C-A1   I-C-A1   I-C-A1
        .           O        O        O
    \end{BVerbatim}
  \end{minipage}
\end{srl}
\captionof{srl}{The two strategies for dealing with less than three predicates: \textbf{Left}:
                The open SRL slots get filled with the special SRL \emph{0}.
                \textbf{Right}: The first SRL structure gets duplicated until all slots
                are filled.}
\label{srl:zeros-duplicate}
\endgroup


However, if there are fewer than three predicate-argument structures
present, I test and report results for two strategies: The first solution (the left sentence
in SRL \ref{srl:zeros-duplicate}) lies in filling the ``unfilled'' predicate-slots with the
special ``0''-SRL. The second (the right sentence in SRL \ref{srl:zeros-duplicate}) simply
copies the first predicate-argument structure to the unfilled slots, thus amplifying the signal
from the first predicate-argument structure.

\begin{tcolorbox}[
  colback=blue!5!white,
  colframe=blue!75!black,
  title={\centering Code}]

  The predicate-filling mode can be controlled via the \texttt{zeros} attribute in
  the \texttt{config.json} file: If it is set to \texttt{true}, then the empty
  predicate-argument slots are filled up with \texttt{0}-SRLs, if it is
  set to \texttt{false}, the first predicate-argument structure gets copied
  to the empty slots.

\end{tcolorbox}



\subsection{GRU}

Finally, the predicted SRLs need to be encoded in a numeric way so they can be concatenated to
the vectors which are computed by BERT. The ``classic'', pre-transformer age, way of encoding
sequential data would be to employ a recurrent neural network architecture. Typically, one would
implement an architecture that counteracts the well-known vanishing / exploding gradients problem
of vanilla RNNs (cf. \citep{bengio1994learning}), such as LSTMs \citep{hochreiter1997long} or
GRUs \citep{cho2014learning}. I decided to use GRUs, since they are less computational intensive
and research has found both architectures for many tasks and datasets performing on par (cf.
\cite{chung2014empirical}).

For this, the three SRLs for each token get transformed into their numerical representation
via look-up in the SRL dictionary of the GRU model, such that each SRL token is a vector $v \in
\mathbb{R}^{20}$. Then, the numerical representations for each token get concatenated and
those 60-dimensial vectors form then the input for the bi-directional two-layer GRU which
computes hidden states for all the inputs.

Because of the subsequent goal of combining BERT embeddings with SRL embeddings, the following
considerations need to be taken into account.


BERT adds several special meta tokens to sequences it embeds: (1) At the start of each
seqeuence, be it a single sentence or sentence pair one, it inserts a special [CLS] token; CLS
standing for classification. (2) At the the end of a sequence and between sentence pairs, BERT
inserts a separation token [SEP], which signals the end of a sequence. (3) If the sequence
is shorter than the defined maximum length, it gets filled up with [PAD] tokens. An XNLI sentence
pair with maximum length 25 woudl thus be represented as tokenizer BERT sequence the following
way (for better readability, the BERT subtokens are marked blue):

\begin{examples}
  \item Du musst dort nicht bleiben.\\
        An genau der Stelle musst du stehenbleiben!

        \customcolorbox{[CLS]}{blue} \customcolorbox{Du}{blue} \customcolorbox{muss}{blue} \customcolorbox{\#\#t}{blue} \customcolorbox{dort}{blue} \customcolorbox{nicht}{blue} \customcolorbox{bleiben}{blue} \customcolorbox{.}{blue} \customcolorbox{[SEP]}{blue} \customcolorbox{An}{blue} \customcolorbox{genau}{blue} \customcolorbox{der}{blue} \customcolorbox{Stelle}{blue} \customcolorbox{muss}{blue} \customcolorbox{\#\#t}{blue} \customcolorbox{du}{blue} \customcolorbox{stehen}{blue} \customcolorbox{\#\#bleiben}{blue} \customcolorbox{!}{blue} \customcolorbox{[SEP]}{blue} \customcolorbox{[PAD]}{blue}  \customcolorbox{[PAD]}{blue}  \customcolorbox{[PAD]}{blue}  \customcolorbox{[PAD]}{blue}  \customcolorbox{[PAD]}{blue}
\end{examples}

One of the goals of my experiments is to compare the standard BERT implementation with the SRL
enriched GLiBERT variant. Since the vanilla classification head predicts only using the last
hidden state of the [CLS] token, I need to represent SRL information also on this special
token. Therefore, I follow \cite{zhang2019semantics} and add a meta-SRL [CLS], which represents
the meta semantic role of the [CLS] special token.

Another question that arises concerncs the SRL encoding of multiple sentences:
Suppose we have a sequence consisting of several sentences (separated by brackets and indexed by $A$, $B$, and $C$):

\begin{examples}
  \item {[}\textsubscript{A} Die Ereignisse von Oni finden im oder nach dem Jahr 2032 statt und beschreiben ein dystopisches Zukunftsbild der Erde.] [\textsubscript{B} Die Welt ist so verschmutzt, dass nur noch kleine Teile bewohnbar sind.] [\textsubscript{C} Um die internationale Wirtschaftskrise zu lösen, haben sich alle Völker unter einer Weltregierung vereinigt.]
\end{examples}

Should each sentence $A$, $B$, and $C$ be encoded separately and these represenatations than
be combined with the aligned BERT subtokens? Or is it more effective to ``glue'' all sentences
together (or rather, the concatenated SRL tokens in those sentences) and embedd this sequence?
Further, also meta SRLs for [SEP] and [PAD] tokens could be added --- so that also sentence
pair tasks are embedded as one long sequence of SRL tokens. All experiments except for XQuAD
$\alpha$ were conducted implementing the latter approach, proving that the model profits more
from the second architecture.

\begin{tcolorbox}[
  colback=blue!5!white,
  colframe=blue!75!black,
  title={\centering Code}]

  The hyperparameters for the GRU that embeds the SRLs are specified in the configuration file
  \texttt{config.json}. The following hyperparameter configuration was used in the experiments
  of this thesis:

  \begin{itemize}
    \itemsep0em
    \item[] GRU\_head\_hidden\_size: 712
	  \item[] GRU\_head\_dropout: 0.5
	  \item[] SRL\_embedding\_dim: 20
	  \item[] SRL\_hidden\_size: 32
	  \item[] SRL\_num\_layers: 2
	  \item[] SRL\_bias: true
	  \item[] SRL\_bidirectional: true
	  \item[] SRL\_dropout: 0.3
  \end{itemize}

\end{tcolorbox}



\section{Combination}

The combing of a token's BERT embedding with its corresponding SRL embedding is done
straightforwardly; both numerical vectors are concatenated:

Take, the first token $t_1$ of the sentence in \ref{srl:zeros-duplicate},
``Wir'': After being consumed by the BERT module, it has a BERT embedding
vector representation $b \in \mathbb{R}^{768}$. Its SRL representation,
let's say the zero-filled variant \texttt{B-A0+0+0}, has a GRU-embedded
representation $s \in \mathbb{R}^{60}$. The final numerical representation
$w_1$ of the token $t_1$ ``Wir'' plus its SRL \texttt{B-A0+0+0} is computed
as:

$w \in \mathbb{R}^{828} = b \cdot  s$

where $\cdot$ denotes the concatenation operation.

Because the tokenization of the SRL module is based on the merged BERT tokenization,
there remain two ways of combining BERT embeddings and SRL embeddings, however: (1) The
embeddings of subtokenized tokens in BERT are merged back to token level, or (2) SRL
embeddings of tokens which were subtokenized in the BERT module get duplicated to align
with the BERT embeddings. Consider the following word as an example: ``Restlichen'' has one
SRL in \ref{srl:zeros-duplicate}, \texttt{I-C-A1+0+0} (in the duplicated variant), which
would lead to one vector $s$ as numerical representation, while it would be tokenized as
``Rest'', ``\#\#lichen'' inside BERT, leading to two vectors $b_1$, $b_2$ as numerical
representation. In the first scenario, $b_1$ and $b_2$ would be merged into one vector
$b = \frac{b_1 + b_2}{2}$; this vector\myfootnote{Of course, this generalizes to all
possible number of subtokens: The numerical representation $b$ of a token $t$ consisting
of $(t_1, t_2, \dotso t_n)$ subtokens corresponding to $(b_1, b_2, \dotso b_n)$ vectors,
would be computed as $b = \frac{1}{n} \sum_{i=1}^{n} b_i$.} would then be concatenated
with $s$ to form the final representation $w$ of ``Restlichen''. In the second variant,
$s$ would be copied once\myfootnote{Similarly, the copying of $s$ would be performed
$n-1$ times for $n$ BERT subtokens.} to align with the BERT vectors, which would lead
to the final representation of ``Restlichen'' as two vectors: $w_1$ = $b_1 \cdot s$ and
$w_2 = b_2 \cdot s$.

For all experiments, I run both variants, reporting in chapter \ref{chap:5_results} the results,
which indicate that variant 1, i.e. merging BERT subtokens back to tokens to align with the SRL,
leads to information loss, and therefore weakens the general model performance.

\begin{tcolorbox}[
  colback=blue!5!white,
  colframe=blue!75!black,
  title={\centering Code}]

  The BERT-SRL-combination mode can be controlled via the \texttt{merge\_subtokens} attribute
  in the \texttt{config.json} file: If it is set to \texttt{true}, then the BERT
  embeddings of subtokens get merged into one token embedding, if it is set to
  \texttt{false}, the SRL embeddings are duplicated to match up with the BERT
  subtokens.
\end{tcolorbox}


\section{Head Module}

Since the vanilla classification and question answering heads differ quite
starkly --- classification only takes into account the [CLS] token while
the start and end index probabilities need to be computed on every token
in the sequence --- I cover them in separate subsections.

\fig{images/bert_classification.png}{fig:BERT-classification}{Schema for
  sentence pair (left) and single sentence (right) classification. Figure
  taken from \citep{devlin2018bert}.}{13}{BERT-Classification}

Additionally, for classification tasks, I found more ways in tackling these
with different heads, while for the question answering task I only saw one
possibility for a GliBERT head which is virtually the vanilla head, except
that SRL information may be added to the BERT subtokens.

\subsection{Classification}

% While for question answering there was little tweeking needed to adapt to the extended BERT
% embeddings, for classification the situation looks a bit more complex. The standard BERT way
% of doing classification tasks runs as follows:

% \begin{itemize}
%   \item Prepare the data: add a [CLS] token at the beginning, a [SEP] token between the two sentences (if there are), and pad with the [PAD] token
%   \item Send the prepared examples through the BERT network
%   \item Select only the embedding for the first token  --- i.e. the [CLS] ---, send it through a dense layer with a softmax and predict the class for this example
% \end{itemize}

% \cite{devlin2018bert} visualize this as can be seen in figure \ref{fig:BERT-classification}.


% The problem now is that in the above described standard implementation, there is no straigthforward
% way to enrich the BERT embeddings with SRLs, since the only embedding that is used for prediction
% is the [CLS] token; since this is a special BERT token it is not present in the original sentence
% and, therefore, it does not have a corresponding SRL. However, as I lais out in the section before,
% I add several meta SRLs which are added to the regular SRLs, to have numerical SRL representations
% also for these special tokens.

For both, single sentence and sentence pair tasks, the vanilla BERT head considers
only the last hidden state of the [CLS] token, after the sequence was sent through
the transformer blocks and predicts class probabilities only relying on the
information present in this vector $w \in \mathbb{R}^{786}$:

In what follows, I present the three classification heads I used for my experiments: The
[CLS] head, similar to the vanilla classification head, the FFNN head, which considers the
numeric representation of all tokens in the sequence, and the GRU head, which implements
a bidirectional recurrent neural network that computes a condensed representation of the
whole sequence before predicting the actual class probabilities.\myfootnote{Early in the
experimental phase, I also devised a Convolutional Neural Network (CNN) head, however, the
preliminary results were not promising, therefore I concentrated on the other three heads.}



\subsubsection{[CLS] Head}

The vanilla BERT classification approach, as proposed by \cite{devlin2018bert}, feeds
``the \texttt{[CLS]} representation \textelp{} into an output layer for classification,
such as entailment or sentiment analysis.''

In other words, to predict a label in a classification task like sentiment analyis, BERT
encodes the whole sequence, i.e. adds its special [CLS], [SEP], and [PAD] tokens, and
sends it through its twelve transformer blocks. After that, only the numeric representation
of the [CLS] token is fed into a layer, wchich outputs the label probabilities.

The GliBERT [CLS] head operates the same way, except that it is also capable of
handling additional SRL input: After adding corresponding SRLs for the special BERT
tokens and embedding the SRL sequence, the last hidden state representation for the
BERT+SRL combined [CLS] token is selected and softmax-scaled probabilities for all
classes are computed. This is also the approach \cite{zhang2019semantics} employ in
their SemBERT architecture.

% In their paper for SemBERT, \cite{zhang2019semantics} do not really address the
% issue laid out above. To the contrary, the differnt pieces of information they
% provide are rather conflicting, only after inspecting the code they released on
% \href{https://github.com/cooelf/SemBERT/}{GitHub}, the picture somewhat cleared:

% After predicitng the SRLs for a given input, they add pseudo-SRLs for the [CLS] and [SEP] tokens.
% In the look-up table of the BiGRU that consumes the SRLs, they then simply add the corresponding
% keys --- so that besides regular SRLs as ``B-V'' (beginning of predicate) or ``I-A0'' (inside
% or end of argument zero), there are also the labels ``[CLS]'' and ``[PRED]'' After sending this
% sequence through the BiGRU, they concatenate the two hidden states of the [CLS] SRL with the
% [CLS] BERT embedding and predict on that vector % Applying this strategy, now there \emph{is} a
% SRL for the [CLS] token after the sequence was sent through the BiGRU which then can be appended
% to its BERT embedding vector

Formally, the [CLS] Head is a one-layer Fully connected Feedforward Neural Network (FFNN) without a
non-linear activation function. A prediction $\hat{y}$ is the result of sending the embedding
of the [CLS] token --- or the the embedding of the [CLS]+SRL token, respectively ---, i.e. the
first token in the sequence, $w_{0}$ through the network layer $W$, normalizing the outputs
by applying the $softmax$ function\myfootnote{The $softmax$ function takes as input a vector
$x \in \mathbb{R}^{n}$ and normalizies it such that all dimensions are positive and add up to
1. In other words, it computes a probability distribution over the input vector's dimension.
Mathematically, the $softmax$ function $\sigma : \mathbb{R}^{n} \rightarrow [0,1]^{n}$ is
defined as: \[ \sigma (x)_i = \frac{e^{x_i}}{\sum_{j=1}^{n} e^{x_j}} \] for $i = 1, \dotso ,
n$.} to it, and selecting the dimensionality, i.e. class, with the highest probablity:

$\hat{y} = max(softmax(Ww_{0}+b))$

where $W^{I\times J}$ is the weight matrix of the FFNN layer and $b$ is a bias term learned during
fine-tuning. The dimenionalities ($I$ and $J$) of $W$ vary according to the setting and dataset:
while $J$ is the number of classes for the task --- this number ranges from 2 (PAWS-X, \emph{true},
\emph{false}) to 7 (deISEAR, \emph{Angst, Wut, Ekel, Freude, Scham, Schuld, Traurigkeit}) ---, $I$
is either equal to 768 (no SRL information) or equal to 828 (plus SRL information).

In figure \ref{fig:cls-head}, the information flow through GliBERT's network and the final FFNN
layer on the [CLS] token is visualized.

\fig{images/head-cls.png}{fig:cls-head}{Head with a one-layer \textbf{F}eed \textbf{F}orward
  \textbf{N}eural \textbf{N}etwork on the [CLS]-token. This corresponds to the vanilla BERT
  classification head.}{10}{{[}CLS{]} Head}

During training, the loss $\ell $ is computed implementing the negative log likelihood loss
(NLLL)\myfootnote{Mathematically, the NLLL is defined as \[ \ell (x, y) = \sum_{n=1}^{N}
\frac{1}{\sum_{n=1}^{N} w_{y_n}}l_n \] with $l_n = -w_{y_n}x_{n, y_n}$ where $x$ is
the input, $y$ is the target, $w$ is the weight, and $N$ is the batch size (cf.
\url{https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html}).} batch-wise and
backpropagating it through the network. The NLLLoss is the loss function applied to all
heads and will therefore not be mentioned in the following head descriptions.

Intuitively, I expected this head to not show stark differences between the +SRL $-$SRL settings,
since the SRL information, which is essentially a relational, sequential mark-up of the sequence is
boiled down into one token. However, as is also confirmed by the results of \cite{zhang2019semantics},
the [CLS] head definitely profits from SRL information.


\subsubsection{FFNN Head}

% FFNN stands for \textbf{F}eed \textbf{F}orward \textbf{N}eural \textbf{N}etwork. This
% is a standard network, where the input --- normally, a real-valued vector --- gets sent
% linearly through the layers of the network, without feedback-looping it through the same
% weights again(RNNs), or having weights influencing each other (CNNs). In fact, the small
% network, which predicted on the last hidden state of the [CLS] token, also is an FFNN.

While the [CLS] head computes class predictions based on the weights of the last layer
output of the [CLS]-token, this head takes the last layer output of all tokens $w_0,
w_1, \dotso , w_n$, concatenates them into one large vector $w = w_0\cdot w_1\cdot \dotso
\cdot w_n$, and sends this vector $w$ through a one-layer FFNN to predict a class label
$\hat{y}$ (depicted in figure \ref{fig:ffnn-head}):

$\hat{y} = max(softmax(Ww+b))$

where $W^{I\times J}$ is the weight matrix of the FFNN layer and $b$ is a bias term learned during
fine-tuning. While $J$ still is equal to the number of classes for the task at hand, $I$ is the
dimensionality of the input vector $w$: If the setting is such that no SRL information is added,
$I$ will amount to $n * 768$, while in the setting where SRL information is added, $I$ equals to $n
* (768+60)$. $n$ denotes the number of tokens, which means that in a +SRL setting with a maximum
length of 100 tokens, $w \in \mathbb{R}^{82800}$ and therefore also $I = 82,800$.

\fig{images/head-lhsa.png}{fig:ffnn-head}{Head with a one-layer
  \textbf{F}eed \textbf{F}orward \textbf{N}eural \textbf{N}etwork on
  the concatenated token sequence. In difference to the vanilla BERT
  classification head, not only the embedding of the [CLS] token is
  used for predicting classes, but the whole sequence of vectors is
  concatenated and fed into the final head layer.}{10}{FFNN Head}

The motivation for the FFNN head lies in the fact that the SRL information, which is essentially
a mark-up of a sequence, is condensed inot one vector for the [CLS] head. Intuitively, I would
suspect that it is difficult for the model to extract reliable, useful information out of this
single token. Consider the following example (the pseudo SRL [CLS] is added):


[\textsubscript{[CLS]} ] [\textsubscript{A-0} The man] [\textsubscript{predicate} asked] [\textsubscript{A-1} his friend] .

After the subscripted SRLs were consumed by the BiGRU, there is some information about all SLRs
in the hidden states of the [CLS] token. While there may be some information about there being a
predicate, an argument zero, and an argument 1 present, it is impossible to determine from which
tokens these signals came. Especially in sentence pair tasks, such as paraphrase identification,
however, this information is absolutely crucial. By concatenating all embedded tokens into one
large vector, the positional information of the SRLs don't get lost.

\begin{wrapfigure}[17]{r}{0.45\linewidth}
  \begin{center}
    \includegraphics[width=1.0\linewidth]{images/bert_qa.png}
  \end{center}
  \stepcounter{myfigure}
  \caption[BERT Q\&A]{Vanilla BERT question answering head. Figure taken from \citep{devlin2018bert}.}
  \label{fig:bert-qa}
\end{wrapfigure}

As has been shown by e.g. \cite{myagmar2019transferable} for Sentiment Analysis, concatenating
BERT embeddings and sending them trough a FFNN produces good results (in fact, it performs the
best in the different architectures the authors tested for their task and the dataset on which
they evaluated.).\myfootnote{In contrast to GliBERT where the FFNN consists of only one layer
without activation function, \citeauthor{myagmar2019transferable} employ a two-layered FFNN
with the ReLU activation function in between.}


\subsubsection{GRU Head}

In the section before, it was assessed that SRLs can be understood as being essentially a
token-wise, sequential ``mark-up'' of a sentence. Therefore, encoding a sequence marked up
with SRLs employing an architecture designed for sequential data is not too far off.

\fig{images/head-gru.png}{fig:gru-head}{Head with a one-layer \textbf{F}eed \textbf{F}orward \textbf{N}eural
  \textbf{N}etwork on the concatenated last hidden states of the bi-directional GRU.}{10}{GRU Head}

The concept of Recurrent Neural Networks (RNNs), i.e. networks whose connections form a directed
graph such that output of the model is re-fed as input into itself, has been around since the
late 1980s, with \citep{hopfield1982neural} often being credited as having implemented the first
recurrent neural network. RNNs were developed for being able to process sequential data, where
not only the information present in the single parts are important, but also the sequential
ordering of those parts conveys information.\myfootnote{Cf. English ``The dog bites the cat.''
vs. ``The cat bites the dog.''} To overcome the problems of the vanishing and exploding gradient
problems that vanilla RNNs expose, \citep{hochreiter1997long} proposed the Long Short-Term
Memory LSTM architecture, which implements a special cell architecture which can mathematically
``forget'' and ``memorize'' the gradients to which the weights get updated via backpropagation.
Before the advent of the transformer architecture, LSTMs with additional attention mechanism
where the SOTA architecture for many NLP and NLU applications.

\cite{cho2014learning} presented the GRU architecture, which similarly to the LSTM
implements a mathematical solution to the vanishing/exploding gradient problem, but
being a slimmer architecture. As for the SRL embedding network in the SRL module
\ref{sec:srl-module}, I chose GRU over LSTM because of the leaner architecture.

Formally, the embedded token sequence $w_0, w_1, \dotso , w_n$ gets consumed by a
forward and a backward operating GRU, which compute hidden states $h_i$ for each token
$w_i$ in the sequence --- so, for each token $w_i$ there are two hidden representations:
$\overrightarrow{h_i} \in \mathbb{R}^{712}$ for the forward and $\overleftarrow{h_n} \in
\mathbb{R}^{712}$ for the backward GRU, respectively. The last hidden states of these
networks, $\overrightarrow{h_n}$ and $\overleftarrow{h_n}$ get concatenated, and this
vector $w = \overrightarrow{h_n} \cdot \overleftarrow{h_n}$ gets fed into a final dense
layer, which computes class probabilities, similar to the [CLS] and FFNN head final
layers.\myfootnote{Note that the $n$th hidden state of the backward network is actually
the state corresponding to the token $t_0$, i.e. the first token, in the sequence.} This
process is illustrated in figure \ref{fig:gru-head}.


\subsection{Question Answering}
\label{sec:question-answering}

As was mentioned earlier, there exist several different formulations of question answering tasks
(cf. chapter \ref{chap:3_datasets}, section \ref{sec:mlqa}). However, Question Answering tasks are
often formulated as essentially token-wise classification tasks:

\citeauthor{devlin2018bert} encode the question and context text pair as one sequence
separated by the special [SEP] token (as for sentence pair classification tasks). For
extracting the answer span, a FFNN predicts for each token $i$ in the sequence its
probability for being the start of the answer span, and its probability of being the
end of the answer span (see figure \ref{fig:bert-qa}). After applying a $softmax$ over
all start/end probabilities, the token with the highest start probability is selected
as beginning of the answer span and the corresponding end token is selected.

% ``\textelp{} in the question answering task, we represent the input question and passage as
% a single packed sequence, with the question using the A embedding and the passage using the
% B embedding. We only introduce a start vector $S \in \mathbb{R}^H$ and an end vector $E \in
% \mathbb{R}^H$ during fine-tuning. The probability of word $i$ being the start of the answer
% span is computed as a dot product between $T_i$ and $S$ followed by a softmax over all of the
% words in the paragraph: $P_i = \frac{e^{S\cdot T_i}}{\sum_{j}^{} e^{S\cdot T_j}}$.'' \citep{devlin2018bert}

% \fig{images/bert_qa.png}{fig:BERT-QA}{Vanilla BERT question answering head. Figure taken from \citep{devlin2018bert}.}{13}{BERT-QA}

\subsubsection{Span Prediction Head}

The standard implementation of the Q\&A BERT head consists of one one-layer FFNN which predicts
on each input token the probability of it being the start of the answer span and the end of the
answer span, respectively (as seen in figure \ref{fig:span-pred-head}. After both probabilities are computed for all tokens, the ones with
the highest probability get selected; no further logic is enforced, such as that the index of
the start token should be smaller than the one of the end token, or that the answer span may
not lie inside the question (i. e. before the first [SEP] token), etc. The computation of the
$index_{start}$ and $index_{end}$ of the answer span can be formalized as:

$index_{start} = max(softmax(\hat{y}_{0_{[0]}}, \hat{y}_{1_{[0]}}, \dotso , \hat{y}_{n_{[0]}})$

$index_{end} = max(softmax(\hat{y}_{0_{[1]}}, \hat{y}_{1_{[1]}}, \dotso , \hat{y}_{n_{[1]}})$

where $\hat{y}_i = Ww_i+b$ for the $i$-th input token with $\hat{y} \in \mathbb{R}^2$ and
the rectangular brackets denote dimension selection.

Since the standard implementation already implements a model that predicts on each token,
the SRL-embedding enriching is relatively straight forward:
First, the input layer of the small head FFNN needs to be adapted to the BERT embedding + SRL
dimensionality, which results in the vector representation $\mathbb{R}^{768+60}$ for each
SRL-enriched token. Secondly, when the setting of merging the BERT subtokens before adding
the SRL embeddings is selected, the start and end span indexes have to be recomputed.

\fig{images/head-span_pred.png}{fig:span-pred-head}{The GliBERT head for span prediction
    in Question Answering Tasks. After the Tokens and SRLs were consumed by BERT and the SRL
    embedding module, one FFNN predict on each token in the sequence, how likely it is first
    and the last token of the answer span. After these predictions have been made for all tokens, the token
    with the highest value for each position gets selected.}{10}{Span Prediction Head}

\begin{tcolorbox}[
  colback=blue!5!white,
  colframe=blue!75!black,
  title={\centering Code}]

  In the file \texttt{gli\_bert.py}, the head modules are defined as the following classes:

  \begin{itemize}
    \itemsep0em
    \item[] \texttt{GliBERTClassifierCLS}
    \item[] \texttt{GliBERTClassifierFFNN}
    \item[] \texttt{GliBERTClassifierGRU}
    \item[] \texttt{GliBERTSpanPrediction}
  \end{itemize}

  In the configuration file \texttt{config.json} the head to be used is to be
  specified under the attribute \texttt{bert\_head} (remember to pass it as string, e.g. \texttt{"GliBERTClassifierCLS"}).
\end{tcolorbox}



