
\newchap{Architecture}
\label{chap:4_architecture}

\section{Overview}

\section{Semantic Role Labeller}

A Semantic Role Labeller (SRL) is a system, that assigns automatically semantic roles to a given input text.\footnote{This may be one or multiple sentences.}

State-of-the-art semantic role labellers (SRLs) are end-to-end models, nowadays often implementing deep learning techniques, like RNNs or attention, that render tedious feature engineering unnecessary.
For my system, I implement the DAMESRL, a model presented by \cite{do2018flexible}.
I use their pre-trained German Character-Attention model which, according to the authors, achieved an F1 score of 73.5\% on the CoNLL'09 task \citep{hajivc2009conll}.
However, their SRL needs as input not only the sentence, but also ``its predicate $w_p$ as input'' \citep{do2018flexible}.

``A major advantage of dependency grammars is their ability to deal with languages that are morphologically rich and have a relatively free word order.'' \citep[p.~274]{jurafsky2019speech}
For extracting predicates, I rely on the dependency tree the ParZu parser \cite{sennrich2013exploiting} generates for a given sentence.
Since one sentence can have multiple predicate-argument structures, I need to device an algorithm to extract the relevant predicates in a sentence.
This is not as straight forward as it seems on the first look.

\subsection{Finding Predicates}

It is a known problem in the analysis of semantic roles that a proper procedure for predicate identification is a hard to tackle problem, consider e.g. the discussion concerning so called light verbs: \cite{wittenberg2016light}.

``First, the predicates which assign semantic roles to the constituents are identified prior to semantic role labelling proper. They are usually identified as the main verbs which head clauses.'' \citep[p.~74]{samardzic2013dynamics}
In a dependency framework like USD \citep{de2014universal}, which explicitly sets the content verb as root, identification of the relevant predicate is straight-forward:
One has simply to look at the dependency parse tree of a given sentence and select the heads --- i.e. roots --- of the clauses.
However, the ParZu parser models not content words as heads but function words.\footnote{This follows general dependency frameworks proposed for German, e.g. \cite{gerdes2001word, gross2015dependency}.}

(interestingly, this stands in contrast to the Pro3Gres parser \citep{schneider2008hybrid} which 

``In a constituency parse, the finite verb is the head of a verb phrase or rather sentence.
A dependency parse, on the other hand, does not consider auxiliaries as heads and therefore finite verbs are usually not the head of the sentence.
Hence, the head of a sentence typically is the verb containing the meaning.
In that sense, dependency structures are closer to the semantics of a sentence.'' \citep[p.~6f.]{aepli2018parsing}

According to the USD, function words are subordinated to content words, which means that in a sentence ``He was hit by a ball.'', \textit{hit} would be analysed as root, not the finitely inflected \textit{was}.
This is an accordance with the view that XXXXXXXXXX
However, there is a ``substantial amount of evidence [that] delivers a strong argument for the \textelp{} approach, which subordinates full verbs to auxiliaries'' \cite{gross2015dependency}.

``The parsing scheme that USD advocates takes the division between function word and content word as its guiding principle.
One major difficulty with doing this is that the dividing line between function word and content word is often not clear.'' \cite{gross2015dependency}


Following \cite{foth2006umfassende}

\begin{examples}
	\label{ex:one-predicate}
	\item Die Keita-Dynastie regierte das vorkaiserliche und kaiserliche Mali vom 12. Jahrhundert bis Anfang des 17. Jahrhunderts.
\end{examples}

\begin{examples}
	\label{ex:one-predicate-mod}
	\item Im tibetischen Buddhismus werden die Dharma-Lehrer/innen gewöhnlich als Lama bezeichnet.
\end{examples}

\begin{examples}
	\label{ex:multiple-predicates}
	\item Die Klage wurde abgewiesen, was als Sieg beschrieben werden kann.
\end{examples}

whose dependency parse tree is shown in \reffig{fig:example-parzu}:
This sentence has five verbs in it, \textit{wurde}, \textit{abgewiesen}, \textit{beschrieben}, \textit{werden}, and \textit{kann} (POS-tag ``V'' in the second row), but only two of them are relevant predicates, i.e. predicates that carry ``true'' semantics.

%\fig{#1: filename}{#2: label}{#3: long caption}{#4: width}{#5: short caption}
\fig{images/exampleParzu2.png}{fig:example-parzu}{Example dependency parse tree for a sentence with multiple predicates.}{16}{Multiple Predicates Dependency Parse Tree}

I propose the following algorithm \ref{alg:find-predicates} deciding whether a verb in a sentence is or isn't a predicate using a heuristic, relying on the token's POS tag that the parser predicts.
The ParZu parser's default output follows the CoNLL scheme \citep{buchholz2006conll} which means that there are two levels of POS tagging: coarse-grained (CPOSTAG) and fine-grained (POSTAG), where the POSTAG corresponds to the token's STTS tag \citep{schiller1999guidelines}.

\begin{algorithm}
\caption{Predicate finding algorithm}
\label{alg:find-predicates}
	\begin{algorithmic}[1]
	\FORALL{token $t \in$ sentence}
		\IF{CPOSTAG $t \neq$ 'V'}
			\STATE $t \leftarrow$ NOT\_PRED
		\ELSE
			\IF{POSTAG $t =$ 'VVFIN'}
				\STATE $t \leftarrow$ PRED
			\ELSE
				\STATE FLAG $\leftarrow True$
				\FORALL{token $u \neq t \in$ subclause \textbf{where} $t \in$ subclause}
					\IF{CPOSTAG $u =$ 'V' $\land$ $u$ dependent on $t$}
						\STATE $t \leftarrow$ NOT\_PRED
						\STATE FLAG $\leftarrow False$
						\STATE \textbf{break}
					\ENDIF
				\ENDFOR
				\IF{FLAG $= True$}
					\STATE $t \leftarrow$ PRED
				\ENDIF
			\ENDIF
		\ENDIF
	\ENDFOR
\end{algorithmic}
\end{algorithm}

The condition on line 9, that only tokens in the respective subclause are considered, is ensured by making sure that if a token \textit{u}'s POS is ``V'' and it points to its head \textit{t}, that it is not itself the head of a subclause --- i.e. its dependency relation is e.g. ``relative clause''.
If that is the case the token \textit{u} is considered to belong to another subclause and therefore not preventing token \textit{t} from getting labelled as a predicate.
Consider again the example \ref{ex:multiple-predicates}: 
Let's say we are in the for-loop at the token \textit{weitergeleitet}.
Because it is a verb but not a finite full-verb, we enter the else-clause on line 7.
If we were now to loop through all token of sentence \ref{ex:multiple-predicates} we would find that token \textit{führt} is a verb that points to our primary token.
Without the above outlined constraint that only verbs in the same subclause pointing to our original verb are preventing it from being labelled a predicate, \textit{weitergeleitet} would be labelled as non-predicate.
This is obviously false.
Taking into account the above considerations, we see that although \textit{führt} points to \textit{weitergeleitet}, its edge label is \textit{rel} --- which means that it's the head of a relative subclause --- therefore it is not anymore in the same subclause and \textit{weitergeleitet} gets labelled as predicate.

\subsection{DAMESRL}

\section{German BERT}

Since its publishing two years ago, BERT \citep{devlin2018bert} has often been called a ``turning-point'' in ML in NLP.

I use the \texttt{bert-base-german-cased} model from deepset which is available in pyTorch through the hugging face library\cite{wolf2019transformers}.

\subsection{Merging of subtokens back to token level}

\subsection{Final Layer}

As has been shown by e.g. \cite{myagmar2019transferable} for sentiment analysis, a simply final fully-connected feed forward layer produces fairly good results.

% \section{BLEU Scores}
% \label{sec:5_bleuscores}
% 
% Table \ref{bleuresults} shows how to use the predefined tab command to have it listed.
% %\tab{#1: label}{#2: long caption}{#3: the table content}{#4: short caption}
% \tab{bleuresults}{BLEU scores of different MT systems}
% {\begin{tabular}{ll|ccc|c}
% language pair		& ABC	& YYY	\\
% \hline
% EN$\rightarrow$DE	& 20.56	& 32.53 \\
% DE$\rightarrow$EN	& 43.35	& 52.53 \\
% \hline
% \end{tabular}
% }{ABC BLEU scores}
% 
% And we can reference the large table in the appendix as Table \ref{appendixTable}
% 
% \section{Evaluation}
% \label{sec:5_evaluation}
% We saw in section \ref{sec:5_bleuscores} 
% 
% We will see in subsection \ref{subsec:5_moreeval} some more evaluations.
% 
% \subsection{More evaluation}
% \label{subsec:5_moreeval}
% 
% 
% \section{Citations}
% Although BLEU scores should be taken with caution (see \citet{Callison-Burch2006})
% or if you prefer to cite like this: \citep{Callison-Burch2006} \ldots
% 
% to cite: \cite[30-31]{Koehn2005} \\
% to cite within parentheses/brackets: \citep{Koehn2005}, \citep[30-32]{Koehn2005}\\ %\usepackage[square]{natbib} => square brackets
% 
% to cite within the text: \citet{Koehn2005}, \citet[37]{Koehn2005}\\
% only the author(s): \citeauthor{Callison-Burch2006}\\
% only the year: \citeyear{Callison-Burch2006}\\
% 
% \section{Graphics}
% 
% To include a graphic that appears in the list of figures, use the predefined fig command:\\
% %\fig{#1: filename}{#2: label}{#3: long caption}{#4: width}{#5: short caption}
% \fig{images/Rosetta_Stone.jpg}{fig:rosetta}{The Rosetta Stone}{10}{Rosetta}
% 
% %\reffig{#1: label}
% And then reference it as \reffig{fig:rosetta} is easy.
% 
% \section{Some Linguistics}
% 
% (With the package 'covington')\\
% 
% Gloss:
% 
% \begin{examples}
%  \item \gll The cat sits on the table.
% 	    die Katze sitzt auf dem Tisch
% 	\glt 'Die Katze sitzt auf dem Tisch.'
%     \glend
% \end{examples}
% 
% Gloss with morphology:
% 
% \begin{examples}
%  \item \gll La gata duerm -e en la cama.
% 	    Art.Fem.Sg Katze schlaf -3.Sg in Art.Fem.Sg Bett
% 	\glt 'Die Katze schl\"aft im Bett.'
%     \glend
% \end{examples}
% 
