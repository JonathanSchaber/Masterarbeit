
\newchap{Architecture}
\label{chap:4_architecture}

\section{Overview}

gliBERT is an architecture that combines different, pre-existing models and tools.  The general
way an input sequence is processed by gliBERT is depicted in figure \ref{fig:architecture}:


``Throughout this work, a «sentence» can be an arbitrary span of contiguous text, rather
than an actual linguistic sentence. A «sequence» refers to the input token sequence to BERT,
which may be a single sentence or two sentences packed together.'' \citep{devlin2018bert}


\fig{images/general_architecture.png}{fig:architecture}{General architecture of gliBERT, exemplified for deISEAR task.}{12}{gliBERT Architecture}

The core parts of the model are the following:

\begin{description}
	\item[\textbf{BERT module}] This is the vanilla BERT base model: It tokenizes the input sequence and sends it through its twelve transformer layers and outputs the final hidden states of each (sub-)token.
  \item[\textbf{SRL module}] This modules actually consists of two submodules: First, the sequence is processed by ParZu to identify predicates.
	Second, the sequence with the information about which tokens are predicates is handed to the DAMESRL model which predicts actual SRLs.
	To ensure there are no tokenization mix-ups between BERT and DAMESRL (because these differences are not reversible as will be seen later), the sequence gets tokenized BERT-style and is passed as a list of tokens to DAMESRL.
  \item[\textbf{Fusion module}] To combine the BERT embeddings and SRLs, first the SRLs are transformed into numerical representations, or, in other words, are embedded into a comparably lo-dimensional space.
	secondly, the BERT and SRL embeddings need to be processed, i.e. splittet or merged, respectively, so that they can be concatenated.
	For this, there exist two approaches:
	(A) Fuse the subtokens of BERT back to tokens, (B) Split the SRLs according to the subtokens of BERT.
  \item[\textbf{Head module}] At last, the combined representation of the input is fed through the final network that transforms it to predict task-dependent output. Several architectures can be applied here: FFNNs, GRUs, CNNs, etc.
\end{description}

\section{BERT module}

Since its publishing two years ago, BERT \citep{devlin2018bert} has often been viewed as a
``turning-point'' in ML in NLP. In the deep learning era

I use the \texttt{bert-base-german-cased} model from deepset which is available in pyTorch
through the hugging face library\cite{wolf2019transformers}.
Throughout the training, the weights of the

\section{SRL Module}

A Semantic Role Labeller (SRL) is a system, that assigns automatically semantic roles to a
given input text.\myfootnote{This may be one or multiple sentences.}

State-of-the-art semantic role labellers (SRLs) are end-to-end models, nowadays often
implementing deep learning techniques, like RNNs or self-attention mechanisms, that render
tedious feature engineering unnecessary.  For my system, I implement the DAMESRL, a model
presented by \cite{do2018flexible}.  I use their pre-trained German Character-Attention
model which, according to the authors, achieved an F1 score of 73.5\% on the CoNLL'09 task
\citep{hajivc2009conll}.  However, their SRL needs as input not only the sentence, but also
``its predicate $w_p$ as input'' \citep{do2018flexible}.

``A major advantage of dependency grammars is their ability to deal with languages that are
morphologically rich and have a relatively free word order.'' \citep[p.~274]{jurafsky2019speech}
For extracting predicates, I rely on the dependency tree the ParZu parser
\cite{sennrich2013exploiting} generates for a given sentence.  Given the parsed sentence, I
have to decide what tokens in it are predicates, and which are not.  While this may seem like
a straightforward task --- just find the verb as in a simple sentence like ``He \emph{ate}
the apple.'' ---, there are actually a few caveats (predicates are emphasised): (1) There
may be no predicates at all: ``What a day!''. (2) There might be more than one predicate:
``We \emph{saw} her \emph{leave} the room''. (3) Not all verbs might be predicates: ``I can
\emph{hear} you''.  In the following section, I will describe how I tackle these problems by
making use of parsing information from ParZu.

\begin{landscape}\centering
  % \vspace*{\fill}
  \fig{images/architecture2.png}{fig:architecture-big}{Detailed architecture of gliBERT.}{24}{gliBERT Architecture detail}
  % \vfill
\end{landscape}

\subsection{Finding Predicates}

It is a known problem in the analysis of semantic roles that a proper procedure for predicate
identification is a problem hard to tackle, consider e.g. the discussion concerning so called
light verbs: \cite{wittenberg2016light}.

``First, the predicates which assign semantic roles to the constituents are identified prior
to semantic role labelling proper. They are usually identified as the main verbs which head
clauses.'' \citep[p.~74]{samardzic2013dynamics} In a dependency framework like the Universal
Stanford Dependencies (USD) \citep{de2014universal}, which explicitly sets the content
verb as root, identification of the relevant predicate is straightforward: One has simply
to look at the dependency parse tree of a given sentence and select the verbal heads ---
i.e. roots --- of the clauses.  However, the ParZu parser models not content verbs as heads
but function verbs.\myfootnote{This follows general dependency frameworks proposed for German,
e.g. \cite{gerdes2001word, gross2015dependency}.}

(interestingly, this stands in contrast to the Pro3Gres parser \citep{schneider2008hybrid} which

``In a constituency parse, the finite verb is the head of a verb phrase or rather sentence.
A dependency parse, on the other hand, does not consider auxiliaries as heads and therefore
finite verbs are usually not the head of the sentence.  Hence, the head of a sentence typically
is the verb containing the meaning.  In that sense, dependency structures are closer to the
semantics of a sentence.'' \citep[p.~6f.]{aepli2018parsing}

According to the USD, function words are subordinated to content words, which means that
in a sentence ``He was hit by a ball.'', the infinite participle \textit{hit} would be
analysed as root, not the finitely inflected \textit{was}.  This is an accordance with the
view that XXXXXXXXXX However, there is a ``substantial amount of evidence [that] delivers a
strong argument for the \textelp{} approach, which subordinates full verbs to auxiliaries''
\cite{gross2015dependency}.

``The parsing scheme that USD advocates takes the division between function word and content
word as its guiding principle.  One major difficulty with doing this is that the dividing line
between function word and content word is often not clear.'' \cite{gross2015dependency}


Following \cite{foth2006umfassende}

\begin{examples}
	\label{ex:one-predicate}
	\item Die Keita-Dynastie regierte das vorkaiserliche und kaiserliche Mali vom 12. Jahrhundert bis Anfang des 17. Jahrhunderts.
\end{examples}

\begin{examples}
	\label{ex:one-predicate-mod}
	\item Im tibetischen Buddhismus werden die Dharma-Lehrer/innen gewöhnlich als Lama bezeichnet.
\end{examples}

\begin{examples}
	\label{ex:multiple-predicates}
	\item Die Klage wurde abgewiesen, was als Sieg beschrieben werden kann.
\end{examples}

whose dependency parse tree is shown in \reffig{fig:example-parzu}: This sentence has five
verbs in it, \textit{wurde}, \textit{abgewiesen}, \textit{beschrieben}, \textit{werden}, and
\textit{kann} (POS-tag ``V'' in the second row), but only two of them are relevant predicates,
i.e. predicates that carry ``true'' semantics.

%\fig{#1: filename}{#2: label}{#3: long caption}{#4: width}{#5: short caption}
\fig{images/exampleParzu2.png}{fig:example-parzu}{Example dependency parse tree for a sentence
with multiple predicates.}{16}{Multiple Predicates Dependency Parse Tree}

I propose the following algorithm \ref{alg:find-predicates} deciding whether a verb in a sentence
is or is not a predicate using a heuristic, relying on the token's POS tag that the parser
predicts.  The ParZu parser's default output follows the CoNLL scheme \citep{buchholz2006conll}
which means that there are two levels of POS tagging: coarse-grained (CPOSTAG) and fine-grained
(POSTAG), where the POSTAG corresponds to the token's STTS tag \citep{schiller1999guidelines}.

\begin{algorithm}
\caption{Predicate finding algorithm}
\label{alg:find-predicates}
	\begin{algorithmic}[1]
	\FORALL{token $t \in$ sentence}
		\IF{CPOSTAG $t \neq$ 'V'}
			\STATE $t \leftarrow$ NOT\_PRED
		\ELSE
			\IF{POSTAG $t =$ 'VVFIN'}
				\STATE $t \leftarrow$ PRED
			\ELSE
				\STATE FLAG $\leftarrow True$
				\FORALL{token $u \neq t \in$ subclause \textbf{where} $t \in$ subclause}
					\IF{CPOSTAG $u =$ 'V' $\land$ $u$ dependent on $t$}
						\STATE $t \leftarrow$ NOT\_PRED
						\STATE FLAG $\leftarrow False$
						\STATE \textbf{break}
					\ENDIF
				\ENDFOR
				\IF{FLAG $= True$}
					\STATE $t \leftarrow$ PRED
				\ENDIF
			\ENDIF
		\ENDIF
	\ENDFOR
\end{algorithmic}
\end{algorithm}

The condition on line 9, that only tokens in the respective subclause are considered, is
ensured by making sure that if a token \textit{u}'s POS is ``V'' and it points to its head
\textit{t}, that it is not itself the head of a subclause --- i.e. its dependency relation
is e.g. ``relative clause''. If that is the case the token \textit{u} is considered to
belong to another subclause and therefore not preventing token \textit{t} from getting
labelled as a predicate. Consider again the example \ref{ex:multiple-predicates}: Let's say
we are in the for-loop at the token \textit{weitergeleitet}. Because it is a verb but not
a finite full-verb, we enter the else-clause on line 7. If we were now to loop through all
token of sentence \ref{ex:multiple-predicates} we would find that token \textit{führt} is a
verb that points to our primary token. Without the above outlined constraint that only verbs
in the same subclause pointing to our original verb are preventing it from being labelled
a predicate, \textit{weitergeleitet} would be labelled as non-predicate. This is obviously
false. Taking into account the above considerations, we see that although \textit{führt}
points to \textit{weitergeleitet}, its edge label is \textit{rel} --- which means that it's
the head of a relative subclause --- therefore it is not anymore in the same subclause and
\textit{weitergeleitet} gets labelled as predicate.

\subsection{Ensuring Tokenization Equivalence}

One of the major difficulties I ran into, was the tokenization differences between different
parsers. In conrete terms, this means that it is not always possible to correctly align the tokens
which two parsers produce for the same sequence. % The consequence of this is that for some
% sentences the SRL tokens cannot be properly combined with their corresponding BERT token.
The DAMESRL system implements the tokenizer provided by the Natural Language
Toolkit (NLTK)\myfootnote{\url{https://www.nltk.org/}} which implements a linguistically motivated
tokenizing. {\color{red} explain what that means}
BERT, in contrast, utilizes an approach called ``WordPieces'', which is a rather information
processing motivated approach: ``Using wordpieces gives a good balance between the flexibility
of single characters and the efficiency of full words for decoding, and also sidesteps the need
for special treatment of unknown words.'' \citep[p.~2]{wu2016google}.
Although the NLTK algorithm is guided by linguistically informed rules and statistic while the
WordPieces approach simply reflects distribution properties of assembled letters, both systems
tokenize sentences in the same way in most cases. However, especially when rare symbols such as
currencies, units, and the like are present, the tokenization slightly differs What is even
worse, often the correct alignment of those differing sequences is rather complicated to obtain.
{\color{red} automatically}


To illustrate this, consider the following sentence from the PAWS-X data set:

\begin{examples}
	\item Die mittlere Oberflächentemperatur wird auf -222 \textdegree{}C ({\fontfamily{ptm}\selectfont\texttildelow}51 K) geschätzt.

\end{examples}

\begin{tabularx}{\textwidth}{@{}l<{}@{\ }X@{}}
  \textbf{BERT} (merged) & \textbf{NLTK} \\
  Die                    & Die \\
  mittlere               & mittlere \\
  Oberflächentemperatur  & Oberflächentemperatur \\
  wird                   & wird \\
  auf                    & auf \\
  -                      & -222 \\
  222                    & \textdegree{}C \\
  {[}UNK{]}              & ( \\
  (                      & {\fontfamily{ptm}\selectfont\texttildelow}51 \\
  {\fontfamily{ptm}\selectfont\texttildelow} & K \\
  51                     &  ) \\
  K                      & geschätzt \\
  )                      & . \\
  geschätzt              & \\
  .                      & \\
\end{tabularx}

The first question that arises is: which tokenization should be mapped onto wich? In other
words: should we try to align the BERT tokens with the corresponding NLTK tokens or vice
versa? Let's assume we decide to align the tokenization $T$ with fewer items to the one
with more items --- in this case this would mean aligning $T_{NLTK}$ with $T_{BERT}$. So,
the first five tokens are no problem, we can align them by simply doing an exact match and
confirm that the elements correspond.
But when we reach the sixth token, the exact match fails. To decide whether the token
$t_{T_{BERT}}$ or the token $t_{T_{NLTK}}$ was split up --- i.e. to determine which token
must be copied to ensure tokenization equality ---, we need to do a mutual substring match.
Doing this, we eoudl find out that ``-'' is a substring of ``-222''. In consequence,
we align the two, duplicate ``-222'' and compare it with token number 7 in $T_{BERT}$. Since
``222'' is a substring of ``-222'', so we align the two of them.

While it is theoretically possible to align tokens that were differently tokenized by the two
algorithms, it is nevertheless quite cumbersome. The main problem, however, arises due to the
[UNK] token BERT introduces for characters --- or character sequences --- which lie out of its
vocabulary. Since there is obviously no more (sub-)string comparison possible, the described
method fails.\myfootnote{Of course, one could heuristically {\color{red} go on until no [UNK]
token is encountered align up in between.}} Eventually, I decided to simply feed into DAMESRL
the BERT-tokenized sequences, to get around this issue.


\begin{tabularx}{\textwidth}{@{}l<{}@{\ }X@{}}
  \textbf{BERT} (merged) & \textbf{NLTK} \\
  Die                    & Die \\
  mittlere               & mittlere \\
  Oberflächentemperatur  & Oberflächentemperatur \\
  wird                   & wird \\
  auf                    & auf \\
  -                      & -222 \\
  222                    & -222\\
  {[}UNK{]}              & \textdegree{}C \\
  (                      & ( \\
  {\fontfamily{ptm}\selectfont\texttildelow} & {\fontfamily{ptm}\selectfont\texttildelow}51 \\
  51                     & {\fontfamily{ptm}\selectfont\texttildelow}51 \\
  K                      & K \\
  )                      & ) \\
  geschätzt              & geschätzt \\
  .                      & .
\end{tabularx}


% \begin{examples}
%   \item Jetzt Werbung im Kauf-App, eine riesengroße Frechheit!!! Bitte im Premium um Option für Kaffee kochen erweitern, dann bezahle ich gerne weitere 1'000.- €\$£ und mehr.

% \end{examples}

% \begin{tabularx}{\textwidth}{@{}l<{}@{\ }X@{}}
%   \textbf{BERT} (merged) & \textbf{NLTK} \\
%   Jetzt                  & Jetzt \\
%   Werbung                & Werbung \\
%   im                     & im \\
%   Kauf                   & Kauf-App \\
%   -                      & \\
%   App                    & \\
%   ,                      & , \\
%   eine                   & eine \\
%   riesengroße            & riesengroße \\
%   Frechheit              & Frechheit \\
%   !                      & ! \\
%   !                      & ! \\
%   !                      & ! \\
%   Bitte                  & Bitte \\
%   im                     & im \\
%   Premium                & Premium \\
%   um                     & um \\
%   Option                 & Option \\
%   für                    & für \\
%   Kaffee                 & Kaffee \\
%   kochen                 & kochen \\
%   erweitern              & erweitern \\
%   ,                      & , \\
%   dann                   & dann \\
%   bezahle                & bezahle \\
%   ich                    & ich \\
%   gerne                  & gerne \\
%   weitere                & weitere \\
%   1                      & 1'000.- \\
%   '                      & \\
%   000                    & \\
%   .                      & \\
%   -                      & \\
%   €                      & € \\
%   \$                     & \$ \\
%   {[}UNK{]}              & £ \\
%   und                    & und \\
%   mehr                   & mehr \\
%   .                      & .
% \end{tabularx}


\subsection{DAMESRL}

There are not too many SOTA SRL frameworks available for German that come with a pre-trained model,
especially such ones that can be integrated in a pipeline in a pipeline of a bigger system.

\cite{do2018flexible} fill exactly this hole: They introduce DAMESRL, an SRL framework that
implements SOTA architecture, namely self-attention mechanisms, similar to BERT's. They report
an F1 score of 73.5 for their best model configuration on the German data set of CoNLL '09.
This best configuration is based on word as well as character embeddings, self-attention and a
softmax layer on top.

The DAMSRL predictor receives the BERT-tokenized sentence along with the information which tokens
in it are predicates (zero or more). For each token labelled as predicate in a sequence it predicts
for each other token in the sequence its SRL.

\subsection{GRU}

Finally, the predicted SRLs need to be encoded in a numeric way similar to the BERT embeddings so .
that they can be combined and sent thorugh the final head network Encoding a sequence is a typical.
seq2seq task, for which recurrent neural networks have proven to be effective mechanisms (CITE)   .

\section{Fusion Module}

\subsection{Embedding the SRLs}

\begin{itemize}
	\item Embed each sentence alone vs. concatenate all sentences
	\item same for text\_1, text\_2
	\item add meta-SRLs [CLS], [SEP]
	\item duplicate if less preds than 3 vs. fill with zeros
\end{itemize}

\subsection{Aligning BERT subtokens with SRL tokens}

A crucial part in the overall architecture is the combining of the numeric representation
of (sub-)words computed by the BERT network and the embedded SRLs. One difficulty lies in
the fact that, as already mentioned above, BERT has its own tokenizer which implements
a so-called sub-word or wordpiece \cite{wu2016google} encoding strategy: BERT has a
fixed length of vocabulary it can hold, namely 30,000 tokens. The wordpiece tokenization
approach is a balance between word and character tokenization in that that it ``gives
a good balance between theflexibility of single characters and the efficiency of full
words for decoding, and also sidesteps the need for special treatment of unknown words''
\citep[p.~2]{wu2016google}. As a consequence, BERT tokenizes a sentence quite differently
than a traditional parser, since the latter adheres to the full tokens. Consider the
following example:

\begin{examples}
	\label{ex:tokenization-diff}
	\item Es ist der Sitz des Bezirks Zerendi in der Region Akmola.
	\item ParZu: Es ist der Sitz des Bezirks Zerendi in der Region Akmola .
	\item germanBERT: Es ist der Sitz des Bezirks Zer \#\#end \#\#i in der Region Ak \#\#mol \#\#a .
\end{examples}


A further challenge besides the alignment of traditional tokenization and wordpiece tokenization is
the general difference in parsing a sentence that exist.

\section{Head}

\subsection{Question Answering}

\subsection{Classifiction}

While for question answering there was little tweeking needed to adapt to the extended BERT
embeddings, for classification the situation looks a bit more complex. The standard BERT way
of doing classification tasks runs as follows:

\begin{itemize}
  \item Prepare the data: add a [CLS] token at the beginning, a [SEP] token between the two sentences (if there are), and pad with the [PAD] token
  \item Send the prepared examples through the BERT network
  \item Select only the embedding for the first token  --- i.e. the [CLS] ---, send it through a dense layer with a softmax and predict the class for this example
\end{itemize}

\cite{devlin2018bert} visualize this as can be seen in figure \ref{fig:BERT-classification}.

\fig{images/bert_classification.png}{fig:BERT-classification}{Schema for sentence pair (left) and single sentence (right) classification.}{10}{BERT-Classification}


The problem now is that in the above described standard implementation, there
is no straigthforward way to enrich the BERT embeddings with SRLs, since the
only embedding that is used for prediction is the [CLS] token; since this is a
special BERT token it is not present in the original sentence and, therefore,
it does not have a corresponding SRL. (And what should that be? Since it is a
meta-token it rcan't really have a SRL?

To tackle this problem, I remodeled the architecture from \cite{zhang2019semantics} on the one
hand, and on the other hand tested several other final layer designs.

\subsubsection{[CLS]}

In their paper for SemBERT, \cite{zhang2019semantics} do not really address the
issue laid out above. To the contrary, the differnt pieces of information they
provide are rather conflicting, only after inspecting the code they released on
\href{https://github.com/cooelf/SemBERT/}{GitHub}, the picture somewhat cleared:

After predicitng the SRLs for a given input, they add pseudo-SRLs for the [CLS] and [SEP] tokens.
In the look-up table of the BiGRU that consumes the SRLs, they then simply add the corresponding.
keys --- so that besides regular SRLs as ``B-V'' (beginning of predicate) or ``I-A0'' (inside   .
or end of argument zero), there are also the labels ``[CLS]'' and ``[PRED]'' After sending this .
sequence through the BiGRU, they concatenate the two hidden states of the [CLS] SRL with the    .
[CLS] BERT embedding and predict on that vector % Applying this strategy, now there \emph{is} a .
SRL for the [CLS] token after the sequence was sent through the BiGRU which then can be appended.
to its BERT embedding vector                                                                    .

In table XYZ I report the gains, losses this strategy yields for the four classifications data sets
in my corpus.

\subsubsection{Fully-Connected Feed Forward}

While the approach implemented by \cite{zhang2019semantics} is able to improve the vanilla BERT
approach, it does not lead to an improvement on others, or worse, brings the perormance down. I
suspect a reason for this may lie in the manner of how the SRLs are processed in this approach.
The information SRLs provide is, what I call, sub-sentence specific and cannot be represented
as a single information piece. By this I mean that it does not suffice to know that given an
utterance $x$ that there is a specific SR in it; rather the information \emph{where} is is is
crucial. Consider the following example (the pseudo SRL [CLS] is added):


[\textsubscript{[CLS]} ] [\textsubscript{A-0} The man] [\textsubscript{predicate} hit] [\textsubscript{A-1} the dog] .

After the subscripted SRLs were consumed by the BiGRU, there is some information about all
SLRs in the hidden states of the [CLS] token. While there may be some information about there
being a predicate, an argument zero, and an argument 1 present, it is completely impossibly
to determine from which tokens these signals came. Especially in sentence pair tasks, such
as paraphrase identification, this information is however absolutely crucial. As can be seen
from results \ref{}, this hypothesis is also supported by the results:



As has been shown by e.g. \cite{myagmar2019transferable} for sentiment analysis, a simply final
fully-connected feed forward layer produces fairly good results (in fact, it performs the best
in the different architectures they tested for their task).

Implementing a fully-connected feed forward network as final layer counters the problem of the
information deprecation that is present in the [CLS] approach: Every token's BERT embedding gets
concatenated with the token's SRL embeddding. The whole sequence is then flattened, i.e. all
BERT+SRL vectors get concatenated into one large vector of size $\mathbbm{R}^{n\times768+XXX}$


\subsubsection{GRU}

While the fully-connected feed forward


% \section{BLEU Scores}
% \label{sec:5_bleuscores}
%
% Table \ref{bleuresults} shows how to use the predefined tab command to have it listed.
% %\tab{#1: label}{#2: long caption}{#3: the table content}{#4: short caption}
% \tab{bleuresults}{BLEU scores of different MT systems}
% {\begin{tabular}{ll|ccc|c}
% language pair		& ABC	& YYY	\\
% \hline
% EN$\rightarrow$DE	& 20.56	& 32.53 \\
% DE$\rightarrow$EN	& 43.35	& 52.53 \\
% \hline
% \end{tabular}
% }{ABC BLEU scores}
%
% And we can reference the large table in the appendix as Table \ref{appendixTable}
%
% \section{Evaluation}
% \label{sec:5_evaluation}
% We saw in section \ref{sec:5_bleuscores}
%
% We will see in subsection \ref{subsec:5_moreeval} some more evaluations.
%
% \subsection{More evaluation}
% \label{subsec:5_moreeval}
%
%
% \section{Citations}
% Although BLEU scores should be taken with caution (see \citet{Callison-Burch2006})
% or if you prefer to cite like this: \citep{Callison-Burch2006} \ldots
%
% to cite: \cite[30-31]{Koehn2005} \\
% to cite within parentheses/brackets: \citep{Koehn2005}, \citep[30-32]{Koehn2005}\\ %\usepackage[square]{natbib} => square brackets
%
% to cite within the text: \citet{Koehn2005}, \citet[37]{Koehn2005}\\
% only the author(s): \citeauthor{Callison-Burch2006}\\
% only the year: \citeyear{Callison-Burch2006}\\
%
% \section{Graphics}
%
% To include a graphic that appears in the list of figures, use the predefined fig command:\\
% %\fig{#1: filename}{#2: label}{#3: long caption}{#4: width}{#5: short caption}
% \fig{images/Rosetta_Stone.jpg}{fig:rosetta}{The Rosetta Stone}{10}{Rosetta}
%
% %\reffig{#1: label}
% And then reference it as \reffig{fig:rosetta} is easy.
%
% \section{Some Linguistics}
%
% (With the package 'covington')\\
%
% Gloss:
%
% \begin{examples}
%  \item \gll The cat sits on the table.
% 	    die Katze sitzt auf dem Tisch
% 	\glt 'Die Katze sitzt auf dem Tisch.'
%     \glend
% \end{examples}
%
% Gloss with morphology:
%
% \begin{examples}
%  \item \gll La gata duerm -e en la cama.
% 	    Art.Fem.Sg Katze schlaf -3.Sg in Art.Fem.Sg Bett
% 	\glt 'Die Katze schl\"aft im Bett.'
%     \glend
% \end{examples}
%
