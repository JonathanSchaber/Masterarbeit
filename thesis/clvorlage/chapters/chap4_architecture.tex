
\newchap{Architecture}
\label{chap:4_architecture}

To eliminate possible misunderstandings and establish a ``standard'' vocabulary discussing the GliBERT
architecture, I define the following terms which will be used in the specified sense throughout
this thesis:

\begin{itemize}
  \item Following \cite{devlin2018bert}, in my thesis ``a `sentence' can be an arbitrary span of contiguous text, rather
    than an actual linguistic sentence.''
  \item A ``model'' or ``system'' denotes any algorithm that is tailored at a specific task and designed
    to handle natural language as input.
  \item As mentioned in the introduction, natural language can be communicated in over different modes (speech, singing, text)
    --- I am, however, only concerned with textual representations of language.
\end{itemize}


% CONTINUATION:
% A `sequence' refers to the input token sequence to BERT,
% which may be a single sentence or two sentences packed together.''

\section{Overview}

GliBERT is an architecture that combines different, pre-existing models and
tools to solve the classification or questionn answering task at hand. The
general way an input sequence is processed by GliBERT is depicted in figure
\ref{fig:architecture}:


\fig{images/architecture_grob}{fig:architecture}{General architecture of GliBERT, exemplified for  the deISEAR task.}{14}{GliBERT Architecture}

An input sentence gets processed in parallel by two models to produce two numerical
representations of its tokens. These represenatations get combined and are sent
through a head module which producec the actual predictions. The core parts of the
model are the following:

\begin{description}
	\item[\textbf{BERT module}] This is the vanilla BERT base model: It tokenizes the input sequence and sends it through its twelve transformer layers and outputs the final hidden states of each (sub-)token.
  \item[\textbf{SRL module}] This modules actually consists of three submodules: First, the sequence is processed by \href{https://github.com/rsennrich/ParZu}{ParZu} \citep{sennrich2009new} to identify predicates.
	Second, the sequence with the information about which tokens are predicates is handed to the \href{https://liir.cs.kuleuven.be/software_pages/damesrl.php}{DAMESRL} model \citep{do2018flexible} which predicts actual SRLs.
	To ensure there are no tokenization mix-ups between BERT and DAMESRL (because these differences are not reversible as will be seen later), the sequence gets tokenized BERT-style and is passed as a list of tokens to DAMESRL. In a last step,
	the SRL squence gets numerically encoded.
  \item[\textbf{combination}] In this step the BERT and SRL representation get combined: to do this, the embeddings need to be processed, i.e. splittet or merged, respectively, so that they can be concatenated.
	For this, there exist two approaches:
	(A) Fuse the subtokens of BERT back to tokens, (B) Split the SRLs according to the subtokens of BERT.
  \item[\textbf{Head module}] At last, the combined token representations of the input is fed through the final network that transforms it to predict task-dependent output. Several architectures can be applied here: FFNNs, GRUs, CNNs, etc.
\end{description}

\begin{landscape}\centering
  % \vspace*{\fill}
  \fig{images/architecture.png}{fig:architecture-big}{Detailed architecture of GliBERT: On the left, the input sentence is passed through two paths: On top, through the German BERT, with the optional subtoken fusion module on top. On the bottom, through ParZu and DAMESRL, with subsequent embedding via a GRU model; after that an optional split token module follows. The bold arrows on the right side show information flow, when BERT subtokens are fusioned for appending with SRLs. The dotted arrows represent the information flow when the SRLs are splitted to match with the BERT subtokens.}{24}{GliBERT Architecture detail}
  % \vfill
\end{landscape}


\section{BERT Module}

Since its publishing two years ago, BERT \citep{devlin2018bert} has often been
viewed as a ``turning-point'' in NLP: The embeddings it computed by implementing
massive ``unsophisticated'' self-supervised pretraining proved to be very potent
representations of language and were successfully implemented in a wide array of
downstream tasks. Pretrained models and APIs for BERT are vastly available for
a multitude of langauges --- I chose to use the \texttt{bert-base-german-cased}
model from \href{https://deepset.ai/german-bert}{deepset} which is available in
\texttt{pyTorch} through the hugging face's \texttt{transformer} library\cite{wolf2019transformers}.

While the original BERT was presented in two different sized variants --- \emph{base}
and \emph{large} --- deepset only provides a BERT base model which has the following
specifications according to it's configuration file:

\tab{tab:bert-yuperconfigs}{German BERT hyperparameter configuration.}{
  \scalebox{0.9}{
    \begin{tabular}{ll}
      Transformer Blocks         & 12             \\
      hidden Size                & 768            \\
      hidden activation function & GeLu           \\
      hidden dropout probability & 0.1            \\
      Attention Heads            & 12             \\
      Vocabulary size            & 30,000 (cased) \\
      Total Parameters           & 110 million    \\
    \end{tabular}
  }
}{German BERT hyperparameter configuration}

The handling of the BERT model is straightforward through huggingface's \texttt{transformer}
library: With a simple function call \texttt{BertModel.from\_pretrained()} one loads the
pretrained BERT, and with another function, \texttt{BertTokenizer.from\_pretrained()},
one instantiates the BERT tokenizer.
After encoding a sentence using the tokenizer's method \texttt{.encode\_plus()}, the
encoded sentence is sent through BERT via its \texttt{.forward()}-method --- or
called implicitly, by simply passing the sentence to the model --- which returns the
vectors for all input tokens, which can then be used in downstream tasks. Fine-tuning
is simply done by passing the computed loss to the specified loss funtion (I use the
AdamW loss function \citep{loshchilov2019decoupled}, a modification of the well-known
Adam (Adaptive Moment Estimation) loss function \citep{kingma2014adam}, implementing
additional weight decay), which updates BERT's weight matrices.


In the \href{https://github.com/JonathanSchaber/Masterarbeit}{GitHub} repository, in
the file \texttt{load\_data.py}, the data gets tokenized and loaded, and in
the file \texttt{gli\_bert.py}, the forward pass and weight-updating is
defined in the \texttt{fine\_tune\_BERT()} function.


\section{SRL Module}
\label{sec:srl-module}

A Semantic Role Labeller (SRL) is a system, that assigns automatically semantic roles to a
given input text.\myfootnote{This may be one or multiple sentences.}

State-of-the-art semantic role labellers (SRLs) are end-to-end models, nowadays often
implementing deep learning techniques, like RNNs or self-attention mechanisms, that render
tedious feature engineering unnecessary.  For my system, I implement the DAMESRL, a model
presented by \cite{do2018flexible}.  I use their pre-trained German Character-Attention
configuration which, according to the authors, achieved an F1 score of 73.5\% on the CoNLL'09 task
\citep{hajivc2009conll}.  However, their SRL needs as input not only the sentence, but also
``its predicate $w_p$ as input'' \citep{do2018flexible}. In other words, DAMESRL expects
as input a sentence $s$ as list of tokens $[ t_1, t_2, \dotsc t_{n-1}, t_n ]$, where for each
token there is an attribute defined whether it functions as predicate in $s$ or not.

% ``A major advantage of dependency grammars is their ability to deal with languages that are
% morphologically rich and have a relatively free word order.'' \citep[p.~274]{jurafsky2019speech}
For extracting predicates, I rely on the dependency tree the ParZu parser
\cite{sennrich2013exploiting} generates for a given sentence. Given the
parsed sentence, I have to decide what tokens in it are predicates, and
which are not. While this may seem like a straightforward task --- just
find the verb as in a simple sentence like ``He \emph{ate} the apple.''
---, there are actually a few caveats (predicates are emphasised): (1)
There may be no predicates at all: ``What a day!''. (2) There might be
more than one predicate: ``We \emph{saw} her \emph{leave} the room''. (3)
Not all verbs might be predicates, e.g. because they play grammatically
the role of a light verb: ``I can \emph{hear} you''. In the following
section, I will describe how I tackle these problems by making use of
parsing information from ParZu.


\subsection{ParZu}

% It is a known problem in the analysis of semantic roles that a proper procedure for predicate
% identification is a problem hard to tackle, consider e.g. the discussion concerning so called
% light verbs: \cite{wittenberg2016light}.

Before analyzing which semantic roles are present in a given sentence, one
must determine the predicates in this sentence: ``First, the predicates which
assign semantic roles to the constituents are identified prior to semantic
role labeling proper. They are usually identified as the main verbs which
head clauses'' \citep[p.~74]{samardzic2013dynamics}. In a dependency framework
like the Universal Stanford Dependencies (USD) \citep{de2014universal},
which explicitly sets the content verb as root,\myfootnote{Note that is not
undisputed:``The parsing scheme that USD advocates takes the division between
function word and content word as its guiding principle. One major difficulty
with doing this is that the dividing line between function word and content
word is often not clear'' \cite{gross2015dependency}.} identification of the
relevant predicate is straightforward: One has simply to look at the dependency
parse tree of a given sentence and select the verbal heads --- i.e. roots ---
of the clauses. However, the ParZu parser models not content verbs as heads but
function verbs.\myfootnote{This follows general dependency frameworks proposed
for German, e.g. \cite{gerdes2001word, gross2015dependency}.} In other words,
in the ``He was hit by a ball.'', ``hit'' would be the predicate that assigns
the semantic roles of proto-patient ``he'' and theme, or instrument ``a ball''.
The dependency parse tree produced by implementing the USD framework would
analyze the word ``hit'' as being the root of this sentence, making it easy to
forward the such annotated sentence to a semantic role labeler which accepts as
input a list of tokens, and marked which ones are the predicates.


Since the parse tree of the German equivalent ``Er wurde von einem Ball getroffen'' produced
by ParZu analyzes the word ``wurde'' as root of the tree, it does not make sense
to forward this as predicate to DAMESRL.

% (interestingly, this stands in contrast to the Pro3Gres parser \citep{schneider2008hybrid} which

% ``In a constituency parse, the finite verb is the head of a verb phrase or rather sentence.
% A dependency parse, on the other hand, does not consider auxiliaries as heads and therefore
% finite verbs are usually not the head of the sentence.  Hence, the head of a sentence typically
% is the verb containing the meaning.  In that sense, dependency structures are closer to the
% semantics of a sentence.'' \citep[p.~6f.]{aepli2018parsing}

% According to the USD, function words are subordinated to content words, which means that
% in a sentence ``He was hit by a ball.'', the infinite participle \textit{hit} would be
% analysed as root, not the finitely inflected \textit{was}.  This is an accordance with the
% view that XXXXXXXXXX However, there is a ``substantial amount of evidence [that] delivers a
% strong argument for the \textelp{} approach, which subordinates full verbs to auxiliaries''
% \cite{gross2015dependency}.

I propose the following algorithm \ref{alg:find-predicates} deciding whether a verb in a
ParZu-parsed sentence is or is not a predicate using a heuristic, relying on the token's
POS tag that the parser predicts. The ParZu parser's default output follows the CoNLL
scheme \citep{buchholz2006conll} which means that there are two levels of POS tagging:
coarse-grained (CPOSTAG) and fine-grained (POSTAG), where the POSTAG corresponds to the
token's STTS tag \citep{schiller1999guidelines}.

\begin{algorithm}
\caption{Predicate finding algorithm}
\label{alg:find-predicates}
	\begin{algorithmic}[1]
	\FORALL{token $t \in$ sentence}
		\IF{CPOSTAG $t \neq$ 'V'}
			\STATE $t \leftarrow$ NOT\_PRED
		\ELSE
			\IF{POSTAG $t =$ 'VVFIN'}
				\STATE $t \leftarrow$ PRED
			\ELSE
				\STATE FLAG $\leftarrow True$
				\FORALL{token $u \neq t \in$ subclause \textbf{where} $t \in$ subclause}
					\IF{CPOSTAG $u =$ 'V' $\land$ $u$ dependent on $t$}
						\STATE $t \leftarrow$ NOT\_PRED
						\STATE FLAG $\leftarrow False$
						\STATE \textbf{break}
					\ENDIF
				\ENDFOR
				\IF{FLAG $= True$}
					\STATE $t \leftarrow$ PRED
				\ENDIF
			\ENDIF
		\ENDIF
	\ENDFOR
\end{algorithmic}
\end{algorithm}


Letting this algorithm run on the dependence parse tree depicted in figure \ref{fig:example-parzu}
of the sentence

\begin{examples}
	\label{ex:multiple-predicates}
	\item Die Klage wurde abgewiesen, was als Sieg beschrieben werden kann.
\end{examples}

leads to the correct identification of ``abgewiesen'' and ``beschrieben'' as predicates,
ldisregarding the ight and modal verbs ``wurde'', ``werden'', and ``kann'.

%\fig{#1: filename}{#2: label}{#3: long caption}{#4: width}{#5: short caption}
\fig{images/exampleParzu2.png}{fig:example-parzu}{Example ParZu dependency parse tree for a sentence
with multiple predicates. Different Information per word is displayed: The normal word form, the
CPOSTAG, the POSTAG, the lemma of the word, morphological information, and the index.}{16}{Multiple Predicates Dependency Parse Tree}

Basically, the algorithm only takes tokens into account which are verbals (have the CPOSTAG
\emph{V}) ; if the POSTAG is \emph{VVFIN}, i.e. if it is a finitely inflected verb form, it is
right away considered to be a predicate. Otherwise, if it is a verbal form but not finitely
inflected, it is checked whether is is dependent on another verbal element of it's subclause,
if this is the case, it's not labeled as predicate otherwise it is. This leads e.g. to the
correct selection of ``beschrieben'' as predicate in the subclause of sentence \ref{ex:multiple-predicates},
since it forms the ``lowest'' verbal element in this clause, only pointing to light verbs modifying
its grammatical function.


% The condition on line 9, that only tokens in the respective subclause are considered, is
% ensured by making sure that if a token \textit{u}'s POS is ``V'' and it points to its head
% \textit{t}, that it is not itself the head of a subclause --- i.e. its dependency relation
% is e.g. ``relative clause''. If that is the case the token \textit{u} is considered to
% belong to another subclause and therefore not preventing token \textit{t} from getting
% labelled as a predicate. Consider again the example \ref{ex:multiple-predicates}: Let's say
% we are in the for-loop at the token \textit{weitergeleitet}. Because it is a verb but not
% a finite full-verb, we enter the else-clause on line 7. If we were now to loop through all
% token of sentence \ref{ex:multiple-predicates} we would find that token \textit{führt} is a
% verb that points to our primary token. Without the above outlined constraint that only verbs
% in the same subclause pointing to our original verb are preventing it from being labelled
% a predicate, \textit{weitergeleitet} would be labelled as non-predicate. This is obviously
% false. Taking into account the above considerations, we see that although \textit{führt}
% points to \textit{weitergeleitet}, its edge label is \textit{rel} --- which means that it's
% the head of a relative subclause --- therefore it is not anymore in the same subclause and
% \textit{weitergeleitet} gets labelled as predicate.

\subsection{Ensuring Tokenization Equivalence}

Another difficulty I faced was the tokenization differences between different
parsers; this means that it is not always possible to correctly align the tokens
which two parsers produce for the same sequence.
% The consequence of this is that for some
% sentences the SRL tokens cannot be properly combined with their corresponding BERT token.
% The DAMESRL system implements the tokenizer provided by the Natural Language
% Toolkit (NLTK)\myfootnote{\url{https://www.nltk.org/}} which implements a linguistically motivated
% tokenizing. {\color{red} explain what that means}
As shown in figure \ref{fig:architecture}, a sentence passes through two pipelines which both
apply tokenization to it: The ParZu/DAMESRL/GRU and the BERT pipeline. After both have computed
numerical representation for the tokens, these must be combined. However, the tokenization of
ParZu (which would get passed on to DAMESRL and then to the SRL-encoder) and the tokenization of
BERT differ. ParZu implements the \href{http://www.statmt.org/moses/}{Moses tokenizer}, while
BERT, in contrast, utilizes an approach called ``WordPieces'', which is a rather information
processing motivated approach: ``Using wordpieces gives a good balance between the flexibility
of single characters and the efficiency of full words for decoding, and also sidesteps the need
for special treatment of unknown words.'' \citep[p.~2]{wu2016google}. As a consequence, aligning the
corresponding tokens to guarantee that the correct information pieces get combined is
necessary.

To illustrate this, consider the following sentence from the XNLI data set:

\begin{examples}
	\item Im letzten Jahr spendeten Sie \$10.000.
\end{examples}

The tokenizations produced by BERT and ParZu, i.e. Moses, are
depicted side-by-side (note that the BERT tokenization has been
merged for better readability):

\begin{tabularx}{\textwidth}{@{}l<{}@{\ }X@{}}
  \textbf{BERT} (merged) & \textbf{Moses}  \\
  Im                     & Im              \\
  letzten                & letzten         \\
  Jahr                   & Jahr            \\
  spendeten              & spendeten       \\
  Sie                    & Sie             \\
  \$                     & \$              \\
  10                     & 10.000          \\
  .                      & .               \\
  000                    &                 \\
  .                      &                 \\
\end{tabularx}

First, I tried to device an algorithm which would duplicate the respective tokens
in the Moses tokenization to lign up with the BERT (sub-)tokenization. However,
it became clear after some time, that this was too error prone and meticulous,
that I chose the following strategy:

Take the ParZu-tokenized and parsed sentence, apply the predicate-finding algorithm to it.
Then, tokenize the sentence using the BERT tokenizer, identify the afore detected
predicates and hand the BERT tokenized, predicate marked token sequence to DAMESRL.


% The first question that arises is: which tokenization should be mapped onto wich? In other
% words: should we try to align the BERT tokens with the corresponding NLTK tokens or vice
% versa? Let's assume we decide to align the tokenization $T$ with fewer items to the one
% with more items --- in this case this would mean aligning $T_{NLTK}$ with $T_{BERT}$. So,
% the first five tokens are no problem, we can align them by simply doing an exact match and
% confirm that the elements correspond.
% But when we reach the sixth token, the exact match fails. To decide whether the token
% $t_{T_{BERT}}$ or the token $t_{T_{NLTK}}$ was split up --- i.e. to determine which token
% must be copied to ensure tokenization equality ---, we need to do a mutual substring match.
% Doing this, we eoudl find out that ``-'' is a substring of ``-222''. In consequence,
% we align the two, duplicate ``-222'' and compare it with token number 7 in $T_{BERT}$. Since
% ``222'' is a substring of ``-222'', so we align the two of them.

% While it is theoretically possible to align tokens that were differently tokenized by the two
% algorithms, it is nevertheless quite cumbersome. The main problem, however, arises due to the
% [UNK] token BERT introduces for characters --- or character sequences --- which lie out of its
% vocabulary. Since there is obviously no more (sub-)string comparison possible, the process
% gets even more complicated: Suppose you have duplicted the ``-222'' in the NLTK column and are
% now on line 7. In the BERT tokenization you see the ``[UNK]'' token, while in the NLTK you see
% a ``\textdegree{}C''. To find out, what all is containt in the ``[UNK]'', you need to look at
% the token before and after it in the BERT tokenization and compare it with the respective NLTK
% tokens. since the the and so on....
% % \myfootnote{Of course, one could heuristically {\color{red}
% % go on until no [UNK] token is encountered align up in between.}}
% % Eventually, I decided to simply feed into DAMESRL
% the BERT-tokenized sequences, to get around this issue.


% \begin{tabularx}{\textwidth}{@{}l<{}@{\ }X@{}}
%   \textbf{BERT} (merged) & \textbf{NLTK} \\
%   Die                                        & Die \\
%   mittlere                                   & mittlere \\
%   Oberflächentemperatur                      & Oberflächentemperatur \\
%   wird                                       & wird \\
%   auf                                        & auf \\
%   -                                          & -222 \\
%   222                                        & \textcolor{blue}{-222}\\
%   {[}UNK{]}                                  & \textdegree{}C \\
%   (                                          & ( \\
%   {\fontfamily{ptm}\selectfont\texttildelow} & {\fontfamily{ptm}\selectfont\texttildelow}51 \\
%   51                                         & \textcolor{blue}{{\fontfamily{ptm}\selectfont\texttildelow}51} \\
%   K                                          & K \\
%   )                                          & ) \\
%   geschätzt                                  & geschätzt \\
%   .                                          & .
% \end{tabularx}


\subsection{DAMESRL}

There are not too many SOTA SRL frameworks available for German that come with a pre-trained model,
especially such ones that can be conveniently integrated in a pipeline of a bigger system.

\cite{do2018flexible} fill exactly this hole: They introduce DAMESRL, an SRL framework that
implements SOTA architecture, namely self-attention mechanisms, similar to BERT's. They report
an F1 score of 73.5 for their best model configuration on the German data set of CoNLL '09.
This best configuration is based on word as well as character embeddings, self-attention and a
softmax layer on top.

The DAMSRL predictor receives the BERT-tokenized sentence along with the information which tokens
in it are predicates (zero or more). For each token labelled as predicate in a sequence it predicts
for each other token in the sequence its SRL.

\fig{images/num_predarg_structs.pdf}{fig:num-predarg-structs}{Number of predicate-argument structures in all data sets.
                                                              Due to it's boilerplate template form, deISEAR shows a peculiar distribution:
                                                              Since it's examples always begin with ``Ich [\textsubscript{PRED} fühlte] $X$, als ....'',
                                                              it's guaranteed that at least one predicate is identifed, and it is very probable,
                                                              because of this sentence structure, that another will occur.
                                                              The other curious pattern exhibits SCARE: In no other
                                                              data set the amount of sentences where ParZu couldn't detect any predicates
                                                              is that high.

                                                              Besides the noted peculiarities, it is safe to say that setting the maximum number of
                                                              predicate-argument structures to three does probably not lead to much information loss;
                                                              on average over all datasets, 92.73\% of all sentences possess three or less such structures.}{14}{Predicate-Argument Structures}


In most cases, a sentences contains not exactly one predicate which distributes semantic roles,
but several, especially in longer sentences, --- or even none, especially in colloquial, short
sentences. Research by \citeauthor{zhang2019semantics} suggests that fixing the number of
predicate-argument structures to three yields the best results; so I adopt this number. In
other words, if a sentence has more than three argument-predicate structures, I only care about
the first three predicates identified (if proceeding from left to right through the sequence),
and disregard the others.



\begingroup
\begin{srl}[!h]
  \begin{minipage}{0.45\linewidth}
  \vspace{0pt}
    \begin{BVerbatim}[commandchars=\\\{\}, fontsize=\footnotesize]
            Slot 1   Slot 2   Slot 3

Wir         B-A0     0        0
wollten     O        0        0
eine        B-A1     0        0
Sache       I-A1     0        0
mehr        I-A1     0        0
retten      B-V      0        0
als         B-C-A1   0        0
die         I-C-A1   0        0
Restlichen  I-C-A1   0        0
.           O        0        0
      \end{BVerbatim}
  \end{minipage}
  % \hfill
  \begin{minipage}{0.45\linewidth}
  \vspace{0pt}
    \begin{BVerbatim}[commandchars=\\\{\}, fontsize=\footnotesize]
                    Slot 1   Slot 2   Slot 3

        Wir         B-A0     B-A0     B-A0
        wollten     O        O        O
        eine        B-A1     B-A1     B-A1
        Sache       I-A1     I-A1     I-A1
        mehr        I-A1     I-A1     I-A1
        retten      B-V      B-V      B-V
        als         B-C-A1   B-C-A1   B-C-A1
        die         I-C-A1   I-C-A1   I-C-A1
        Restlichen  I-C-A1   I-C-A1   I-C-A1
        .           O        O        O
    \end{BVerbatim}
  \end{minipage}
\end{srl}
\captionof{srl}{The two strategies for dealing with less than three predicates: \textbf{Left}:
                The open SRL slots get filled with the special SRL \emph{0}.
                \textbf{Right}: The first SRL structure gets duplicated until all slots
                are filled.}
\label{srl:zeros-duplicate}
\endgroup


However, if there are fewer than three predicate-argument structures
present, I test and report results for two strategies: The first solution (the left sentence
in SRL \ref{srl:zeros-duplicate}) lies in filling the ``unfilled'' predicate-slots with the
special ``0''-SRL. The second (the right sentence in SRL \ref{srl:zeros-duplicate}) simply
copies the first predicate-argument structure to the unfilled slots, thus amplifying the signal
from the first predicate-argument structure.



\subsection{GRU}

Finally, the predicted SRLs need to be encoded in a numeric way so they can be concatenated
to the vectors which are computed by BERT. The ``classic'', pre-transformer age times, way
of encoding sequential data would be to employ a recurrent neural network architecture.
I decided to use GRUs, since they are less computational intesive and research has found
both architectures for many tasks and datasets performing on par (cf. \cite{chung2014empirical}).

For this, the three SRLs for each token get transformed into their numerical representation
via look-up in the SRL dictionary of the GRU model, such that euch SRL token is a vector $v \in
\mathbb{R}^{20}$. Then, the numerical representations for each token get concatenated and
those 60-dimensial vectors form then the input for the bi-directional two-layer GRU which
computes hidden states for all the inputs.

Because of the subsequent goal of combining BERT embeddings with SRL embeddings, the following
considerations need to be taken into account.


BERT adds several special meta tokens to sequences it embeds: (1) At the start of each
seqeuence, be it a single sentence or sentence pair one, it inserts a special [CLS] token; CLS
standing for classification. (2) At the the end of a sequence and between sentence pairs, BERT
insert a separation token [SEP], which signals the end of a sequence. (3) If the sequence
is shorter than the defined maximum length, it gets filled up with [PAD] tokens. An XNLI sentence
pair with maximum length 25 woudl thus be represented as tokenizer BERT sequence the following
way (for better readability, the BERT sutokens are surrounded by brackets):

\begin{examples}
  \item Du musst dort nicht bleiben.\\
        An genau der Stelle musst du stehenbleiben!

        [CLS] [Du] [muss] [\#\#t] [dort] [nicht] [bleiben] [.] [SEP] [An] [genau] [der] [Stelle] [muss] [\#\#t] [du] [stehen] [\#\#bleiben] [!] [SEP] [PAD]  [PAD]  [PAD]  [PAD]  [PAD]
\end{examples}

One of the goals of my experiments is to compare the standard BERT implementation with the SRL
enriched GLiBERT variant. Since the vanilla classification head predicts only using the last
hidden state of the [CLS] token I need to represent SRL information also on this special
token. Therefore, I follow \cite{zhang2019semantics} and add a meta-SRL [CLS], which represents
the meta semantic role of the [CLS] special token.

Another question that arises concerncs the SRL encoding of multiple sentences:
Suppose we have a sequence consisting of several sentences (highlighted with brackets):

\begin{examples}
  \item {[}\textsubscript{A} Die Ereignisse von Oni finden im oder nach dem Jahr 2032 statt und beschreiben ein dystopisches Zukunftsbild der Erde.] [\textsubscript{B} Die Welt ist so verschmutzt, dass nur noch kleine Teile bewohnbar sind.] [\textsubscript{C} Um die internationale Wirtschaftskrise zu lösen, haben sich alle Völker unter einer Weltregierung vereinigt.]
\end{examples}

Should each sentence  $A$, $B$, and $C$ be encoded separately and the represenatations than
be combined with the aligned BERT subtokens? Or is it more
effective to ``glue'' all sentences together (or rather, the concatenated SRL tokens in those
sentences) and embedd this sequence? Further, also meta SRLs for [SEP] and [PAD] tokens could be added ---
so that also sentence pair tasks are embedded as one long sequence of SRL tokens. All experiments
except for XQuAD $\alpha$ were conducted implementing the latter approach, proving that the model
profits more from the second architecture.



\section{combination}

A crucial part in the overall architecture is the combining of the numeric representation
of (sub-)words computed by the BERT network and the embedded SRLs. One difficulty lies in
the fact that, as already mentioned above, BERT has its own tokenizer which implements
a so-called sub-word or wordpiece \cite{wu2016google} encoding strategy: BERT has a
fixed length of vocabulary it can hold, namely 30,000 tokens. The wordpiece tokenization
approach is a balance between word and character tokenization in that that it ``gives
a good balance between the flexibility of single characters and the efficiency of full
words for decoding, and also sidesteps the need for special treatment of unknown words''
\citep[p.~2]{wu2016google}. As a consequence, BERT tokenizes a sentence quite differently
than a traditional parser, since the latter adheres to the full tokens. Consider the
following example:

\begin{examples}
	\label{ex:tokenization-diff}
	\item Es ist der Sitz des Bezirks Zerendi in der Region Akmola.
	\item ParZu: Es ist der Sitz des Bezirks Zerendi in der Region Akmola .
	\item GermanBERT: Es ist der Sitz des Bezirks Zer \#\#end \#\#i in der Region Ak \#\#mol \#\#a .
\end{examples}


A further challenge besides the alignment of traditional tokenization and wordpiece tokenization is
the general difference in parsing a sentence that exist.

\section{Head Module}

Since the vanilla classification and question answering heads differ quite
starkly --- classification only takes into account the [CLS] token while
the start end end index probabilities need to be computed on every token
in the sequence --- I cover them in separate subsections.

Additionally, for classifiaction tasks, I found more ways in tackling these
with different heads, while for the question answering task I only saw one
possibility for a GliBERT head which is virtually the vanilly head, except
that SRL information may be added to the BERT subtokens.


\subsection{Classifiction}

% While for question answering there was little tweeking needed to adapt to the extended BERT
% embeddings, for classification the situation looks a bit more complex. The standard BERT way
% of doing classification tasks runs as follows:

% \begin{itemize}
%   \item Prepare the data: add a [CLS] token at the beginning, a [SEP] token between the two sentences (if there are), and pad with the [PAD] token
%   \item Send the prepared examples through the BERT network
%   \item Select only the embedding for the first token  --- i.e. the [CLS] ---, send it through a dense layer with a softmax and predict the class for this example
% \end{itemize}

% \cite{devlin2018bert} visualize this as can be seen in figure \ref{fig:BERT-classification}.


% The problem now is that in the above described standard implementation, there is no straigthforward
% way to enrich the BERT embeddings with SRLs, since the only embedding that is used for prediction
% is the [CLS] token; since this is a special BERT token it is not present in the original sentence
% and, therefore, it does not have a corresponding SRL. However, as I lais out in the section before,
% I add several meta SRLs which are added to the regular SRLs, to have numerical SRL representations
% also for these special tokens.

For both, single sentence and sentenc pair tasks, the vanilla BERT head considers
only the last hidden state of the [CLS] token, after the sequence was sent through
the transformer blocks and predicts class probabilities only relying on the
information present in this vector $\in \mathbb{R}^{786}$:

\fig{images/bert_classification.png}{fig:BERT-classification}{Schema for sentence pair (left) and single sentence (right) classification. Figure taken from \citep{devlin2018bert}.}{13}{BERT-Classification}

Follwing, I present the three classification heads I used for my experiments: The [CLS]
head, similar to the vanilla classifiaction head, the FFNN head, which consideres the
numeric representation of all tokens in the sequence, and the GRU head, which implements
a biderectional recurrent neural network which computes a condensed representation of
the whole sequence before predicting the actual class porbabilities.\myfootnote{Early in
the experimental phase, I also devised a CNN head, however, the preliminary results were
not promising, so i concentrated on the other three heads.}



\subsubsection{[CLS] Head}

\cite{zhang2019semantics} follow in their implementation the vanilla BERT classification
head, as proposed by \cite{devlin2018bert}:

After adding corresponding SRLs for the special BERT tokens and embedding the SRL sequence,
the last hidden state representation for the BERT+SRL combind [CLS] token is selected
and softmax-scaled probabilities for all classes are computed.

% In their paper for SemBERT, \cite{zhang2019semantics} do not really address the
% issue laid out above. To the contrary, the differnt pieces of information they
% provide are rather conflicting, only after inspecting the code they released on
% \href{https://github.com/cooelf/SemBERT/}{GitHub}, the picture somewhat cleared:

% After predicitng the SRLs for a given input, they add pseudo-SRLs for the [CLS] and [SEP] tokens.
% In the look-up table of the BiGRU that consumes the SRLs, they then simply add the corresponding
% keys --- so that besides regular SRLs as ``B-V'' (beginning of predicate) or ``I-A0'' (inside
% or end of argument zero), there are also the labels ``[CLS]'' and ``[PRED]'' After sending this
% sequence through the BiGRU, they concatenate the two hidden states of the [CLS] SRL with the
% [CLS] BERT embedding and predict on that vector % Applying this strategy, now there \emph{is} a
% SRL for the [CLS] token after the sequence was sent through the BiGRU which then can be appended
% to its BERT embedding vector

Formally, the [CLS] Head is a one layer Fullyconnected Feedforwrd Neural Network. The
prediction is the results of sending the embedding of the [CLS] token through the network:
$\hat{y} = softmax(L_1)$. The input dimensions of the layer vary according to the setting,
i. e. if the SRLs are included, or not. if not, then the demsion is simply the standard
BERT hidden size $h_i \in \mathbb{R}^{768}$, if the SRL embedding is appended, then $h_i \in
\mathbb{R}^{788}$. the output dimensionality of the network depends on the number of classes
for the task: This number ranges from 2 (PAWS-X, \emph{true}, \emph{false}) to 7 (deISEAR,
\emph{Traurigkeit, Scham, Freude, Angst, Wut, Ekel, Schuld}).


\fig{images/head-cls.png}{fig:cls-head}{Head with a one-layer \textbf{F}eed \textbf{F}orward \textbf{N}eural
    \textbf{N}etwork on the [CLS]-token}{10}{{[}CLS{]} Head}

Intuitively, I expected this head to not show too big differences between the +SRL $-$SRL settings,
since the SRL information, which is essentially a relational, sequential mark-up of the sequence is
boied down into one token. However, as is also confirmed by the results of \cite{zhang2019semantics},


\subsubsection{FFNN Head}

FFNN stands for Feed Forward Neural Network. While th [CLS] Head procudes predictions based on the
weights of the last layer output of the [CLS]-token, this head takes the last layer output of all
tokens and concatenates them.
While the approach implemented by \cite{zhang2019semantics} is able to improve the vanilla
BERT approach, it does not lead to an improvement on others, or worse, brings the perormance
down. I suspect a reason for this may lie in the manner of how the SRLs are processed in this
approach. The information SRLs provide is, what may be called, sub-sentence specific and
cannot be adequately represented as a single information piece. By this I mean that it does
not suffice to know that given an utterance $x$ that there is a specific SR in it; rather the
information \emph{where} is crucial. Consider the following example (the pseudo SRL [CLS] is
added):


[\textsubscript{[CLS]} ] [\textsubscript{A-0} The man] [\textsubscript{predicate} asked] [\textsubscript{A-1} his friend] .

After the subscripted SRLs were consumed by the BiGRU, there is some information about all
SLRs in the hidden states of the [CLS] token. While there may be some information about there
being a predicate, an argument zero, and an argument 1 present, it is completely impossibly
to determine from which tokens these signals came. Especially in sentence pair tasks, such
as paraphrase identification, this information is however absolutely crucial. As can be seen
from results \ref{tab:results}, this hypothesis is also supported by the results:

\fig{images/head-lhsa.png}{fig:lloa-head}{Head with a one-layer \textbf{F}eed \textbf{F}orward \textbf{N}eural \textbf{N}etwork
    on the concatenated token sequence.}{10}{FFNN Head}

As has been shown by e.g. \cite{myagmar2019transferable} for sentiment analysis, a simply final
fully-connected feed forward layer produces fairly good results (in fact, it performs the best
in the different architectures they tested for their task).

Implementing a fully-connected feed forward network as final layer counters the problem of the
information deprecation that is present in the [CLS] approach: Every token's BERT embedding gets
concatenated with the token's SRL embeddding. The whole sequence is then flattened, i.e. all
BERT+SRL vectors get concatenated into one large vector of size $\mathbbm{R}^{n\times768+20}$.


\subsubsection{GRU Head}

Since the SRLs are essentially a sequential ``mark-up'' of the sentences, the thought of
encoding them with an architecture designed for sequential data is not too far. Inspired by
the biological properties of the brain, the concept of recurrent neural networks has been
around since the late 80ies, with \citep{hopfield1982neural} often being credited as having
implemented the first recurrent neural network. To overcome the problems of the vanishing
and explofing gradient problems, \citep{hochreiter1997long} proposed the LSTM architecture.
\citep{cho2014learning} GRU

\fig{images/head-gru.png}{fig:gru-head}{Head with a one-layer \textbf{F}eed \textbf{F}orward \textbf{N}eural
    \textbf{N}etwork on the concatenated last hidden states of the bi-directional GRU.}{10}{GRU Head}





\subsection{Question Answering}
\label{sec:question-answering}



``\textelp{} in the question answering task, we represent the input question and passage as
a single packed sequence, with the question using the A embedding and the passage using the
B embedding. We only introduce a start vector $S \in \mathbb{R}^H$ and an end vector $E \in
\mathbb{R}^H$ during fine-tuning. The probability of word $i$ being the start of the answer
span is computed as a dot product between $T_i$ and $S$ followed by a softmax over all of the
words in the paragraph: $P_i = \frac{e^{S\cdot T_i}}{\sum_{j}^{} e^{S\cdot T_j}}$.'' \citep{devlin2018bert}

% \fig{images/bert_qa.png}{fig:BERT-QA}{Vanilla BERT question answering head. Figure taken from \citep{devlin2018bert}.}{13}{BERT-QA}

\subsubsection{Span Prediction Head}

The ``standard'' implementation of the Q\&A BERT head consists of one one-layer FFNN which predicts
on each input token the probability of it being the start of the answer span and the end of the
answer span, respectively. After both probabilities are computed for all tokens, the ones with
the highest probability get selected; no further logic is enforced, such as that the index of
the start token should be smaller than the one of the end token, or that the answer span may
not lie inside the question (i. e. before the first [SEP] token), etc.

\begin{wrapfigure}[21]{r}{0.45\linewidth}
  \begin{center}
    \includegraphics[width=1.0\linewidth]{images/bert_qa.png}
  \end{center}
  \stepcounter{myfigure}
  \caption[BERT Q\&A]{Vanilla BERT question answering head. Figure taken from \citep{devlin2018bert}.}
\end{wrapfigure}

Since the standard implementation already implements a model that predicts on each token,
the SRL-emgedding enriching is relatively straight forward:
First, the input layer of the small head FFNN needs to be adapted to the BERT-token + SRL
dimensionality, which results in the vector representation $\mathbb{R}^{768+20}$ for each
SRL-enriched token. Secondly, when the setting of merging the BERT subtokens before adding
the SRL embeddings is used, the start and end span indexes have to be recomputed.


\fig{images/head-span_pred.png}{fig:Span-Prediction-Head}{The GliBERT head for span prediction
    in Question Answering Tasks. After the Tokens and SRLs were consumed by BERT and the SRL
    embedding module, one FFNN predict on each token in the sequence, how likely it is first
    and the last token of the answer span. After these predictions have been made for all tokens, the token
    with the highest value for each position gets selected.}{10}{Span Prediction Head}



