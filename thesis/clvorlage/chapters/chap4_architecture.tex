
\newchap{Architecture}
\label{chap:4_architecture}

In this chapter, I describe the different parts of the GliBERT model, how they
interact and what a priori decisions were taken during designing its architecture
(e.g. the number of predicate-argument structures considered, etc.).

To eliminate possible misunderstandings and establish a ``standard'' vocabulary discussing the GliBERT
architecture, I define the following terms which will be used in the specified sense throughout
this thesis:

\begin{itemize}
  \item Following \cite{devlin2018bert}, in my thesis ``a `sentence' can be an arbitrary span of contiguous text, rather
    than an actual linguistic sentence.''
  \item A ``model'' or ``system'' denotes any algorithm that is tailored at a specific task and designed
    to handle natural language as input.
  \item As mentioned in the introduction, natural language can be communicated through different channels (speech, singing, text)
    --- I am, however, only concerned with textual representations of language.
\end{itemize}


% CONTINUATION:
% A `sequence' refers to the input token sequence to BERT,
% which may be a single sentence or two sentences packed together.''

\section{Overview}

GliBERT is an architecture that combines different, pre-existing models and
tools to solve the classification or question answering task at hand. The
general way an input sequence is processed by GliBERT is depicted in figure
\ref{fig:architecture}:


\fig{images/architecture_grob}{fig:architecture}{General architecture of GliBERT, exemplified for  the deISEAR task.}{14}{GliBERT Architecture}

An input sentence gets processed in parallel by two systems, the vanilla German BERT
and the SRL producing and encoding module, to produce two numerical representations
of its tokens. These represenatations get combined and are sent through a head module
which producec the actual predictions. The core parts of the model are the following:

\begin{description}
	\item[\textbf{BERT module}] This is the vanilla BERT base model: It tokenizes the input sequence, sends it through its twelve transformer layers and outputs the final hidden states of each (sub-)token.
  \item[\textbf{SRL module}] This modules actually consists of three submodules: First, the sequence is processed by the \href{https://github.com/rsennrich/ParZu}{ParZu} \citep{sennrich2009new} parser to identify predicates.
	Secondly, the sequence along the information about which tokens are predicates is handed to the \href{https://liir.cs.kuleuven.be/software_pages/damesrl.php}{DAMESRL} model \citep{do2018flexible} which predicts actual SRLs.
	To ensure there are no tokenization mix-ups between BERT and DAMESRL (because these differences are not reversible as will be seen later), the sequence gets tokenized BERT-style and is passed as this list of tokens to DAMESRL. In a last step,
	the SRL squence gets numerically encoded, using a bi-directional two-layer GRU.
  \item[\textbf{combination}] In this step the BERT and SRL representation get combined: to do this, the embeddings need to be processed, i.e. splittet or merged, respectively, so that they can be concatenated.
	For this, there exist two approaches:
	(A) Fuse the subtokens of BERT back to tokens, or (B) split the SRLs according to the subtokens of BERT.
  \item[\textbf{Head module}] At last, the combined token representations of the input is fed through the final network that transforms it to predict task-dependent output. Several architectures can potentially be applied here: FFNNs, GRUs, CNNs, etc.
\end{description}

\begin{landscape}\centering
  % \vspace*{\fill}
  \fig{images/architecture.png}{fig:architecture-big}{
    Detailed architecture of GliBERT: On the left, the input sentence is passed through two paths:
    On top, through the German BERT, with the optional subtoken fusion module on top. On the bottom,
    through ParZu and DAMESRL, with subsequent embedding via a GRU model; after that an optional
    split token module follows. The bold arrows on the right side show information flow, if BERT
    subtokens are fusioned for appending with SRLs. The dotted arrows represent the information flow
    if the SRLs are splitted to match with the BERT subtokens.}{24}{GliBERT Architecture detail}
  % \vfill
\end{landscape}


\section{BERT Module}

Since its publishing two years ago, BERT \citep{devlin2018bert} has
often been viewed as a turning-point in NLP: The embeddings it computes
by implementing massive self-supervised pre-training proved to be very
potent representations of language and were successfully implemented in
a wide array of applications addressing downstream tasks via transfer
learning (see chapter \ref{chap:2_approach}). Pretrained models and
APIs for BERT are by now vastly available for a multitude of langauges
--- I chose to use the \texttt{bert-base-german-cased} model from
\href{https://deepset.ai/german-bert}{deepset} which is available
in \texttt{pyTorch} through the hugging face's \texttt{transformer}
library\cite{wolf2019transformers}.

While the original BERT was presented in two different sized variants --- \emph{base}
and \emph{large} --- deepset only provides a BERT base model which has the following
specifications according to it's configuration file:

\tab{tab:bert-yuperconfigs}{German BERT hyperparameter configuration.}{
  \scalebox{0.9}{
    \begin{tabular}{ll}
      Transformer Blocks         & 12             \\
      hidden Size                & 768            \\
      hidden activation function & GeLu           \\
      hidden dropout probability & 0.1            \\
      Attention Heads            & 12             \\
      Vocabulary size            & 30,000 (cased) \\
      Total Parameters           & 110 million    \\
    \end{tabular}
  }
}{German BERT hyperparameter configuration}

The handling of the BERT model is straightforward through huggingface's \texttt{transformer}
library: With a simple function call \texttt{BertModel.from\_pretrained()} one loads the
pretrained BERT, and with another function, \texttt{BertTokenizer.from\_pretrained()},
one instantiates the BERT tokenizer.
After encoding a sentence using the tokenizer's method \texttt{.encode\_plus()}, the
encoded sentence is sent through BERT via its \texttt{.forward()}-method --- or
called implicitly, by passing the sentence to the model --- which returns the
vectors for all input tokens, which can then be used in downstream tasks. Fine-tuning
is done by passing the computed loss to the specified optimizer funtion (I use the
AdamW optimizer \citep{loshchilov2019decoupled}, a modification of the well-known
Adam (Adaptive Moment Estimation) optimizing function \citep{kingma2014adam}, implementing
additional weight decay), which updates BERT's weight matrices.\myfootnote{After wiring
all the different parts together, GliBERT is one big model having one loss function, which means that all
weights of all layers in the system get update by the AdamW optimizer, not just BERT.}

In the \href{https://github.com/JonathanSchaber/Masterarbeit}{GitHub} repository, in
the file \texttt{load\_data.py}, the data gets tokenized and loaded, and in
the file \texttt{gli\_bert.py}, the forward pass and weight-updating is
defined in the \texttt{fine\_tune\_BERT()} function.


\section{SRL Module}
\label{sec:srl-module}

A Semantic Role Labeller (SRL) is a system, that assigns automatically Semantic Roles to a
given input text.\myfootnote{This may be one or multiple sentences.}

State-of-the-art SRLs are end-to-end models, in the sense that there is no need of
complex pre-processing of the input sentence, such as POS-tagging, syntax parsing, etc.
in advance before the actual labeling takes place. However, as is the case for the model
I employ, some pre-processing remains. For GliBERT, I implement the DAMESRL, a model
presented by \cite{do2018flexible}. I use their pre-trained German Character-Attention
configuration which, according to the authors, achieved an F1 score of 73.5\% on the
CoNLL'09 task \citep{hajivc2009conll}. Despite bein g characterized as an end-to-end
model by the authors, their SRL needs as input not only the tokenized sentence, but also
``its predicate $w_p$ as input'' \citep{do2018flexible}. In other words, DAMESRL expects
as input a sentence $s$ as list of tokens $[ t_1, t_2, \dotsc t_{n-1}, t_n ]$, where for
each token there is an attribute defined whether it functions as predicate in $s$ or
not.

% ``A major advantage of dependency grammars is their ability to deal with languages that are
% morphologically rich and have a relatively free word order.'' \citep[p.~274]{jurafsky2019speech}
For extracting predicates, I rely on the dependency parse tree the ParZu parser
\cite{sennrich2013exploiting} generates for a German sentence. Given the parsed
sentence, I need to decide which tokens in it are predicates, and which are not.
While this may seem like a straightforward task --- just find the verb as in a
simple sentence like ``He \emph{ate} the apple.'' ---, there are actually a few
caveats that need to be considered, e.g. (predicates are emphasised): (1) There
may be no predicates at all: ``What a day!''. (2) There might be more than one
predicate: ``We \emph{saw} her \emph{leave} the room''. (3) Not all verbs might
be predicates, e.g. because they play grammatically the role of a light verb: ``I
can \emph{hear} you''. In the following section, I will describe how I tackle
these problems by making use of the parse tree information of ParZu.


\subsection{ParZu}

% It is a known problem in the analysis of semantic roles that a proper procedure for predicate
% identification is a problem hard to tackle, consider e.g. the discussion concerning so called
% light verbs: \cite{wittenberg2016light}.

Before analyzing which semantic roles are present in a given sentence, one
must determine the predicates in this sentence: ``First, the predicates which
assign semantic roles to the constituents are identified prior to semantic
role labeling proper. They are usually identified as the main verbs which
head clauses'' \citep[p.~74]{samardzic2013dynamics}. In a dependency framework
like the Universal Stanford Dependencies (USD) \citep{de2014universal},
which explicitly sets the content verb as root,\myfootnote{Note that is not
undisputed:``The parsing scheme that USD advocates takes the division between
function word and content word as its guiding principle. One major difficulty
with doing this is that the dividing line between function word and content
word is often not clear'' \cite{gross2015dependency}.} identification of the
relevant predicate is straightforward: One needs just to look at the dependency
parse tree of a given sentence and select the verbal heads --- i.e. roots ---
of the clauses. However, the ParZu parser models not content verbs as heads but
function verbs.\myfootnote{This follows general dependency frameworks proposed
for German, e.g. \cite{gerdes2001word, gross2015dependency}.} In other words,
in the ``He was hit by a ball.'', ``hit'' would be the predicate that assigns
the semantic roles of proto-patient ``he'' and theme, or instrument ``a ball''.
The dependency parse tree produced by implementing the USD framework would
analyze the word ``hit'' as being the root of this sentence, making it easy to
forward the such annotated sentence to a semantic role labeler which accepts as
input a list of tokens, and marked which ones are the predicates.


Since the parse tree of the German equivalent ``Er wurde von einem Ball getroffen'' produced
by ParZu analyzes the word ``wurde'' as root of the tree, it does not make sense
to forward this as predicate to DAMESRL.

% (interestingly, this stands in contrast to the Pro3Gres parser \citep{schneider2008hybrid} which

% ``In a constituency parse, the finite verb is the head of a verb phrase or rather sentence.
% A dependency parse, on the other hand, does not consider auxiliaries as heads and therefore
% finite verbs are usually not the head of the sentence.  Hence, the head of a sentence typically
% is the verb containing the meaning.  In that sense, dependency structures are closer to the
% semantics of a sentence.'' \citep[p.~6f.]{aepli2018parsing}

% According to the USD, function words are subordinated to content words, which means that
% in a sentence ``He was hit by a ball.'', the infinite participle \textit{hit} would be
% analysed as root, not the finitely inflected \textit{was}.  This is an accordance with the
% view that XXXXXXXXXX However, there is a ``substantial amount of evidence [that] delivers a
% strong argument for the \textelp{} approach, which subordinates full verbs to auxiliaries''
% \cite{gross2015dependency}.

I propose the following algorithm \ref{alg:find-predicates} deciding whether a verb in a
ParZu-parsed sentence is or is not a predicate using a heuristic, relying on the token's
POS tag that the parser predicts. The ParZu parser's default output follows the CoNLL
scheme \citep{buchholz2006conll} which means that there are two levels of POS tagging:
coarse-grained (CPOSTAG) and fine-grained (POSTAG), where the POSTAG corresponds to the
token's STTS tag \citep{schiller1999guidelines}.

\begin{algorithm}
\caption{Predicate finding algorithm}
\label{alg:find-predicates}
	\begin{algorithmic}[1]
	\FORALL{token $t \in$ sentence}
		\IF{CPOSTAG $t \neq$ 'V'}
			\STATE $t \leftarrow$ NOT\_PRED
		\ELSE
			\IF{POSTAG $t =$ 'VVFIN'}
				\STATE $t \leftarrow$ PRED
			\ELSE
				\STATE FLAG $\leftarrow True$
				\FORALL{token $u \neq t \in$ subclause \textbf{where} $t \in$ subclause}
					\IF{CPOSTAG $u =$ 'V' $\land$ $u$ dependent on $t$}
						\STATE $t \leftarrow$ NOT\_PRED
						\STATE FLAG $\leftarrow False$
						\STATE \textbf{break}
					\ENDIF
				\ENDFOR
				\IF{FLAG $= True$}
					\STATE $t \leftarrow$ PRED
				\ENDIF
			\ENDIF
		\ENDIF
	\ENDFOR
\end{algorithmic}
\end{algorithm}


Letting this algorithm run on the dependence parse tree depicted in figure \ref{fig:example-parzu}
of the sentence

\begin{examples}
	\item \label{ex:multiple-predicates} Die Klage wurde abgewiesen, was als Sieg beschrieben werden kann.
\end{examples}

leads to the correct identification of ``abgewiesen'' and ``beschrieben'' as predicates,
disregarding the light and modal verbs ``wurde'', ``werden'', and ``kann'.

%\fig{#1: filename}{#2: label}{#3: long caption}{#4: width}{#5: short caption}
\fig{images/exampleParzu2.png}{fig:example-parzu}{
  Example ParZu dependency parse tree for a sentence with two predicates out of several verbal
  forms. Different Information per word is displayed: The normal word form, the CPOSTAG, the
  POSTAG, the lemma of the word, morphological information, and the index.
}{16}{Multiple Predicates Dependency Parse Tree}

Basically, the algorithm only takes tokens into account which are verbals (have the CPOSTAG
\emph{V}) ; if the POSTAG is \emph{VVFIN}, i.e. if it is a finitely inflected verb form,
it is right away considered to be a predicate. Otherwise, if it is a verbal form but not
finitely inflected, it is checked whether it is dependent on another verbal element of its
subclause, if this is the case, it's not labeled as predicate, otherwise it is. This leads
e.g. to the correct selection of ``beschrieben'' as predicate in the subclause of sentence
\ref{ex:multiple-predicates}, since it forms the ``lowest'' verbal element in this clause,
only pointing to a light verb which modifies its grammatical function, which in turn points
to another light verb which is the head of the subclause.


% The condition on line 9, that only tokens in the respective subclause are considered, is
% ensured by making sure that if a token \textit{u}'s POS is ``V'' and it points to its head
% \textit{t}, that it is not itself the head of a subclause --- i.e. its dependency relation
% is e.g. ``relative clause''. If that is the case the token \textit{u} is considered to
% belong to another subclause and therefore not preventing token \textit{t} from getting
% labelled as a predicate. Consider again the example \ref{ex:multiple-predicates}: Let's say
% we are in the for-loop at the token \textit{weitergeleitet}. Because it is a verb but not
% a finite full-verb, we enter the else-clause on line 7. If we were now to loop through all
% token of sentence \ref{ex:multiple-predicates} we would find that token \textit{führt} is a
% verb that points to our primary token. Without the above outlined constraint that only verbs
% in the same subclause pointing to our original verb are preventing it from being labelled
% a predicate, \textit{weitergeleitet} would be labelled as non-predicate. This is obviously
% false. Taking into account the above considerations, we see that although \textit{führt}
% points to \textit{weitergeleitet}, its edge label is \textit{rel} --- which means that it's
% the head of a relative subclause --- therefore it is not anymore in the same subclause and
% \textit{weitergeleitet} gets labelled as predicate.

\subsection{Ensuring Tokenization Equivalence}

Another difficulty I faced was the tokenization differences between different parsers; which can
lead to sitautions where it is actually impossible to correctly automatically align the tokens
which two parsers produce for the same sequence.
% The consequence of this is that for some
% sentences the SRL tokens cannot be properly combined with their corresponding BERT token.
% The DAMESRL system implements the tokenizer provided by the Natural Language
% Toolkit (NLTK)\myfootnote{\url{https://www.nltk.org/}} which implements a linguistically motivated
% tokenizing. {\color{red} explain what that means}
For GliBERT, this problem occurs at the interface where the embeddings of the BERT
module and the embeddings for the SRLs, which are based on the ParZu tokenization,
need to be aligned. As shown in figure \ref{fig:architecture}, a sentence passes
through two pipelines which both apply tokenization to it: The ParZu/DAMESRL/GRU
and the BERT pipeline. After both have computed numerical representation for the
tokens, these must be combined. However, the tokenization of ParZu (which would
get passed on to DAMESRL and then to the SRL-encoder) and the tokenization of BERT
differ. ParZu implements the \href{http://www.statmt.org/moses/}{Moses tokenizer},
while BERT, in contrast, utilizes an approach called ``WordPieces'', which is a
rather information processing motivated approach, rather then the linguistically
motivated Moses: ``Using wordpieces gives a good balance between the flexibility of
single characters and the efficiency of full words for decoding, and also sidesteps
the need for special treatment of unknown words.'' \citep[p.~2]{wu2016google}. As
a consequence, aligning the corresponding tokens to guarantee that the correct
information pieces get combined is necessary.

To illustrate this, consider the following made-up but not unplausible text snippet:

\begin{examples}
	\item \label{itm:tok-sentence} Anstiege um 4° zwischen 1990-2010
\end{examples}

The tokenizations produced by ParZu, i.e. Moses, and BERT are
depicted side-by-side (note that the BERT tokenization has been
merged for better readability):

% \begin{tabularx}{\textwidth}{@{}l<{}@{\ }l<{}@{\ }X@{}}
\tab{tab:tokenization}{Tokenization aligning difficulties displayed on example sentence
\ref{itm:tok-sentence}. Note that the BERT subtokens are already merged back to tokens for
better readability (``Anstieg'', ``\#\#e'' = ``Anstiege''). In this case both tokenization
sequences would need duplication (duplicated tokens are highlighted in blue).
Achieving this reliably for all potential edge cases is nearly impossible.}
{\begin{tabularx}{\textwidth}{m{3.4cm}m{3.4cm}|m{3.4cm}m{3.4cm}}
  \multicolumn{2}{c}{\textbf{normal}} & \multicolumn{2}{c}{\textbf{aligend}} \\
  \texttt{BERT}  & \texttt{Moses}     & \texttt{BERT}                    & \texttt{Moses}                        \\ \hline
  Anstiege       & Anstiege           & \customcolorbox{Anstiege}{white} & \customcolorbox{Anstiege}{white}      \\
  um             & um                 & \customcolorbox{um}{white}       & \customcolorbox{um}{white}            \\
  {[}UNK{]}      & 4                  & \customcolorbox{[UNK]}{white}    & \customcolorbox{4}{white}             \\
  zwischen       & \textdegree{}      & \customcolorbox{[UNK]}{blue}     & \customcolorbox{\textdegree{}}{white} \\
  1990           & zwischen           & \customcolorbox{zwischen}{white} & \customcolorbox{zwischen}{white}      \\
  -              & 1990-2021          & \customcolorbox{1990}{white}     & \customcolorbox{1990-2021}{white}     \\
  2021           &                    & \customcolorbox{-}{white}        & \customcolorbox{1990-2021}{blue}      \\
                 &                    & \customcolorbox{2021}{white}     & \customcolorbox{1990-2021}{blue}      \\
\end{tabularx}
}{Tonekization Alignment}

In the beginning, I attempted to device an algorithm which would duplicate the respective
tokens in the Moses tokenization to lign up with the BERT (sub-)tokenization. However, it soon
became clear that this was an endeavor too error prone and meticulous: It is e.g. not a priori
clear which tokenization is the shorter one and therefore needs to be duplicated (sometimes, as
in table \ref{tab:tokenization}, both sequences need duplication); BERT tokenizes tokens which
are out-of-vocabulary (OOV) or which contain OOV subtokens as [UNK] further excessively comlicates the picture etc.
Therefore, I chose the following strategy which avoids all these problems:

(1) Take the ParZu-tokenized and parsed sentence, apply the predicate-finding algorithm to it.
Then, (2) tokenize the sentence using the BERT tokenizer, merge subtokenized tokens back to
regular tokens, identify the afore detected predicates\myfootnote{Since predicates are normally
verbal forms without any special characters in them, a string comparison search suffices to
achieve this.} and hand the BERT tokenized, predicate marked token sequence to DAMESRL.



% The first question that arises is: which tokenization should be mapped onto wich? In other
% words: should we try to align the BERT tokens with the corresponding NLTK tokens or vice
% versa? Let's assume we decide to align the tokenization $T$ with fewer items to the one
% with more items --- in this case this would mean aligning $T_{NLTK}$ with $T_{BERT}$. So,
% the first five tokens are no problem, we can align them by simply doing an exact match and
% confirm that the elements correspond.
% But when we reach the sixth token, the exact match fails. To decide whether the token
% $t_{T_{BERT}}$ or the token $t_{T_{NLTK}}$ was split up --- i.e. to determine which token
% must be copied to ensure tokenization equality ---, we need to do a mutual substring match.
% Doing this, we eoudl find out that ``-'' is a substring of ``-222''. In consequence,
% we align the two, duplicate ``-222'' and compare it with token number 7 in $T_{BERT}$. Since
% ``222'' is a substring of ``-222'', so we align the two of them.

% While it is theoretically possible to align tokens that were differently tokenized by the two
% algorithms, it is nevertheless quite cumbersome. The main problem, however, arises due to the
% [UNK] token BERT introduces for characters --- or character sequences --- which lie out of its
% vocabulary. Since there is obviously no more (sub-)string comparison possible, the process
% gets even more complicated: Suppose you have duplicted the ``-222'' in the NLTK column and are
% now on line 7. In the BERT tokenization you see the ``[UNK]'' token, while in the NLTK you see
% a ``\textdegree{}C''. To find out, what all is containt in the ``[UNK]'', you need to look at
% the token before and after it in the BERT tokenization and compare it with the respective NLTK
% tokens. since the the and so on....
% % \myfootnote{Of course, one could heuristically {\color{red}
% % go on until no [UNK] token is encountered align up in between.}}
% % Eventually, I decided to simply feed into DAMESRL
% the BERT-tokenized sequences, to get around this issue.


% \begin{tabularx}{\textwidth}{@{}l<{}@{\ }X@{}}
%   \textbf{BERT} (merged) & \textbf{NLTK} \\
%   Die                                        & Die \\
%   mittlere                                   & mittlere \\
%   Oberflächentemperatur                      & Oberflächentemperatur \\
%   wird                                       & wird \\
%   auf                                        & auf \\
%   -                                          & -222 \\
%   222                                        & \textcolor{blue}{-222}\\
%   {[}UNK{]}                                  & \textdegree{}C \\
%   (                                          & ( \\
%   {\fontfamily{ptm}\selectfont\texttildelow} & {\fontfamily{ptm}\selectfont\texttildelow}51 \\
%   51                                         & \textcolor{blue}{{\fontfamily{ptm}\selectfont\texttildelow}51} \\
%   K                                          & K \\
%   )                                          & ) \\
%   geschätzt                                  & geschätzt \\
%   .                                          & .
% \end{tabularx}


\subsection{DAMESRL}

There are not too many end-toend SOTA SRL frameworks available for German that come with a pre-trained model,
especially such ones that can be conveniently integrated in a pipeline of a bigger system.

\cite{do2018flexible} fill exactly this hole: They introduce DAMESRL, an SRL framework
that implements SOTA architecture, namely self-attention mechanisms, similar to BERT's.
They report an F1 score of 73.5 for their best model configuration on the German data set
of CoNLL '09. This best configuration is based on word embeddings as well as character
embeddings, self-attention and a softmax layer on top.

The DAMSRL predictor receives the BERT-tokenized sentence along with the information which tokens
in it are predicates (zero or more). For each token labelled as predicate in a sequence it predicts
for each other token in the sequence its SRL.

\fig{images/num_predarg_structs.pdf}{fig:num-predarg-structs}{Number of predicate-argument structures in all data sets.
                                                              Due to it's boilerplate template form, deISEAR shows a peculiar distribution:
                                                              Since it's examples always begin with ``Ich [\textsubscript{PRED} fühlte] $X$, als ....'',
                                                              it's guaranteed that at least one predicate is identifed, and it is very probable,
                                                              because of this sentence structure, that another will occur.
                                                              The other curious pattern exhibits SCARE: In no other
                                                              data set the amount of sentences where ParZu couldn't detect any predicates
                                                              is that high.

                                                              Besides the noted peculiarities, it is safe to say that setting the maximum number of
                                                              predicate-argument structures to three does probably not lead to much information loss;
                                                              on average over all datasets, 92.73\% of all sentences possess three or less such structures.}{14}{Predicate-Argument Structures}


In most cases, a sentences contains not exactly one predicate which distributes semantic roles,
but several, especially in longer sentences, --- or even none, especially in colloquial, short
sentences. Research by \citeauthor{zhang2019semantics} suggests that fixing the number of
predicate-argument structures to three yields the best results; so I adopt this number. In
other words, if a sentence has more than three argument-predicate structures, I only care about
the first three predicates identified (if proceeding from left to right through the sequence),
and disregard the others.

\begingroup
\begin{srl}[!h]
  \begin{minipage}{0.45\linewidth}
  \vspace{0pt}
    \begin{BVerbatim}[commandchars=\\\{\}, fontsize=\footnotesize]
            Slot 1   Slot 2   Slot 3

Wir         B-A0     0        0
wollten     O        0        0
eine        B-A1     0        0
Sache       I-A1     0        0
mehr        I-A1     0        0
retten      B-V      0        0
als         B-C-A1   0        0
die         I-C-A1   0        0
Restlichen  I-C-A1   0        0
.           O        0        0
      \end{BVerbatim}
  \end{minipage}
  % \hfill
  \begin{minipage}{0.45\linewidth}
  \vspace{0pt}
    \begin{BVerbatim}[commandchars=\\\{\}, fontsize=\footnotesize]
                    Slot 1   Slot 2   Slot 3

        Wir         B-A0     B-A0     B-A0
        wollten     O        O        O
        eine        B-A1     B-A1     B-A1
        Sache       I-A1     I-A1     I-A1
        mehr        I-A1     I-A1     I-A1
        retten      B-V      B-V      B-V
        als         B-C-A1   B-C-A1   B-C-A1
        die         I-C-A1   I-C-A1   I-C-A1
        Restlichen  I-C-A1   I-C-A1   I-C-A1
        .           O        O        O
    \end{BVerbatim}
  \end{minipage}
\end{srl}
\captionof{srl}{The two strategies for dealing with less than three predicates: \textbf{Left}:
                The open SRL slots get filled with the special SRL \emph{0}.
                \textbf{Right}: The first SRL structure gets duplicated until all slots
                are filled.}
\label{srl:zeros-duplicate}
\endgroup


However, if there are fewer than three predicate-argument structures
present, I test and report results for two strategies: The first solution (the left sentence
in SRL \ref{srl:zeros-duplicate}) lies in filling the ``unfilled'' predicate-slots with the
special ``0''-SRL. The second (the right sentence in SRL \ref{srl:zeros-duplicate}) simply
copies the first predicate-argument structure to the unfilled slots, thus amplifying the signal
from the first predicate-argument structure.



\subsection{GRU}

Finally, the predicted SRLs need to be encoded in a numeric way so they can be concatenated to
the vectors which are computed by BERT. The ``classic'', pre-transformer age, way of encoding
sequential data would be to employ a recurrent neural network architecture. Typically, one would
implement an architecture that counteracts the well-known vanishin / exploding gradients problem
of vanilla RNNs (cf. \citep{bengio1994learning}), such as LSTMs \citep{hochreiter1997long} or
GRUs \citep{cho2014learning}. I decided to use GRUs, since they are less computational intensive
and research has found both architectures for many tasks and datasets performing on par (cf.
\cite{chung2014empirical}).

For this, the three SRLs for each token get transformed into their numerical representation
via look-up in the SRL dictionary of the GRU model, such that euch SRL token is a vector $v \in
\mathbb{R}^{20}$. Then, the numerical representations for each token get concatenated and
those 60-dimensial vectors form then the input for the bi-directional two-layer GRU which
computes hidden states for all the inputs.

Because of the subsequent goal of combining BERT embeddings with SRL embeddings, the following
considerations need to be taken into account.


BERT adds several special meta tokens to sequences it embeds: (1) At the start of each
seqeuence, be it a single sentence or sentence pair one, it inserts a special [CLS] token; CLS
standing for classification. (2) At the the end of a sequence and between sentence pairs, BERT
inserts a separation token [SEP], which signals the end of a sequence. (3) If the sequence
is shorter than the defined maximum length, it gets filled up with [PAD] tokens. An XNLI sentence
pair with maximum length 25 woudl thus be represented as tokenizer BERT sequence the following
way (for better readability, the BERT subtokens are marked blue):

\begin{examples}
  \item Du musst dort nicht bleiben.\\
        An genau der Stelle musst du stehenbleiben!

        \customcolorbox{[CLS]}{blue} \customcolorbox{Du}{blue} \customcolorbox{muss}{blue} \customcolorbox{\#\#t}{blue} \customcolorbox{dort}{blue} \customcolorbox{nicht}{blue} \customcolorbox{bleiben}{blue} \customcolorbox{.}{blue} \customcolorbox{[SEP]}{blue} \customcolorbox{An}{blue} \customcolorbox{genau}{blue} \customcolorbox{der}{blue} \customcolorbox{Stelle}{blue} \customcolorbox{muss}{blue} \customcolorbox{\#\#t}{blue} \customcolorbox{du}{blue} \customcolorbox{stehen}{blue} \customcolorbox{\#\#bleiben}{blue} \customcolorbox{!}{blue} \customcolorbox{[SEP]}{blue} \customcolorbox{[PAD]}{blue}  \customcolorbox{[PAD]}{blue}  \customcolorbox{[PAD]}{blue}  \customcolorbox{[PAD]}{blue}  \customcolorbox{[PAD]}{blue}
\end{examples}

One of the goals of my experiments is to compare the standard BERT implementation with the SRL
enriched GLiBERT variant. Since the vanilla classification head predicts only using the last
hidden state of the [CLS] token, I need to represent SRL information also on this special
token. Therefore, I follow \cite{zhang2019semantics} and add a meta-SRL [CLS], which represents
the meta semantic role of the [CLS] special token.

Another question that arises concerncs the SRL encoding of multiple sentences:
Suppose we have a sequence consisting of several sentences (separated by brackets and indexed by $A$, $B$, and $C$):

\begin{examples}
  \item {[}\textsubscript{A} Die Ereignisse von Oni finden im oder nach dem Jahr 2032 statt und beschreiben ein dystopisches Zukunftsbild der Erde.] [\textsubscript{B} Die Welt ist so verschmutzt, dass nur noch kleine Teile bewohnbar sind.] [\textsubscript{C} Um die internationale Wirtschaftskrise zu lösen, haben sich alle Völker unter einer Weltregierung vereinigt.]
\end{examples}

Should each sentence $A$, $B$, and $C$ be encoded separately and these represenatations than
be combined with the aligned BERT subtokens? Or is it more effective to ``glue'' all sentences
together (or rather, the concatenated SRL tokens in those sentences) and embedd this sequence?
Further, also meta SRLs for [SEP] and [PAD] tokens could be added --- so that also sentence
pair tasks are embedded as one long sequence of SRL tokens. All experiments except for XQuAD
$\alpha$ were conducted implementing the latter approach, proving that the model profits more
from the second architecture.



\section{Combination}

The combing of a token's BERT embedding with its correpsonding SRL embedding is done
straightforwardly; both numerical vectors are concatenated:

Take, the first token $t_1$ of the sentence in \ref{srl:zeros-duplicate},
``Wir'': After being consumed by the BERT module, it has a BERT embedding
vector representation $b \in \mathbb{R}^{768}$. Its SRL representation,
let's say the zero-filled variant \texttt{B-A0+0+0}, has a GRU-embedded
representation $s \in \mathbb{R}^{60}$. The final numerical representation
$w_1$ of the token $t_1$ ``Wir'' plus its SRL \texttt{B-A0+0+0} is computed
as:

$w \in \mathbb{R}^{788} = b \cdot  s$

where $\cdot$ denotes the concatenation operation.

Because the tokenization of the SRL module is based on the merged BERT tokeniazation,
there remain two ways of combining BERT embeddings and SRL embeddings, however: (1) The
embeddings of subtokenized tokens in BERT are merged back to token level, or (2) SRL
embeddings of tokens which were subtokenized in the BERT module get duplicated to align
with the BERT embeddings. Consider the following word as example: ``Restlichen'' has one
SRL in \ref{srl:zeros-duplicate}, \texttt{I-C-A1+0+0} (in the duplicated variant), which
would lead to one vector $s$ as numerical representation, while it would be tokenized as
``Rest'', ``\#\#lichen'' inside BERT, leading to two vectors $b_1$, $b_2$ as numerical
representation. In the first scenario, $b_1$ and $b_2$ would be merged into one vector
$b$, by averaging all dimensions; this vector would then be concatenated with $s$ to
form the final representation $w$ of ``Restlichen''. In the second variant, $s$ would be
copied once to align with the BERT vectors, which would lead to the final representation
of ``Restlichen'' as two vectors: $w_1$ = $b_1 \cdot s$ and $w_2 = b_2 \cdot s'$.

For all experiments, I run both variants, reporting in chapter \ref{chap:5_results} the results,
which indicate that variant 1, i.e. merging BERT subtokens back to tokens to align with the SRL,
leads to information loss, and therefore weakens the general model performance.


\section{Head Module}

Since the vanilla classification and question answering heads differ quite
starkly --- classification only takes into account the [CLS] token while
the start end end index probabilities need to be computed on every token
in the sequence --- I cover them in separate subsections.

Additionally, for classifiaction tasks, I found more ways in tackling these
with different heads, while for the question answering task I only saw one
possibility for a GliBERT head which is virtually the vanilly head, except
that SRL information may be added to the BERT subtokens.


\subsection{Classifiction}

% While for question answering there was little tweeking needed to adapt to the extended BERT
% embeddings, for classification the situation looks a bit more complex. The standard BERT way
% of doing classification tasks runs as follows:

% \begin{itemize}
%   \item Prepare the data: add a [CLS] token at the beginning, a [SEP] token between the two sentences (if there are), and pad with the [PAD] token
%   \item Send the prepared examples through the BERT network
%   \item Select only the embedding for the first token  --- i.e. the [CLS] ---, send it through a dense layer with a softmax and predict the class for this example
% \end{itemize}

% \cite{devlin2018bert} visualize this as can be seen in figure \ref{fig:BERT-classification}.


% The problem now is that in the above described standard implementation, there is no straigthforward
% way to enrich the BERT embeddings with SRLs, since the only embedding that is used for prediction
% is the [CLS] token; since this is a special BERT token it is not present in the original sentence
% and, therefore, it does not have a corresponding SRL. However, as I lais out in the section before,
% I add several meta SRLs which are added to the regular SRLs, to have numerical SRL representations
% also for these special tokens.

For both, single sentence and sentenc pair tasks, the vanilla BERT head considers
only the last hidden state of the [CLS] token, after the sequence was sent through
the transformer blocks and predicts class probabilities only relying on the
information present in this vector $\in \mathbb{R}^{786}$:

\fig{images/bert_classification.png}{fig:BERT-classification}{Schema for sentence pair (left) and single sentence (right) classification. Figure taken from \citep{devlin2018bert}.}{13}{BERT-Classification}

Follwing, I present the three classification heads I used for my experiments: The [CLS]
head, similar to the vanilla classifiaction head, the FFNN head, which consideres the
numeric representation of all tokens in the sequence, and the GRU head, which implements
a biderectional recurrent neural network which computes a condensed representation of
the whole sequence before predicting the actual class porbabilities.\myfootnote{Early in
the experimental phase, I also devised a CNN head, however, the preliminary results were
not promising, so i concentrated on the other three heads.}



\subsubsection{[CLS] Head}

\cite{zhang2019semantics} follow in their implementation the vanilla BERT classification
head, as proposed by \cite{devlin2018bert}:

After adding corresponding SRLs for the special BERT tokens and embedding the SRL sequence,
the last hidden state representation for the BERT+SRL combind [CLS] token is selected
and softmax-scaled probabilities for all classes are computed.

% In their paper for SemBERT, \cite{zhang2019semantics} do not really address the
% issue laid out above. To the contrary, the differnt pieces of information they
% provide are rather conflicting, only after inspecting the code they released on
% \href{https://github.com/cooelf/SemBERT/}{GitHub}, the picture somewhat cleared:

% After predicitng the SRLs for a given input, they add pseudo-SRLs for the [CLS] and [SEP] tokens.
% In the look-up table of the BiGRU that consumes the SRLs, they then simply add the corresponding
% keys --- so that besides regular SRLs as ``B-V'' (beginning of predicate) or ``I-A0'' (inside
% or end of argument zero), there are also the labels ``[CLS]'' and ``[PRED]'' After sending this
% sequence through the BiGRU, they concatenate the two hidden states of the [CLS] SRL with the
% [CLS] BERT embedding and predict on that vector % Applying this strategy, now there \emph{is} a
% SRL for the [CLS] token after the sequence was sent through the BiGRU which then can be appended
% to its BERT embedding vector

Formally, the [CLS] Head is a one layer Fullyconnected Feedforwrd Neural Network. The
prediction is the results of sending the embedding of the [CLS] token through the network:
$\hat{y} = softmax(L_1)$. The input dimensions of the layer vary according to the setting,
i. e. if the SRLs are included, or not. if not, then the demsion is simply the standard
BERT hidden size $h_i \in \mathbb{R}^{768}$, if the SRL embedding is appended, then $h_i \in
\mathbb{R}^{788}$. the output dimensionality of the network depends on the number of classes
for the task: This number ranges from 2 (PAWS-X, \emph{true}, \emph{false}) to 7 (deISEAR,
\emph{Traurigkeit, Scham, Freude, Angst, Wut, Ekel, Schuld}).


\fig{images/head-cls.png}{fig:cls-head}{Head with a one-layer \textbf{F}eed \textbf{F}orward \textbf{N}eural
    \textbf{N}etwork on the [CLS]-token}{10}{{[}CLS{]} Head}

Intuitively, I expected this head to not show too big differences between the +SRL $-$SRL settings,
since the SRL information, which is essentially a relational, sequential mark-up of the sequence is
boied down into one token. However, as is also confirmed by the results of \cite{zhang2019semantics},


\subsubsection{FFNN Head}

FFNN stands for Feed Forward Neural Network. While th [CLS] Head procudes predictions based on the
weights of the last layer output of the [CLS]-token, this head takes the last layer output of all
tokens and concatenates them.
While the approach implemented by \cite{zhang2019semantics} is able to improve the vanilla
BERT approach, it does not lead to an improvement on others, or worse, brings the perormance
down. I suspect a reason for this may lie in the manner of how the SRLs are processed in this
approach. The information SRLs provide is, what may be called, sub-sentence specific and
cannot be adequately represented as a single information piece. By this I mean that it does
not suffice to know that given an utterance $x$ that there is a specific SR in it; rather the
information \emph{where} is crucial. Consider the following example (the pseudo SRL [CLS] is
added):


[\textsubscript{[CLS]} ] [\textsubscript{A-0} The man] [\textsubscript{predicate} asked] [\textsubscript{A-1} his friend] .

After the subscripted SRLs were consumed by the BiGRU, there is some information about all
SLRs in the hidden states of the [CLS] token. While there may be some information about there
being a predicate, an argument zero, and an argument 1 present, it is completely impossibly
to determine from which tokens these signals came. Especially in sentence pair tasks, such
as paraphrase identification, this information is however absolutely crucial. As can be seen
from results \ref{tab:results}, this hypothesis is also supported by the results:

\fig{images/head-lhsa.png}{fig:lloa-head}{Head with a one-layer \textbf{F}eed \textbf{F}orward \textbf{N}eural \textbf{N}etwork
    on the concatenated token sequence.}{10}{FFNN Head}

As has been shown by e.g. \cite{myagmar2019transferable} for sentiment analysis, a simply final
fully-connected feed forward layer produces fairly good results (in fact, it performs the best
in the different architectures they tested for their task).

Implementing a fully-connected feed forward network as final layer counters the problem of the
information deprecation that is present in the [CLS] approach: Every token's BERT embedding gets
concatenated with the token's SRL embeddding. The whole sequence is then flattened, i.e. all
BERT+SRL vectors get concatenated into one large vector of size $\mathbbm{R}^{n\times768+20}$.


\subsubsection{GRU Head}

Since the SRLs are essentially a sequential ``mark-up'' of the sentences, the thought of
encoding them with an architecture designed for sequential data is not too far. Inspired by
the biological properties of the brain, the concept of recurrent neural networks has been
around since the late 80ies, with \citep{hopfield1982neural} often being credited as having
implemented the first recurrent neural network. To overcome the problems of the vanishing
and explofing gradient problems, \citep{hochreiter1997long} proposed the LSTM architecture.
\citep{cho2014learning} GRU

\fig{images/head-gru.png}{fig:gru-head}{Head with a one-layer \textbf{F}eed \textbf{F}orward \textbf{N}eural
    \textbf{N}etwork on the concatenated last hidden states of the bi-directional GRU.}{10}{GRU Head}





\subsection{Question Answering}
\label{sec:question-answering}



``\textelp{} in the question answering task, we represent the input question and passage as
a single packed sequence, with the question using the A embedding and the passage using the
B embedding. We only introduce a start vector $S \in \mathbb{R}^H$ and an end vector $E \in
\mathbb{R}^H$ during fine-tuning. The probability of word $i$ being the start of the answer
span is computed as a dot product between $T_i$ and $S$ followed by a softmax over all of the
words in the paragraph: $P_i = \frac{e^{S\cdot T_i}}{\sum_{j}^{} e^{S\cdot T_j}}$.'' \citep{devlin2018bert}

% \fig{images/bert_qa.png}{fig:BERT-QA}{Vanilla BERT question answering head. Figure taken from \citep{devlin2018bert}.}{13}{BERT-QA}

\subsubsection{Span Prediction Head}

The ``standard'' implementation of the Q\&A BERT head consists of one one-layer FFNN which predicts
on each input token the probability of it being the start of the answer span and the end of the
answer span, respectively. After both probabilities are computed for all tokens, the ones with
the highest probability get selected; no further logic is enforced, such as that the index of
the start token should be smaller than the one of the end token, or that the answer span may
not lie inside the question (i. e. before the first [SEP] token), etc.

\begin{wrapfigure}[21]{r}{0.45\linewidth}
  \begin{center}
    \includegraphics[width=1.0\linewidth]{images/bert_qa.png}
  \end{center}
  \stepcounter{myfigure}
  \caption[BERT Q\&A]{Vanilla BERT question answering head. Figure taken from \citep{devlin2018bert}.}
\end{wrapfigure}

Since the standard implementation already implements a model that predicts on each token,
the SRL-emgedding enriching is relatively straight forward:
First, the input layer of the small head FFNN needs to be adapted to the BERT-token + SRL
dimensionality, which results in the vector representation $\mathbb{R}^{768+20}$ for each
SRL-enriched token. Secondly, when the setting of merging the BERT subtokens before adding
the SRL embeddings is used, the start and end span indexes have to be recomputed.


\fig{images/head-span_pred.png}{fig:Span-Prediction-Head}{The GliBERT head for span prediction
    in Question Answering Tasks. After the Tokens and SRLs were consumed by BERT and the SRL
    embedding module, one FFNN predict on each token in the sequence, how likely it is first
    and the last token of the answer span. After these predictions have been made for all tokens, the token
    with the highest value for each position gets selected.}{10}{Span Prediction Head}



