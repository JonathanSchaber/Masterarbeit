\newchap{Approach}
\label{chap:2_approach}

% \epigraph{\textgreek{πάντες ἄνθρωποι τοῦ εἰδέναι}}{\textit{Heraclitus of Ephesus}}


In this chapter, I will give a brief overview of several things: In a first step, I present
the BERT architecture and its impact on NLP in recent years. Secondly, I will shortly demonstrate
Problems that have been identified relating to the performance of BERT. Then, I will point out
some submitted solutions countering those problems. And lastly, I will describe my approach and
elucidate the topic of semantic roles.

\section{BERT}

Since the publication of the seminal paper ``BERT: Pre-training of deep bidirectional transformers
for language understanding'' \cite{devlin2018bert} and the accompanying open-sourcing of its
architecture\myfootnote{\url{https://github.com/google-research/bert}}, BERT has probably been the
most studied and cited NLP model since word2vec \cite{mikolov2013distributed} --- amassing over
17,000 citations on Google Scholar as of April 2021. This massive interest from the NLP community
in BERT suggests that it somehow must be accomplishing something which is of greater significance
to the field than regular benchmark SOTA cracking by ``normal'' new or improved architectures.


The basic concept of BERT is straight forward: (1) Let a big, sophisticated neural network learn
contextualized embeddings for words by training it unsupervised on huge amounts of data. (2)
Use these embeddings as representations for words in downstream tasks, put a very simple neural
network on top (mostly a simple FFNN) and fine-tune them during training on the downstream task.

One of the surprising findings of \citeauthor{devlin2018bert} was the transferability of
these word embeddings: Although the task of the pretraining learning has nothing to do
with the downstream task, where the embeddings are used in, the BERT embeddings can be
fine-tuned in a lean manner to achieve SOTA results on established NLU datasets.
Another advantage of this approach is that the cost, hardware, and data intensive pre-training of the
embeddings (step one) must only be computed once; the downstream task dependent fine-tunig can then
be carried out in a lean set up.\myfootnote{To give an impression on the expenses of pre-training
the BERT architecture: \cite{schwartz2019green} estimate the pre-training for BERT-large to have lasted four days
on 64 TPU chips, resulting in power expenditures of about \$7,000. However, this has to be considered
rather cheap compared to recent architectures' sizes: The largest architecture to this date is the
T-NLG (Turing Natural Language Generation) built by Microsoft, possessing a staggering 17 billion
parameters --- that is approximately 48 times the size of BERT-large (350 million parameters), cf.
\cite{sharir2020cost}. Open AI's GPT-3's \citep{brown2020language} pretraining is estimated to have costed
\$12 million \citep{floridi2020gpt}. This trend of increasingly bigger language models has earned severe critique from
several sides, ranging from ecological and social to linguistic concerns over such models; for
a good overview of these points see \cite{bender2021dangers}.}

From a more technical point of view is BERT first and foremost a neural
network architecture. More precise, it is an implementation of the
self-attention mechanism introduced by \cite{vaswani2017attention}; the
main difference, and apparently advantage, to other architectures
that implement the transformer architecture is that  BERT is a bi-directional
language model.I will not go into too much details here since BERT is
now a very well-known structure and has been described in a
plethora of papers, blogs, and videos.%\myfootnote{For a really well made visualization of self-attention
% in BERT, I can recommend \url{https://www.youtube.com/watch?v=-9vVhYEXeyQ}}.
In simple terms, BERT takes an input sequence, tokenizes it and computes contextualized vector
representations for each token via stacked blocks employing self-attention and linear combination.
Figure \ref{fig:bert-architecture} shows one of these blocks.

\fig{images/BERT_clipped.png}{fig:bert-architecture}{3D-visualization of the BERT architecture. Nicely illustrated are the 12 attention heads and following linear projections in one block (here called ``Layer''). Credit for the image goes to \href{https://peltarion.com/blog/data-science/illustration-3d-bert}{Peltarion}}{11}{BERT Architecture}

The computation of the weight matrices and initial vector representations for the tokens is done
via an unsupervised learning phase, often referred to as pre-training. The basic concept is
that BERT is given sentence-chunks of large amounts of text (\citeauthor{devlin2018bert} use the BooksCorpus, consisting
of 800 million words, plus the English Wikipedia, consisting of 2.5 billion words) and BERT
needs to optimize on two training objectives: (1) One word is randomly masked and BERT has
to predict it, and (2) BERT has to decide if, given two randomly sampled sentences, the second
is a valid continuation of the first. Crucially, both tasks can be generated automatically, no
tedious human annotation of data is needed.

With this ``simple'' approach --- i.e. unsupervised pretraining of contextualized embeddings and
fin-tuning on target tasks with very simple head on top --- \citeauthor{devlin2018bert} beat the
hitherto leading architecture on the GLUE benchmark by an outstanding average of 7,0\%. This is
especially remaking, since BERT is not a highly specialized model\myfootnote{Until then, SOTAs
were achieved by complex interwiring of some embeddings with a specialized architecture. }, but
apparently still more effective on most tasks than highly task-specific optimized models.
Thus, the NLP community was awestruck.




\subsection{Problems}
\label{sec:problems}

In recent years, a lot of research went into analyzing, improving, and deluding the BERT architecture;
in the NLP field, those efforts are often referred to by the notion of ``BERTology'' (cf. \cite{rogers2020primer}).
While BERT showed to be astonishingly effective on several established data sets and benchmarks such as GLUE \citep{wang2018glue},
it soon became obvious that it also {\color{red} had} its weak-spots: \citeauthor{ettinger2020bert} confronted
BERT with three language tasks originally coming from psycholinguistics which are well-known to be difficult
to tackle, even for humans. They find

\begin{quote}
that [BERT] shows sensitivity to role reversal and
same-category distinctions, albeit less than humans, and it succeeds with noun hypernyms,
but it struggles with challenging inferences and role-based event prediction—and it shows
clear failures with the meaning of negation.'' \citep[p.~46]{ettinger2020bert}
\end{quote}

Apparently, while being able to solve ``regular'' tasks, BERT seems to be prone to fail
in situations where proper semantic understanding of text sequences is crucial, such as
detecting role reversal or sentence completion tasks.

\cite{jiang2019evaluating} also state ``that despite the high F1 scores, BERT models have
systematic error patterns'', which for them suggests ``that they still do not capture the
full complexity of human pragmatic reasoning.''

\cite{jin2020bert}even go further and created so-called adversarial attacks on BERT:
After observing that BERT seems to rely only on the statistical cues of a small number of the input tokens to
form its predictions, \citeauthor{jin2020bert} create a sophisticated algorithm to permute
the input sentences wihtout actually changing its meaning. Following is an example from the
SNLI \citep{bowman2015snli} dataset, exemplifying one such attack:

\begin{examples}
  \item \textbf{Premise}: A child with wet hair is holding a butterfly decorated beach ball.

        \textbf{Original Hypothesis}: The \emph{child} is at the \emph{beach}.\\
        \textbf{Adversarial Hypothesis}: The \emph{youngster} is at the \emph{shore}.
\end{examples}

The italicized words are the ones that were affected by the adversarial algorithm.
Obviously --- for a human --- the meaning of the adversarial hypothesis has not
changed from the original one. One could argue that there is a slight difference
in style (the adversarial sounds somewhat overblown to me) but it would still count
as an entailment of the premise. However, as the authors report, BERT is affected
by such attacks and changes its predictions.

For SNLI, \citeauthor{jin2020bert} report to bring down BERT's original accuracy of
89,4\% to an astonishing 4,0\% by permuting 18,5\% of the input tokens. Recall that
the permutations are, simply put, nothing else than exchanging words with identical meanings.



% ``BERT shows a complete inability to prefer true over false completions for negative sentences.''
% \citep[p.~45]{ettinger2020bert}

\subsection{Solutions / Related Work}

But the NLP community was not only passive and/or destructive concerning BERT, it also
produced a vast number of adaptions, variations, and improvements to the ``vanilla''-BERT.
The motivations behind all those BERTlings are as manifold as one can think: some are simply
adaptions to other languages than English, some are general variations (in hope of improvement)
of the BERT architecture, other are explicitly addressing above outlined problems.

The following overview sheds some light on the iridescent potpourri of the BERT
family.\myfootnote{This compilation is partly drawn from the towards-data-science article
\href{https://towardsdatascience.com/a-review-of-bert-based-models-4ffdc0f15d58}{``A
review of BERT based models''} by Ajit Rajasekharan.}
Note however that this is by no means an exhaustive presentation of all BERT-variants produced so far.


\begin{description}
  \item[\textbf{Adapting to other languages}] One of the most straightforward modifications to the BERT model is to pre-train it on different langauges. Examples for this are the French CamemBERT \cite{martin2019camembert}, the Italian AlBERTo \cite{polignano2019alberto}, or the Dutch BERTje \cite{de2019bertje}.\myfootnote{\cite{devlin2018bert} also trained a multi-lingual BERT (mBERT), which was trained on 104 languages. However, language-specific BERTs have been shown to be more performant than employing the mBERT.}
  \item[\textbf{Adapting to specialized domains}] BERT was pre-trained on two corpora: (1) The BooksCorpus \citep{zhu2015aligning}, consisting of 800 million words and comprising 16 different genres. (2) The English Wikipedia, lists and tables excluded (2.5 billion words).
  Examples of BERTs pre-trained on specialized domains are for example BioBERT \cite{lee2020biobert}, which  is an adaption to biomedical language, and LEGAL-BERT \cite{chalkidis2020legalbert} which is a whole family of BERT models pre-trained on legal texts.
  \item[\textbf{Including different modalities}] Another, highly interesting amplification of the BERT architecture is the inclusion of additional modalities, for exmple (moving) images: VideoBERT \cite{sun2019videobert} learns embeddings for image-enriched texts and can be used for example for image captioning or image classification tasks.
  Several researchers claim that the future of NLP relies on combining text with sensory, e.g. visual, data for creating more stable and reliable models; cf. \citep{bisk2020experience, bender2021dangers}.
  \item[\textbf{Optimizing architecture/training objective(s)}] Several BERT-variations modify the actual architecture of BERT:  DistilBERT \cite{sanh2019distilbert} is a variant 60\% of the size of the original BERT while retaining 97\% of its original performance.
  RoBERTa \cite{liu2019roberta} essentially modifies core hyper-parameteres such as batch-size, byte-level BPE, and the like, creating a more stable BERT.
  DeBERTa \cite{he2020deberta} modifies the attention mechanism and the position encoding, while TransBERT \cite{li2021transbert} introduces a new pre-training framework.
  \item[\textbf{Incorporating structured information}] One of the strengths of BERT is the unsupervised pre-training on unstructered, raw text. However, research has shown that including structured linguistic information can stabilize BERT and even counterbalance some of the known weaknesses (see above) to some extent:
  ERNIE \cite{sun2019ernie} includes a knowledge graph into BERT, making structural fact representations available to BERT.
  %StructBERT \cite{wang2019structbert},
  VGCN-BERT \cite{lu2020vgcn} combines a Vocabulary Graph Convolutional Network with the standard BERT and \cite{zhang2019semantics} include semantic role labels in their SemBERT during fine-tuning.
\end{description}


\section{GliBERT}

Infected by the pandemic BERT-fever and slightly annoyed by the hegemony of English, I decided to
enqueue in this illustrious list by combining two of the {\color{red} advancement startegies}: I try to improve
the German BERT by enhancing it with structured, linguistic information during fine-tuning.

Since it is common practice to give your enhanced/variegated BERT architecture an appropriate name --- you may have spotted
a pattern in the sections before --- I decided to not deviate from this tradition, and decided to call my breed
\textbf{G}erman \textbf{l}inguistic \textbf{i}nformed BERT, or short: \textbf{GliBERT}.
And thus a new

There exist several linguistic structures which could be hypothetically included into BERT:
\href{https://uni-tuebingen.de/en/faculties/faculty-of-humanities/departments/modern-languages/department-of-linguistics/chairs/general-and-computational-linguistics/ressources/lexica/germanet/}{GermaNet} \citep{hamp1997germanet}
is a large lexical-semantic net that relates noun, verbs, and adjectives semantically by
grouping lexical units that express the same concept into synsets and by defining semantic
relations between these synsets. It can also be characterized as a thesaurus or a light-weight
ontology.

Another, much simpler, possibility would be to identify named entities
and include the encoded structured
information related to them, e.g. using the DBpedia \citep{auer2007dbpedia}, and enrich
the BERT embeddings with them.

However, I decided to concentrate on semantic roles for several reasons: \citeauthor{zhang2019semantics}
demonstrated the feasibility of this undertaking for English. Further, it occurred to me to be a good
balance between two extremes: (1) Including sophisticated knowledge structures, which would
require extensive preprocessing (stemming, identifying content words, potential word sense
disambiguation, look-up in the knowledge base) and rather cumbersome encoding; and (2)
stright-forward on the fly ``mark-up'' of input text, with low information substance in the
case of named entities. with semantic roles, I get the best from both worlds: Easy to implement
structured information, while --- hopefully --- truly adding semantic substance to the vanilla
BERT embeddings.



All code relating to the following dataset set ups, GliBERT architecture, and training can be found in
my \href{https://github.com/JonathanSchaber/Masterarbeit}{GitHub repository}.


\section{Semantic Roles}

% Traditionally in linguistics, language is analyzed into different structural levels, where
% different tools for describing these levels, or strata, are used.
% In most theories, there are are four of these structural levels proposed:
% Beginning from the Bottom, there is the level of Phonetics and Phonology, followed by Morphology,
% then there is the level of Syntax, and the last one is Semantics.\myfootnote{Sometimes Pragmatics
% is conceptualized as an additional fifth layer on top, sometimes it is considered to form a field
% of its own; I follow the latter.}
% While the first three levels deal with the form of utterances of human language, semantics is
% concerned with the meaning of such utterances \citep[p.~4ff.]{kracht2007introduction}.

% \tab{tab:levels-of-lang}{Levels of language analysis and description.}
% {\begin{tabular}{|l|l|l|l|}
% \hline
% \multicolumn{2}{|c|}{Form}     & \multicolumn{2}{c|}{Meaning}     \\ \hline
%   % a & b & c & d \\ \hline
%   & \multicolumn{2}{c|}{Level} &                   \\ \hhline{====}
%         Syntax  & \multicolumn{2}{l|}{Sentence} & \multirow{3}{*}{Semantics} \\ \cline{1-3}
%         Morphology & \multicolumn{2}{l|}{Word} &                   \\ \cline{1-3}
%         Phonology & \multicolumn{2}{l|}{Sound} &                   \\ \hline
% \end{tabular}
% }{Levels of language}

One difficulty a system targeted at NLU must tackle is the ability to cope with the vast amount of
flexibility and freedom in natural language to express things. In NLU, often one would probably
talk about propositions: Facts about the world are being stated and properties of this propositions
need to be understood or processed in some way or the other.
{\color{red} Problems now arise due to t}

\begin{examples}
  \label{ex:semantics}
  \item Due to the big waves, the ship was severely damaged and went down.
  \item The stormy ocean caused the vessel to sink.
  \item Unable to save the stricken freighter, the crew had to be evacuated.
\end{examples}

Although these three sentences make use of very different vocabulary --- an unweighted BLEU score
is virtually zero between them --- it is obvious to a speaker of English that they all convey more
or less the same meaning, that all of them tell the same state of affairs: A ship sank because of the
forces of nature.

Maybe the most obvious way of saying the same thing with different words is synonymy: ``Ship'',
``vessel'', and ``freighter'' all refer to the same object in the examples above; having several
options when choosing a word to denote something is a paramount feature of human language.

Further, the communication of one and the same event, e.g. the sinking
of a ship, can be transmitted in various ways: In the second sentence,
the process is denominated explicitly using the verb ``to sink''; in
the first, the semantically more obscure semi-fixed expression ``to go
down'' is used to inform about that very situation; while in the third
the sinking of the ship is not mentioned explicitly but inferable from
the circumstance of ``not being able to save'' it.

For a human speaker, all this disentangling, recognizing coreference,
reconstructing not explicitly mentioned context, etc. happens effortless
and automatic --- for an algorithm, however, phenomena like the ones
mentioned before pose serious challenges.

``For computers to make effective use of information encoded in text, it is essential that
they be able to detect the events that are being described and the event participants.''
\citep{palmer2010semantic}

As I laid out in section \ref{sec:problems}, a modern, unsupervised model
like BERT seems to perform surprisingly good in tasks where such ``understanding'' of
events are being tested.\myfootnote{Of course, one does not really measure epistemical
understanding in such tests, {\color{red} but this is maybe the closest we get} (cf. \cite{sahlgren2021singleton})}
Simultaneously, some investigated failures of BERT seem to indicate that this ``understanding''
goes not too deep.

Semantic Roles are an attempt at creating an instrument with which it is possible to analyze
sentences as \ref{ex:semantics} and capture the semantic similarity between them. The central
idea hereby is that every utterance has an underlying semantic structure\myfootnote{Often,
especially in Generative Grammar traditions, this level is also known as deep structure, or
D-structure.} (sloppily phrased: ``Who did What to Whom, and How, When and Where?'') which
can be realized in different surface structures. There have been various undertakings in
creating a vocabulary for describing such structures, putting the focus on different aspects and showing varying degrees of {\color{red} analytic detail}.

The paper ``The Case for Case'' \citep{fillmore1967case} is often seen as the starting point for
the theory of semantic roles in modern linguistics. \citeauthor{fillmore1967case} argued in it
that what he called ``Deep-Cases'' play a crucial role in the Deep-Structure of sentences ---
the hitherto prevalent view in Generative Grammar was that case was a purely Surface-Structure
related phenomenon and only one of several possibilities to realize syntactic relationships.
Interestingly, these ``Deep-Cases'' were semantically-motivated; he proposed
seven Deep-Cases, e.g. the ``Agentive'': `` [T]he case of the typically animate perceived instigator of the
action identified by the verb'', or the so-called ``Factitive'': `` [T]he case of the object or being resulting from the action or
state identified by the verb, or understood as a part of the meaning of the verb'' \citep[p.~46]{fillmore1967case}.
The observable Surface-From cases could than through elaborate tests give insights as to what the
underlying Deep-Cases were.

Building upon those core concepts introduced by Fillmore, other linguists added features
to the project of formalizing the core semantic structures found in language, as summarized by
\citeauthor{palmer2010semantic}:
In the beginnings of the 70ies, \cite{jackendoff1972semantic} expanded and refined Fillmore's
model by introducing the concept of primitive conceptual predicates and their property of governing
arguments, which were conceptualized as bearing some proto-semantics, similar to the Fillmorian
Deep-Cases. This approach, known as ``Lexical Conceptual Structure'' (LCS), proved to be
an elegant theory and capable of generalizing well between different verbs; in the 90ies LCS
was implemented as system for representing semantics in early NLU and translation models \citep{palmer2010semantic}.
But, due to its detailed analysis of verbs into (several) primitive predicates and the highly verb specific
conceptualized semantic roles of them, LCS turned out to be cumbersome to extend to the whole range
of a vocabulary of a language.

\cite{dowty1991thematic} in contrast, approached the problem of constructing
a framework for analyzing core conceptual semantic structures from a different
angle: Instead of providing a detailed description of the primitive predicate and
idiomatic argument structure for each individual verb, he attempted to identify
general functions of noun phrases, what he called ``thematic proto-roles'',
present in sentences. To accomplish this, \citeauthor{dowty1991thematic} drew
from the theory of ``family resemblance'' and defined a set of attributes which
would indicate such a thematic role.
``The hypothesis put forth here about thematic roles is suggested by the
reflection that we may have had a hard time pinning down the traditional role
types because role types are simply not discrete categories at all, but rather
are cluster concepts \textelp{}'' \citep[p.~571]{dowty1991thematic}

Proto-Agent properties (after \citep[p.~572]{dowty1991thematic}):

\begin{enumerate}[label=\alph*]
  \item volitional involvement in the event or state
  \item sentence (and/or perception)
  \item causing an event or change of state in another participant
  \item movement (relative to the position of another participant)
  \item (exists independently of the event named by the verb)
\end{enumerate}

{\color{red} I will not elaborate this further, but other theories have been put forward.}

Like many theories in linguistics, Semantic Roles remain a disputed topic in the field until today:
``There may be general agreement on the cases (or Thematic Roles or Semantic Roles) \textelp{},
but there is substantial disagreement on exactly when and where they can be assigned and which
additional cases should be added, if any.'' \citep{palmer2010semantic}

% \begin{itemize}
%   \item ``conceptual relations that the referents of the noun phrases play with respect to the verb'' \citep{palmer2010semantic}
%   \item different syntactic constituents can bear same SR
%   \item Linking theory: Map from syntactic surface structure to underlying predicate argument semantics
% \end{itemize}

However, different resources have been created, implementing some variety of the various Semantic Roles frameworks.
One of these is the PropBank \citep{kingsbury2002treebank}.


Semantic Roles are systematic abstractions of semantic functions that are attributed to the participants
in a factual situation: The volitional acting entity in a situation is abstracted as ``(Proto-)Agent''; regardless
of the actual, concrete act. Similarly, noun phrases which denote participants that

``Because verbs generally provide the bulk of the event semantics of any given sentence, verbs have
been the target of most of the existing two million words of PropBank annotation. Nonetheless,
to fully capture event relations, annotations must recognize the potential for their expression
in the form of nouns, adjectives and multi-word expressions, such as Light Verb Constructions
(LVCs).'' \citep[p.~3014]{bonial2014propbank}

\begin{examples}
  \item He fears bears.
  \item His fear of bears.
  \item He is afraid of bears.
\end{examples}

\cite{bonial2012english} define the following proto-roles for the numbered arguments, modifiers,
and relations:

\begin{description}
  \item[\customcolorbox{\textbf{Arg0}}{blue}] agent
  \item[\customcolorbox{\textbf{Arg1}}{green}] patient
  \item[\customcolorbox{\textbf{Arg2}}{yellow}] instrument, benefactive, attribute
  \item[\customcolorbox{\textbf{Arg3}}{red}] starting point, benefactive, attribute
  \item[\customcolorbox{\textbf{Arg4}}{manatee}] ending point
  \item[\customcolorbox{\textbf{ArgM}}{sage}] modifier
  \item[\customcolorbox{\textbf{Rel}}{llight-blue}] Relation (can be a verb, noun, or adjective)
\end{description}

Following are some exmample sentences from the PropBank frames\myfootnote{accessible through this
\href{https://github.com/propbank/propbank-frames.git}{GitHub repository}}. Semantic roles are
highlighted using the colors from the previous list. Note that only one relation is marked in the
sentences, even if there are multiple. Since DAMESRL only treats verbs as semantic roles distributing
relations, I include only verbal ``Rel''s in the examples:

\begin{examples}
  \item \customcolorbox{[\textsubscript{Arg0} Yasser Arafat]}{blue} has \customcolorbox{[\textsubscript{Rel} written]}{llight-blue} \customcolorbox{[\textsubscript{Arg2} to the chairman of the International Olympic Committee]}{yellow}, asking him to back a Palestinian bid to join the committee.
  \item Once \customcolorbox{[\textsubscript{Arg0} he]}{blue} \customcolorbox{[\textsubscript{Rel} realized]}{llight-blue} \customcolorbox{[\textsubscript{Arg1} that Paribas's intentions weren't friendly]}{green}, he said, but before the bid was launched, he sought approval to boost his Paribas stake above 10\%.
  \item \customcolorbox{[\textsubscript{Arg1} National Market System volume]}{green} \customcolorbox{[\textsubscript{Rel} improved]}{llight-blue} \customcolorbox{[\textsubscript{Arg4} to 94,425,00 shares]}{manatee} \customcolorbox{[\textsubscript{Arg3} from 71.7 million Monday]}{red}.
  \item \customcolorbox{[\textsubscript{Arg0} The new round of bidding]}{blue} would seem to \customcolorbox{[\textsubscript{Rel} complicate]}{llight-blue} \customcolorbox{[\textsubscript{Arg1} the decision making]}{green} \customcolorbox{[\textsubscript{Arg2} for Judge James Yacos]}{yellow}.
  \item \label{itm:wrongsrl} The action followed by one day an Intelogic announcement that it will retain \customcolorbox{[\textsubscript{Arg0} an investment banker]}{blue} to explore alternatives ``to \customcolorbox{[\textsubscript{Rel} maximize]}{llight-blue} \customcolorbox{[\textsubscript{Arg1} shareholder value]}{green},'' including the possible sale of the company.
  \item \customcolorbox{[\textsubscript{Arg0} He]}{blue} \customcolorbox{[\textsubscript{ArgM-mod} would]}{sage} scream and \customcolorbox{[\textsubscript{Rel} cut]}{llight-blue} \customcolorbox{[\textsubscript{Arg1} himself]}{green} \customcolorbox{[\textsubscript{Arg3} with rocks]}{red}.
\end{examples}

NOTE: \myfootnote{To me, not all annotations in PropBank are beyond all doubt; for example, in sentence \ref{itm:wrongsrl} ``an investment banker'' is labelled as agent ``maximzing'' the the patient ``shareholder value'' --- however, I would argue that it's rather the ``alternative'' that take proto-agentive role in maximizing the shareholder values.}

``The main reason computational systems use semantic roles is to act as a shallow meaning
representation that can let us make simple inferences that aren’t possible from the pure surface
string of words, or even from the parse tree.'' \cite[p.~375]{jurafsky2019speech}

In the literature, often \cite{gildea2002automatic} is considered to have formally defined the
task of automatic SRL.

``Analysis of semantic relations and predicate-argument structure is one of the core pieces of any
system for natural language understanding.'' \citep{palmer2010semantic}

PropBank Roles, according to \cite{bonial2012english}:

Thanks to lexical resources such as the PropBank, a multitude of models aiming at labeling
sentences with semantic roles are now available. DAMESRL \citep{do2018flexible}, trained on
the CoNLL '09 \citep{hajivc2009conll} data (which implements PropBank style SRLs).

Following some examples of DAMESRL-labelled sentences stemming from the GliBERT corpus (see chapter
\ref{chap:3_datasets}). The sentences are represented vertically with the leftmost column being the
actual sentence; each column represents one identified verb (B-V) and its predicted semantic roles,
labelled using the BIO-schema\myfootnote{Introduced by \citep{ramshaw1999text}, the BIO-schema is a
convenient way of adding a label to each token in a sequence, indicating if it belongs to a certain
subgroup, or chunk, of the sequence. For example, to mark the prepositional phrase in a syntagma
like ``He is running from the bear'', one would mark the word beginning the PP with \textbf{B}, any
other words inside the PP with \textbf{I}, and all other words outside of it, using \textbf{O}:
``He[O] is[O] running[O] from[B-PP] the[I-PP] bear[I-PP]''.}

\textbf{deISEAR}

\begin{Verbatim}[commandchars=\\\{\}]
  Ich        \colorbox{white}{B-A0}         \colorbox{white}{O}
  fühlte     \colorbox{white}{B-V}          \colorbox{white}{O}
  [MASK]     \colorbox{white}{B-A1}         \colorbox{white}{O}
  ,          \colorbox{white}{I-A1}         \colorbox{white}{O}
  als        \colorbox{white}{I-A1}         \colorbox{white}{O}
  ich        \colorbox{white}{I-A1}         \colorbox{white}{B-A0}
  aus        \colorbox{white}{I-A1}         \colorbox{white}{O}
  Versehen   \colorbox{white}{I-A1}         \colorbox{white}{O}
  schlechte  \colorbox{white}{I-A1}         \colorbox{white}{B-A1}
  Milch      \colorbox{white}{I-A1}         \colorbox{white}{I-A1}
  getrunken  \colorbox{white}{I-A1}         \colorbox{white}{B-V}
  habe       \colorbox{white}{I-A1}         \colorbox{white}{O}
\end{Verbatim}

\textbf{MLQA}

\begin{verbatim}
  Welche       B-A1       B-A2
  Positionen   I-A1       I-A2
  muss         O          O
  man          B-A0       B-A0
  erreichen    B-V        O
  ,            O          O
  um           O          O
  die          O          B-A1
  von          O          I-A1
  Kaius        O          I-A1
  angeordnete  O          I-A1
  Position     O          I-A1
  eines        O          I-A1
  Läufers      O          O
  einzunehmen  O          B-V
  ?            O          O
\end{verbatim}

\textbf{XNLI}

\begin{verbatim}
  Es           O          O               O
  war          O          O               O
  das          O          O               O
  Wichtigste   O          O               O
  was          B-A1       O               O
  wir          B-A0       O               O
  sichern      B-V        O               O
  wollten      O          O               O
  da           O          O               O
  es           O          O               O
  keine        O          B-A1            O
  Möglichkeit  O          I-A1            O
  gab          O          B-V             O
  eine         O          B-A1            B-A3
  20           O          I-A1            I-A3
  Megatonnen   O          I-A1            I-A3
  -            O          I-A1            I-A3
  H            O          I-A1            I-A3
  -            O          I-A1            I-A3
  Bombe        O          I-A1            I-A3
  ab           O          I-A1            O
  zu           O          I-A1            B-A5
  werfen       O          I-A1            B-V
  von          O          I-A1            I-A5
  einem        O          I-A1            I-A5
  30           O          I-A3            I-A5
  ,            O          O               O
  C124         O          O               O
  .            O          O               O
\end{verbatim}





% \label{sec:5_bleuscores}
%
% Table \ref{bleuresults} shows how to use the predefined tab command to have it listed.
% %\tab{#1: label}{#2: long caption}{#3: the table content}{#4: short caption}
% \tab{bleuresults}{BLEU scores of different MT systems}
% {\begin{tabular}{ll|ccc|c}
% language pair		& ABC	& YYY	\\
% \hline
% EN$\rightarrow$DE	& 20.56	& 32.53 \\
% DE$\rightarrow$EN	& 43.35	& 52.53 \\
% \hline
% \end{tabular}
% }{ABC BLEU scores}
%
% And we can reference the large table in the appendix as Table \ref{appendixTable}
%
% \section{Evaluation}
% \label{sec:5_evaluation}
% We saw in section \ref{sec:5_bleuscores}
%
% We will see in subsection \ref{subsec:5_moreeval} some more evaluations.
%
% \subsection{More evaluation}
% \label{subsec:5_moreeval}
%
%
% \section{Citations}
% Although BLEU scores should be taken with caution (see \citet{Callison-Burch2006})
% or if you prefer to cite like this: \citep{Callison-Burch2006} \ldots
%
% to cite: \cite[30-31]{Koehn2005} \\
% to cite within parentheses/brackets: \citep{Koehn2005}, \citep[30-32]{Koehn2005}\\ %\usepackage[square]{natbib} => square brackets
%
% to cite within the text: \citet{Koehn2005}, \citet[37]{Koehn2005}\\
% only the author(s): \citeauthor{Callison-Burch2006}\\
% only the year: \citeyear{Callison-Burch2006}\\
%
% \section{Graphics}
%
% To include a graphic that appears in the list of figures, use the predefined fig command:\\
% %\fig{#1: filename}{#2: label}{#3: long caption}{#4: width}{#5: short caption}
% \fig{images/Rosetta_Stone.jpg}{fig:rosetta}{The Rosetta Stone}{10}{Rosetta}
%
% %\reffig{#1: label}
% And then reference it as \reffig{fig:rosetta} is easy.
%
% \section{Some Linguistics}
%
% (With the package 'covington')\\
%
% Gloss:
%
% \begin{examples}
%  \item \gll The cat sits on the table.
% 	    die Katze sitzt auf dem Tisch
% 	\glt 'Die Katze sitzt auf dem Tisch.'
%     \glend
% \end{examples}
%
% Gloss with morphology:
%
% \begin{examples}
%  \item \gll La gata duerm -e en la cama.
% 	    Art.Fem.Sg Katze schlaf -3.Sg in Art.Fem.Sg Bett
% 	\glt 'Die Katze schl\"aft im Bett.'
%     \glend
% \end{examples}
%
