\newchap{Conclusion}
\label{chap:6_conclusion}


% In a first paragraph, I will quickly point out the core contributions of my thesis. In a second part, I
% come back to the research question ; and lastly, I will look into further steps that could be
% taken from this point on.

The core contributions of this thesis consist mainly of three components: (1)
The compilation of a German NLU dataset, \emph{GerGLUE}, incorporating different
tasks and modalities, as well as including domain-specific language, ranging from
colloquial to highly stylized texts. (2) An NLP pipeline that brings all datasets in
the same workable format, computes SRLs by implementing two freely available tools
(ParZu, DAMESRL) and several instruments for training and analyzing models. (3)
\emph{GliBERT}, a BERT-based architecture which combines vanilla BERT-embeddings
with embedded SRLs during fine-tuning, comprising several heads, and convenient
hyperparameter setting.\myfootnote{All code relating to this thesis is available
under \url{https://github.com/JonathanSchaber/Masterarbeit}.}

Extensive experimentation and subsequent result analysis indicate that providing
NLU-targeted models with contextualized word embeddings from German BERT merged
with additional, numerically encoded SRL information leads to performance gains on
two thirds of the GerGLUE datasets compared to the vanilla representations on most
tested datasets. The improvements were found to be most stable if the enriched
word embeddings were processed by the \emph{GRU Head} for the classifiction tasks.
For question answering, SRLs showed very little performance increase.

Since a model that employs enriched contextualized word embeddings still considerably relies on the
information stemming from the extensive language modeling, this information should be corrupted as
little as possible: In the case for BERT and SRLs, this means that when aligning the BERT subtokens
with the ``regular'' SRL tokens, the experiments confirmed that merging the BERT subtokens back
to token level via embedding averaging leads to irreversible numerical blurring of the resulting
embedding, which in turn leads to an overall model performance decrease --- in this case, it showed
to be more efficient to repeat the SRLs to align with the BERT subtokens.

Further research elucidated the importance of the relational nature of SRLs: The core
features of SRLs are (1) the marking of the predicate and (2) the highlighting of its
arguments and their respective roles --- while each of these two basic characteristics
in isolation does moderately leverage the model's performance, only the combination of
them leads to reliable and significant gains.

Another key insight of this thesis is the confirmation of the well-known dependence of data-driven
machine learning algorithms on data quality: Several noise sources in the data of this thesis that
hinder reliable weight parametrization were identified and analyzed. Especially the soundness of
the automatically predicted, explicit linguistic information is crucial: The predicted German
SRLs of the used DAMESRL system show a significantly lower information content than comparable
systems for English, e.g. completely lacking the ability of detecting modifiers and negation
particles. A conceivable improvement in the predicted SRLs would presumably lead to a substantial
overall NLU capability enhancement.

All these results imply that models employing self-supervised pre-trained representations learned
from unstructured, raw text data generally profit from additional, explicit linguistic information
made available during fine-tuning.

% After carrying out a multitude of experiments with different heads, SRL implementations,
% and merging techniques, I come to a guarded conclusion: For classification tasks, combining
% embedded SRL information with contextualized word representations during fine-tuning leads to
% a performance gain as was demonstrated for German BERT embeddings. But the findings are not
% too clear, sometimes there was even a negative effect observed for certain datasets and
% configurations. However, it has to be kept in mind that there was noise detected on several
% levels of the used data and implemented instruments, so that hypothetically, with cleaner
% data and better SRLs, the effect would probably be clearly positive. For the both question
% answering tasks, there was no positive effect of SRL information observable --- in contrary,
% the effect was clearly negative. On the reasons for this, I can only speculate, and need
% to delay this investigation for the other points on the list for future research in this field.


\section*{Outlook / Future Work}

During the course of the experimental phase of this thesis, several ideas and possible paths
disclosed itself which could not be pursuited due to the limited scope of this project. The
following list sketches some of these which seem worthy of investigation to me:

\begin{description}
  \item[Better SRL-predicting system] Re-run experiments with an off-th-shelf SRL-predictor that reliably
    predicts SRLs and modifiers, especially negation. My expectations are that the performance boost would
    be remarkably higher, given the comparably low informationality of the used SRLs.
  \item[Manually add negation particle] Virtually all NLU tasks are highly sensitice to
    meaning changes introduced by negativity polarity items. Since DAMESRL is not capable
    of detecting them --- at least for German ---, negation particles could be detected in the ParZu parse tree and
    added manually as \texttt{MOD-NEG} to the SRLs.
  \item[Systematic hyperparameter search] For my experiments, I tested several different hyperparameter
    settings (such as the SRL embedding size, learning rate, etc.) and implemented the ones that
    showed the best preliminary performance. Executing systematic hyperparameter space search, implementing e.g. grid search, may
    lead to additional increases in performance.
  % \item more thoroughful SRL quality estimation
  \item[Phenomenon-targeted dataset creation] For confidently assessing the counteracting of semanticity
    weak spots of BERT through SRL information, datasets compiled at targeting those would be of more
    explanatory power than general performance on regular NLU datasets.
  \item[Model weights analysis] To further assess that the model relies on the additional SRL information for
    making better predictions, analyses of the actual weight matrices of the GliBERT model could be carried out.
  \item[Other word embeddings / linguistic information] In this thesis the general hypothesis of providing
    contextualized word embeddings with explicit linguistic information was demonstrated for BERT and SRLs.
    It would be interesting, if the positive results could be replicated for different word embeddings like
    ELMo and other explicit linguistic information like e.g. knowledge graphs.
\end{description}

Reassured by the outcome of this thesis, I am convinced that future work in NLU and
NLP in general will see a comeback of linguistic instrumentary into the field. In
order to reasonably handle the highly variable input of natural language, future NLP
models will probably continue to implement some form of weight parametrization that
will be streamlined data-driven by learning from large corpora of real-world examples.
However, a complete abandoning of linguistically motivated instruments is clearly not
constructive, as various results of analyzing the limited NLU capabilities of
such models unambiguously demonstrate. In order to build systems that are able
to process natural language in a deep, semantics conveying way, systematic employment
of tools provided by linguistic insights into the inner workings of human language
are indispensable. I see GliBERT as a case of such a fertile combination of deep
learning techniques with structured linguistic knowledge representations.

% Therefore, neuro-symbolic architectures, i.e. models that learn epistemically from data while implementing
% mechanisms of symbolic reasoning, are in my opinion unavoidable for further progress in the field.
