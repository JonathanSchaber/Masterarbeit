\newchap{Conclusion}
\label{chap:6_conclusion}


In a first paragraph, I will quickly point out the core elements of my thesis. In a second part, I
locate my findings in the current debate; and lastly, I will look into further steps that could be
taken from this point on.

My experiment consists mainly of three components: (1) The compilation of a German NLU
dataset, \emph{GerGLUE}, incorporating different tasks and modalities, as well as
including domain-specific language, ranging from colloquial to highly stylized texts.
(2) A pipeline which brings all data sets in the same workable format, computes SRLs
by implementing two freely available tools (ParZu, DAMESRL) and several instruments
for training and analyzing models. (3) \emph{GliBERT}, a BERT-derived architecture
which combines vanilla BERT-embeddings with embedded SRLs during fine-tuning, with
several heads on top.

After carrying out a multitude of experiments with different heads, SRL implementations,
and merging techniques, there is a slight tendence visible: For classification tasks
injecting SRLs during finetuning seems to boost vanilla BERT embeddings to some degree.
However, it has to be kept in mind that there was noise detected on several levels of
the process, so that hypothetically, with cleaner data and better SRLs, the effect
could be {\color{red} even stronger}. For the both question answering tasks, there
was no positive effect of SRL information observable. Generally,



\section{Outlook / Future Work}



\begin{itemize}
  \item better SRL system (bspw. mit MOD-NEG detection)
  \item ``sincere'' hyper-param search
  \item more thoroughful SRL quality estimation
  \item compile data sets targeted at weaknesses of BERT
  \item ``Manually'' add negation particle from parse tree information
\end{itemize}


