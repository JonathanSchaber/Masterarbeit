\newchap{Introduction}
\label{chap:1_intro}

% \epigraph{Not only the entire ability to think rests on language... but language is also the crux of the misunderstanding of reason with itself.}{\textit{Johann Georg Hamann}}

In this chapter, I will expand a bit further to put the intent of this master's thesis into
a bigger picture. Therefore, I will line out the general problems and topics of NLU and elaborate
a little bit on the methods and techniques that were developed to address those questions.

Further, I will tie in my {\color{red} approach} with the current research efforts in NLU.


\section{Motivation}

Human language bears some truly mesmerizing features and puzzles, a lot of them are still not yet
understood in all its depths: For example, it is still unclear how children are able to learn the
grammar of their mother tongue from the corrupted and comparatively scarce language material they
are exposed to (cf. \cite{lust2006child, lewis2001learnability}).\myfootnote{The famous term describing this
phenomenon, ``Poverty of Stimulus'' (POS), was coined by \cite{chomsky1980cognitive}, arguing for
an innate language learning/processing faculty. This has led to a fierce debate over that issue:
Many researchers claim that the POS-motivated necessity for such an innate, human-specific,
genetic trait does not hold, and humans learn language simply by means of extremely sophisticated
statistic analysis. It has indeed been shown that infants are amazingly apt at extracting
statistical information from auditive input \citep{saffran1996statistical}. However, as was
countered by Chomsky and others, this does not fully explain the ability of children to apply
these statistical input cues to \emph{hierarchical} structures : ``The issue that is so central
to this particular POS problem is tacit knowledge by the child that grammatical rules apply to
such [hierarchical] structures'' \citep{berwick2013poverty} --- there is no behavioral explanation
for the tacit knowledge about hierarchical structures in the mind of the child. Therefore,
the POS argument still holds, although it has been somewhat weakend by empirical findings about
acquisition driven by statisitcal insights.}

%(POS, Chomsky etc) Another astonishing fact about human language
% is the overwhelming amount of languages that exist today, even that number was probably much
% higher a few centuries ago.  As to how languages evolve, change over time and what trajectories
% of possible change may be, lots of questions are still open, and there remains enough work
% to do.

But maybe the most trivial and enigmatic trait about human language is that we actually
\emph{understand} each other so well: That during a discourse, person X can
retrieve the intentioned meaning of expressions uttered by person Y, and
vice versa. Further, we are able to logically deduce a whole lot information
that is not explicitly stated in an expression, and uphold such a state of
affairs during the whole conversation. In fact, communication is perceived
as such an intrinsic human capability, that it is often used as a proxy for
attesting intelligence to an entity: \cite{turing1950computing} suggested
his renowned ``Turing-Test'' as method for determining the capabilities of
artificial intelligence systems: If the communicative behavior of an agent
is indistinguishable from a human agent, it is justifiable, according to
this view, to attribute ``intelligence'' to such a system.

%Maybe one of the most
% enigmatic and fascinating traits of human language is the fact that for us humans, exchanging
% information through language is a meaninglessly working process.

That reproducing human communication is not as trivial as it might look like on first glance,
reveal the following considerations, which are only a subset of all the complexities involved
in human communication:

\begin{description}
  \item[Vagueness] We have no problem dealing with vague statements like ``Most people never heard of it''; in a specific conversation situation we rely on extra-linguistic cues like pragmatics and common world knowledge to decide if ``most'' means 7.5 billion people, or 70\% of our friends, or if the approximate number is even relevant (maybe it was ironically spoken, etc.)
  \item[Ambiguity] The fact that a linguistic sign can't be interpreted in only one way, is an ubiquitous phenomenon in human language that is present on all levels: phonological, lexical, syntactic, semantic, pragmatic. A classic example for syntactic ambiguity would be the phrase ``He saw the elephant with a telescope''.
  \item[Corruption] Contrary to how we perceive language when speaking to each other, utterances are mostly not well-formed, grammatical sentences, but show a varying degree of stutter, incomplete phrases, repetitions, and other ``mistakes''. While not as present in written language, depending on the domain, there still is quite some noise present, e.g. in online chat threads etc. Still, we mostly have no problems at all, reconstructing the encoded information from such a corrupted signal.
  \item[Common world knowledge] Normally the information we encode in ordinary conversation is highly condensed and as scarce as possible --- most of the actual information is reconstructed by the receiver, making use of general knowledge about the world (factual knowledge, such as ``Bern is the capital of Switzerland'', logical consequence such as ``If $X$ was at the party yesterday evening, then $X$ was not in his apartment yesterday evening'', etc.) and the actual situation, time, and place the conversation takes place.
\end{description}

% ``The engineering side of computational linguistics, often called natural language processing
% (NLP), is largely concerned with building computational tools that do useful things with
% language'' \cite{johnson2009statistical}

So, every system that is built aiming at processing natural language in a ``deep'', human-like
manner must cope with this inherent fuzzinesses of human communication. In the field of
applied computational linguistics, often referred to as Natural Language Processing (NLP),
this subfield of research is known as Natural Language Understanding (NLU).\myfootnote{Note
that I will not engage in the discussion about whether it is philosophically appropriate
to claim that computational models ``understand'' human language. A lot of controversy has
arisen around this issue, with positions ranging from completely denying language models any
sort of (linguistic) understanding \citep{bender2021dangers} to the current trend of simply
concentrating on beating NLU SOTAs with ever larger models and blindly taking this as proof
of building architectures capable of learning human language. I would argue that the thruth
lies somewhere in between: Of course it is not enough to perform well on a standardized data
set to speak of ``understanding'', however, as \cite{sahlgren2021singleton} point out: Also
in humans, especially children, we measure language competences indirectly ``by using various
language proficiency tests, such as vocabulary tests, cloze tests, reading comprehension, as
well as various forms of production, interaction, and mediation tests \textelp{}''.}
NLU, therefore aims at producing systems which are able to retreive the semantic
content encoded in natural language and are able to further act upon it:
For example, a chatbot should be capable of ``understanding'' that the questions ``What's
the weather like?'', ``Can you tell me today's weather forecast, please?'', ``Will I need
an umbrella today?'' all have more or less the same meaning and should provoke the same
answer.\myfootnote{Although one could argue that the third question differs from the first
two since it is a polar question; i.e. a simple ``yes'' or ``no'' would be grammatically
correct --- however, I have the strong feeling one would perceive this as a very dry, or
even rude, answer and would expect a more elaborated answer in a regular conversation
context.}

% TODO: embed this quote


% ``It is always precarious to build arguments on inherently vague and general
% concepts such as `language', `understanding' and `meaning,' as the resulting
% theoretical constructs may become so overly general that they almost become
% vacuous.'' \cite{sahlgren2021singleton}

% \begin{quote}
%   learning a language entails learning the set of symbols and rules that define the
%   system. Learning the set of symbols equals vocabulary acquisition, while learning
%   the rules entails recognizing and formalizing grammatical, morphological, and
%   syntactic regularities.i We measure these competencies in humans — often indirectly
%   — by using various language proficiency tests, such as vocabulary tests, cloze
%   tests, reading comprehension, as well as various forms of production, interaction,
%   and mediation tests \textelp{}. \cite{sahlgren2021singleton}
% \end{quote}

% \begin{quote}
%   there is an overwhelming body of empirical results to demonstrate that current language models
%   have a passable capacity to detect the symbols and rules that define human language. This is not
%   what is under dispute in the current debate; what is under dispute is whether such structural
%   knowledge about the workings of the language system suffices.
% \end{quote}

\subsection{History, Methods, Problems of NLU}


% ``The `understanding' these models have of language is entirely structural,
% and thus does not extend beyond the language system'' \cite{sahlgren2021singleton}

% ``We can of course always question whether there is any `real' under-
% standing going on, but if the absence or presence of this `real' understanding
% has no effect on the behavior of the system, it will be a mere epiphenomenon
% that need not concern us.'' \cite{sahlgren2021singleton}

% ``consciousness is not an extra ingredient in addition to the complexity of a system:
% consciousness is the complexity of the system. Our point is that understanding is also
% not an extra ingredient of a symbol manipulation system: `understanding' is a term we
% use to describe the complexity of such a system.'' \cite{sahlgren2021singleton}

% ``Our point is that when the behavior of an NLU system becomes sufficiently complex, it will be
% easier to explain its behavior using intentional terms such as `understanding,' than to use a
% purely functional explanation.'' \cite{sahlgren2021singleton}

% The subsection of NLP that deals with the semantics, i.e. meanings, of utterances, is NLU.

During the first phase of NLP, approximately from the 1950ies until the 1980ies, systems
that addressed NLU problems were architectures that consisted of carefully hand-written
symbolic grammars and knowledge bases that aimed at tackling a specific problem, such as
recognizing textual entailment, coreference resolution, sentiment analysis, and so on.

From the 90ies on, the so-called emph{statistical revolution} took place, and NLU related
problems were now being addressed by learning patterns from huge data collections. One driver
of this paradigm shift were the various difficulties the traditional systems bore: Their
development ``requiring a great deal of domain-specific knowledge engineering. In addition,
the systems were brittle and could not function adequately outside the restricted tasks for
which they were designed'' \citep[p.~13]{brill1997overview}. The new, statistical {\color{red}
approach} tries to tackle NLU-problems by shifting the focus from tedious hand-crafting ``to
empirical, or corpus-based, methods in which development is much more data driven and is at
least partially automated by using statistical or machine-learning methods to train systems
on large amounts of real language data'' \citep[p.~13]{brill1997overview}. The main challenge
for engineers and scientists now laid in discovering suitable features, according to which the
algorithm would hopefully learn helpful patterns from the language data for solving the task
at hand. With this orientation towards data-driven NLP solutions came also the possibility to
compare different architectures on the same standardized data set and measured

For half a decade now a next stage in NLU and NLP in general was entered: we are now in the
middle of the \emph{neural age} of computational linguistics:
{\color{red} ``Deep Learning waves have lapped at the shores of computational linguistics for several
years now, but 2015 seems like the year when the full force of the tsunami hit the
major Natural Language Processing (NLP) conferences.'' \citep[p.~701]{manning2015computational}}

In contrast to the statistical
period's main challenge --- the identification and extraction of suitable features ---, now the
algorithms are itself learning the features that are the most informative for a given task. The
human part in the process is reduced to design the overall model architecture and compile large
enough amounts of data that are, in the best case, also of good quality.

While the roughly sketched methods above apply to a wide ranges of applications in NLP, I will now
point to some of the enquiries NLU aims at: Simply put, NLP is concernced with the structural side
of natural language text, while NLU looks at the content of these utterances. For example, typical
NLP tasks such as dependency parsing, POS-tagging, and coreference soltution don't require a
semantic representation of words or phrases --- often, it suffices to look at structural properties
such as morphology, simple frequency statstics, or transition probablities to solve such problems
to an acceptable extent. On the other hand, NLU tries to process language more in a manner as
humans do it: We infer something from language, answer questions, detect logical inconsistencies
etc.

{\color{red} A concise summary of the scope of the NLU and the numerous, non-intuitive pitfalls is given by
\cite{mcshane2017natural} in her paper on NLU in cognitive agents, where she also argues that
truly NLU-capable systems's abilities must go beyond ``mere'' symbol processing:

\begin{quote}
  cognitive agents must be nimble in the face of incomplete interpretations since even people do not
  perfectly understand every aspect of every utterance they hear. This means that once an agent
  has reached the best interpretation it can, it must determine how to proceed — be that acting
  upon the new information directly, remembering whatever it has understood and waiting to see
  what happens next, seeking out information to fill in the blanks, or asking its interlocutor
  for clarification. The reasoning needed to support NLU extends far beyond language itself,
  including, nonexhaustively, the agent’s understanding of its own plans and goals; its dynamic
  modeling of its interlocutor’s knowledge, plans, and goals, all guided by a theory of mind;
  its recognition of diverse aspects of human behavior, such as affect, cooperative behavior,
  and the effects of cognitive biases; and its integration of linguistic interpretations with
  its interpretations of other perceptive inputs, such as simulated vision and nonlinguistic
  audition. Considering all of these needs, it seems hardly possible that fundamental NLU
  will ever be achieved through the kinds of knowledge-lean text-string manipulation being
  pursued by the mainstream natural language processing (NLP) community. Instead, it requires
  a holistic approach to cognitive modeling of the type we are pursuing in a paradigm called
  OntoAgent.
\end{quote}}

All disagreements about the very nature of ``understanding'' and ``communication'' set aside,
there is probably consensus that we are far from being able to construct a system that
would pass the Turing test.
As laid out in the quote above, such a model would need
to be equipped with



\subsection{Subtasks of Solving NLU}

As the endeavor of building a holistic, fully NLU capable agent is probably an objective too
ambitious to achieve at once, the strategy in modern computational linguistics is to break it
down into smaller ``subproblems'':

Several core NLU {\color{red} skills} have been identified and datasets tailored at those
have been constructed.

For example:

\begin{description}
  \item[Sentiment Detection] Given a sentence, we can normally assess if the emotion transmitted by
      it is rather positive, negative, or neutral: ``Oh my, what a lovely day!'' is clearly positive,
      while  ``asdasd'' probably is not.
  \item[Grammaticality Recognition] Being able to understand utterances in a language also implies
      being able to judge the grammaticality of any statement in that language: ``Eagles that fly can
      eat'' is judged as valid by English speakers while ``Eagles that fly eat can'' is not.
  \item[Entailment Recognition] An important ability of understanding is the recognition of the
      logical relationship in which two utterances stand. In other words, given two sentences $A$ and
      $B$, does $B$ follow from $A$? For example, given the sentence ``The weather forecast predicts
      rainfall the next days'' does the sentence ``Tomorrow will be a beautiful, sunny day'' conform,
      contradict or stand in a neutral relation to the first sentence.
  \item[Question Answering] To locate a (or rather, the) relevant text span in a given context according to some
      questions is also a task where one would assume that some sort of understanding is needed:
      Given the question `` What was the name of the King of England?'' and the context ``Henry
      V (16 September 1386 – 31 August 1422) was King of England from 1413 until his death in
      1422.'', a system would would need to extract the span ``Henry V''.
\end{description}

Datasets addressing those tasks are listed in table ref{tab:original-GLUE} describing the GLUE
benchmark dataset compilation.


\subsection{Text Representations}

Text can be analysed and represented in various ways.\myfootnote{For simplicity, I will
focus on written language in my thesis, however this applies to other modalities like
speech and signing as well.} For example, one could represent each word of a sentence as
a number, e.g. the page number of the Oxford English Dictionary on which the definition
of the word --- or its lemma base form ---is given: <Every> <event> <has> <a> <cause>
$\rightarrow$ <234> <229> <388> <12> <176>. However, that would probably not be very
informative, but if we represent each word of a sequence by its POS, we might be able to
get some information out of it; especially if both representations, the ``normal'' one
and the POS one, are combined: <Every DET> <event NOUN> <has VERB> <a DET> <cause NOUN>.


\subsubsection*{Embeddings}

{\color{red} ``Where has Deep Learning helped NLP? The gains so far have not so much been from
true Deep Learning (use of a hierarchy of more abstract representations to promote
generalization) as from the use of distributed word representations—through the use
of real-valued vector representations of words and concepts. Having a dense, multi-
dimensional representation of similarity between all words is incredibly useful in NLP,
but not only in NLP.'' \citep[p.~703]{manning2015computational}}

Most NLU models don't operate on the text as it is, i.e. as an array of UTF-8 encoded
signs. Often, it is easier to implement an algorithm that process text in some way, to
encode this text numerically. A strategy adopted quite early is the so-called Bag of
Words \citep{harris1954distributional} technique, which encoded a sequence of words $s$
in one vector, basically indicating what items of a given vocabulary or present in $s$.
% Given a corpus of texts, the whole vocabulary $V$, or some subset,
% e.g. the most frequent 10,000 tokens, gets encoded in one large vector $v \in 0^{|V|}$. Where
% the $i$th dimension of $v$ stands for the $i$th item in $V$.
% To encode a sentence $S$ in this vector, for each token in it, the correlating dimension of the
% vector gets set to 1.
While this BOW technique is quite effective for certain tasks, e.g. information extraction
systems often make use of it, it has some flaws: It fails in particular to reflect the core
property of language being inherently sequential --- the BOW encodings of ``Alice hit Bob''
and ``Bob hit Alice'' are indistinguishable.

Therefore, other methods of representing word sequences numerically have been devised, which assign
to each word in a sequence a numeral representation, thus preserving the sequential information.
Examples of such a representation would be Latent Semantic Analysis \citep{furnas1988using}, or
word2vec \citep{mikolov2013distributed} which showed the possibility of letting a neural model
compute those embeddings in an unsupervised manner.


\paragraph*{Contextualized Word Embeddings}

First implementation\cite{bengio2003neural}

% Since the beginning of the neural age, there was the problem as to how could text be numerically
% represented, so that the algorithms could extract meaningful feature patterns and that there
% is as little information loss as possible (however, since a numeric representation is always
% an abstraction of the real data, there naturally is some unpreventable information loss). The
% solution that was proposed by \cite{mikolov2013distributed} is the approach that is still in use
% today in its core idea:

% \begin{itemize}
%   \item Initialize a random vector for each word in the vocabulary.
%   \item Train a neural model to learn the best numerical representation of each word
%     by giving it a simple task on huge amounts of unlabeled data (like CBOW, next word
%     prediction, etc.).
%   \item Save those numeric representations and use them in target task at hand.
% \endkol{itemize}

% While the basic approaches of this approach still hold --- train randomly initialized vectors on
% large amounts of unlabeled data with a neural network with a simple training goal ---, some important
% changes or additions to today's implementation have been made:

% \begin{itemize}
%   \item The original word2vec embeddings were \emph{fixed}, in the sense that a word had always
%     the same representation, regardless of the context
%   \item The neural networks that computed these vectors were quite small (two layers of dimensionality
%     300) and could be run on a standard machine.
%     Today's models are huge (hundreds of millions of parameters are not unusual) and computationally
%     very intensive and cannot be run locally.
%   \item Due to the last point, practice has shifted towards pretraining these computationally heavy
%     embeddings and finetuning them on the specific task along with it's goal
% \end{itemize}

% `` The general approach to
% building Deep Learning systems is compelling and powerful: The researcher defines a
% model architecture and a top-level loss function and then both the parameters and the
% representations of the model self-organize so as to minimize this loss, in an end-to-end
% learning framework.'' \citep[p.~703]{manning2015computational}

The architecture computing word embeddings that has caused the most uproar
recently was probably BERT \cite{devlin2018bert}, an architecture that led
to so many variants of it, that it created a whole new field inside the NLP
community --- the BERTology \cite{rogers2020primer}. BERT is a good example
for a typical neural age NLP model: It's architecture is completely agnostic
of knowledge about language whatsoever, it ``only'' operates on sequental
concatenations of symbols; there is also no preprocessing of the data --- no
POS-tagging, no dependency parsing, no NER.\myfootnote{Because of this non
linguistic specific architecture, BERT can easily be adapted to operate on
other sequential data. This has actually been done: \cite{ji2020dnabert} for
example trained a DNABERT model for successfully deciphering non-coding DNA.}
Albeit this complete lack of any sort of explicit linguistic knowledge, by
only extracting statistical patterns it learns from processing huge amounts
of text, BERT achieved several SOTAs on well-established NLU data sets, such
as GLUE \cite{wang2018glue}.

However, as I will describe in more detail in the next chapter, BERT exposes also
some weaknesses and undesirable flaws: While being able to perform surprisingly well
on some tasks, there are situations where BERT fails in rather trivial situations.



\subsubsection*{Semantic Structures}

While the motivation for representing a sentence numerically is to let algorithms operate
then ``autonomously'' on these representations, for example by computing some distance measures
between a query and a set of texts for information retrieval, one can also try to encode
some structural information of the sentence at hand.
To support a model with information about what grammatical relations between
parts of a sentence are present, one could provide the algorithm with the syntax parse
tree of the sentence. This way, the difference between the afore mentioned
two ``hitting''-sentences would be clearly distinguishable.

\paragraph*{Semantic Role Labels}

Semantic Roles are a linguistic tool developed for analyzing a sentence regarding the semantic
relations that hold between different entitles involved in the action described by it. The core
idea is to identify generalizable semantic functions, or semantic roles, that participants in an
event can engage in. With such an instrument, it is possible to model the semantic equivalence
of grammatical and syntactical quite different sentences (after \cite{palmer2010semantic}:

\begin{examples}
  \item John broke the window.
  \item The window broke.
\end{examples}

Although ``break'' is a transitive verb in the first sentence while in the second
an intransitive; although ``window'' is the grammatical object in the prior whil
the grammatical object in the latter --- the expressed action is both times that
an object, the window, was shattered.
Semantic Roles are an attempt to formalize this by attributing each noun phrase
a generalizable label. In this case, ``the window'' would be labelled as \emph{patient}
, i.e. the particiapnt undergoing a change of state, in both sentences.
In thge first, where there is also a clear initiator of this change, ``John''
would be labelled as the \emph{agent} of the event.

Thus, any sentence can be represented by the words in it replaced by their Semnantic Role Label
(SRL): <John> <broke> <the> <window> $\rightarrow$ <AGENT> <PREDICATE> <PATIENT> <PATIENT>.
(Or, as mentioned before, a combination of the ``normal'' text represenatation combined with SRLs)




\section{Contributions/Goals}


``Recently at ACL conferences, there has been
an over-focus on numbers, on beating the state of the art. Call it playing the Kaggle
game.'' \citep[p.~702]{manning2015computational}

{\color{red} As I laid out before, in the past decades computational linguistics has undergone several
``revolutions'' which, although some people might see this differently, can be described as
moving from a strong emphasis on linguistics to a more data-driven computational discipline.

Furthermore, the introduction of deep learning into computational linguistics has introduced a
so called \emph{black box}; which means essentially that although the underlying formulas and the
architecture of neural nets are well-known --- the mathematics behind them is rather simple ---,
it is nevertheless impossible to determine \emph{what exactly} those models learn from the data.

``It would be good to return some emphasis within NLP to
cognitive and scientific investigation of language rather than almost exclusively using
an engineering model of research.'' \citep[p.~706]{manning2015computational}


I see one of the contributions of my model also in re-introducing some sort of linguistic
considerations in the current NLP efforts and also to}




\section{Research Questions}

The research questions that shall be answered in this thesis, are:

\begin{enumerate}
  \item \label{ques:one} Does enriching BERT embeddings with SRL information have a positive,
                         measurable effect on NLU tasks?
  \item \label{ques:two} What role play different implementations of SRL-enriching?
\end{enumerate}

Question \ref{ques:one} has the following subquestions: \textbf{(a)} Since I replicate
in some sense the work of \cite{zhang2019semantics}, are my findings similar to what
they reported? \textbf{(b)} Are there differences between datsets, registers, tasks,
etc.? \textbf{(c)} Are the datasets well-suited for making sound statements about the
effect of enriching BERT embeddings with SRLs? \textbf{(d)} In case (c) does not hold,
is there a way to determine nevertheless if SRLs might have a positive effect in more
appropriate settings? \textbf{(e)} Can I determine what aspect(s) of SRLs support
models in downstream tasks?

Question \ref{ques:two} can be broken down into: \textbf{(a)} The number of predicate-argument
structures is defined to be three per sentence; is it more effective to fill empty slots with
0-SRLs, or duplicating existing SRLs? \textbf{(b)} To merge BERT embeddings with SRLs, either
the splitted BERT subtokens need to be merged back or the SRLs have to splitted up --- does
this lead to different results?




% \begin{enumerate}
%  \item What do I do? Try to boost BERT embeddings on NLU tasks
%  \item How do I do it?  Combine BERT embeddings with encoded SRL information, similar ti \cite{zhang2020paws}
%  \item And why? BERT lacks any linguistic knwoledge and shows error behaviour which suggests adding some could help
%  \item more concrete questions:
%  \begin{itemize}
%     \item Can I reproduce \cite{zhang2019semantics} for German?
%     \item Am I able to reach reported SOTAs of the data sets?
%     \item Is there a difference for different head architectures? And if yes, why?
%     \item general outlooks
%  \end{itemize}
% \end{enumerate}

% Questions and Claims that are \textbf{not} being made:

% \begin{itemize}
%   \item No claim of ``real'' understanding
%   \item carefully interpreting purely empirical results $\rightarrow$ data quality critique
% \end{itemize}



\section{Thesis Structure}

In this first chapter I gave a very brief overview of general trends in
NLP over the past decades and highlighted some

Chapter \ref{chap:2_approach} introduces the basic concept of BERT,
its {\color{red} shortcomings} and presented solution or improvements. Further,
I explain my approach and the relating linguistic concepts.

The datasets I compiled to test my models on are described in detail
in chapter \ref{chap:3_datasets}.

In chapter \ref{chap:4_architecture}, I describe the details of the architecture of
my BERT-variant in all detail: The identification and encoding of the semantic role
labels, the different combination procedures of the BERT embeddings with these, as
well as the different head architectures.

The overview and discussion of the performance of the various model architectures on
the GerGLUE data set is made in chapter \ref{chap:5_results}.

Finally, I draw some insights and conclusions in the last chapter \ref{chap:6_conclusion}.

