\newchap{Results}
\label{chap:5_results}

% \epigraph{Yada, yada, yada}{\textit{Someone, somewhere}}

In this chapter, I will report the results of my experiments on the six data sets. Additionally, I
conducted an ablation study on the XNLI data set, that is described in section \ref{sec:ablation}

% \subsection{deISEAR}

% \subsubsection{Example 1}

% \fig{images/SRLs_deISEAR_1.png}{fig:SRL-PAWS-X-1}{}{10}{}

% \subsubsection{Example 2}

% \fig{images/SRLs_deISEAR_2.png}{fig:SRL-PAWS-X-1}{}{15}{}

% \subsection{PAWS-X}

% \begin{landscape}

% \subsubsection{Example 1}

% \textbf{Sentence 1}

% Im Gegenzug [\textsubscript{predicate} gab] Grimoald [\textsubscript{A1} seine Tochter zur
% Hochzeit] und gewährte ihm das Herzogtum Spoleto nach dem Tod von Atto.

% Im Gegenzug gab Grimoald [\textsubscript{A0} seine Tochter] zur Hochzeit und
% [\textsubscript{predicate} gewährte] [\textsubscript{A2} ihm] [\textsubscript{A1} das Herzogtum
% Spoleto nach dem Tod von Atto] .

% \textbf{Sentence 2}

% %[\textsubscript{O} Im Gegenzug] [\textsubscript{predicate} gab] [\textsubscript{O} Grimoald]
% [\textsubscript{A1} seine Tochter] [\textsubscript{A3} in die Ehe] [\textsubscript{O} und
% gewährte ihm das Herzogtum Spoleto nach dem Tod von Atto] [\textsubscript{O} .]  Im Gegenzug
% [\textsubscript{predicate} gab] Grimoald [\textsubscript{A1} seine Tochter] [\textsubscript{A3}
% in die Ehe] und gewährte ihm das Herzogtum Spoleto nach dem Tod von Atto.

% %[\textsubscript{O} Im Gegenzug gab Grimoald] [\textsubscript{A0} seine Tochter]
% [\textsubscript{O} in die Ehe und] [\textsubscript{predicate} gewährte] [\textsubscript{A2}
% ihm] [\textsubscript{A1} das Herzogtum Spoleto nach dem Tod von Atto] [\textsubscript{O} .]  Im
% Gegenzug gab Grimoald [\textsubscript{A0} seine Tochter] in die Ehe und [\textsubscript{predicate}
% gewährte] [\textsubscript{A2} ihm] [\textsubscript{A1} das Herzogtum Spoleto nach dem Tod
% von Atto] .


% \fig{images/SRLs_PAWS-X_1.png}{fig:SRL-PAWS-X-1}{}{15}{}

% \subsubsection{Example 2}

% \textbf{Sentence 1}

% Camm [\textsubscript{predicate} entschied] , [\textsubscript{A1} dass beide Motoren eingesetzt
% werden sollten: Der Tempest Mk 5 hatte den Napier Saber eingebaut, während der Tempest Mk 2
% der Bristol Centaurus war] .

% Camm entschied, dass [\textsubscript{A1} beide Motoren] [\textsubscript{predicate} eingesetzt]
% werden sollten: [\textsubscript{A1} Der Tempest Mk 5 hatte den Napier Saber eingebaut, während]
% der Tempest Mk 2 der Bristol Centaurus war.

% Camm entschied, dass beide Motoren eingesetzt werden sollten: [\textsubscript{A0} Der Tempest
% Mk 5] hatte [\textsubscript{A3} den Napier Saber] [\textsubscript{predicate} eingebaut],
% während der Tempest Mk 2 der Bristol Centaurus war.

% Camm entschied, dass beide Motoren eingesetzt werden sollten: Der Tempest Mk 5 hatte den
% Napier Saber eingebaut, während [\textsubscript{A1} der Tempest Mk 2 der Bristol Centaurus]
% [\textsubscript{predicate} war] .

% \textbf{Sentence 2}

% Camm [\textsubscript{predicate} entschied] , [\textsubscript{A1} dass beide Motoren eingesetz
% werden sollten: Der Tempest Mk 5 war mit dem Napier Saber ausgestattet, während der Tempest
% Mk 2 den Bristol Centaurus hatte] .

% Camm entschied, dass [\textsubscript{A1} beide Motoren] [\textsubscript{predicate} eingesetzt]
% werden sollten: [\textsubscript{A1} Der Tempest Mk 5 war mit dem Napier Saber ausgestattet,
% während der Tempest Mk 2 den Bristol Centaurus hatte] .

% Camm entschied, dass beide Motoren eingesetzt werden sollten: [\textsubscript{A0} Der Tempest
% Mk 5] war [\textsubscript{A1} mit dem Napier Saber] [\textsubscript{predicate} ausgestattet]
% , während der Tempest Mk 2 den Bristol Centaurus hatte.

% Camm entschied, dass beide Motoren eingesetzt werden sollten: Der Tempest Mk 5 war mit dem
% Napier Saber ausgestattet, während [\textsubscript{A1} der Tempest Mk 2 den Bristol Centaurus]
% [\textsubscript{predicate} hatte] .

% \fig{images/SRLs_PAWS-X_2.png}{fig:SRL-PAWS-X-1}{}{15}{}

% \end{landscape}

% \subsubsection{Example 3}

% \fig{images/SRLs_PAWS-X_3.png}{fig:SRL-PAWS-X-1}{}{10}{}

% xwubsubsection{Example 4}

% \fig{images/SRLs_PAWS-X_4.png}{fig:SRL-PAWS-X-1}{}{15}{}

% \subsubsection{Example 5}

% \fig{images/SRLs_PAWS-X_5.png}{fig:SRL-PAWS-X-1}{}{10}{}

% \subsubsection{Example 6}

% \fig{images/SRLs_PAWS-X_6.png}{fig:SRL-PAWS-X-1}{}{10}{}

% \subsubsection{Example 7}

% \fig{images/SRLs_PAWS-X_7.png}{fig:SRL-PAWS-X-1}{}{10}{}

% \subsubsection{Example 8}

% \fig{images/SRLs_PAWS-X_8.png}{fig:SRL-PAWS-X-1}{}{10}{}

\section{Data Set Results}

\begin{itemize}
  \item conjecture: SRLs are rather adding noise in sequences that are too long. Extreme exmples are the Q\&A datasets.
  \item GRU architecture is probably strongest: most best models (even though mostly -SRL) and second best models, no worst performance
  \item 6 sgnificantly better +SRL vs. 3 significantly worse +SRL. There seems to be a slight trend that when merging subtokens, duplicating SRLs when too less predicates is better.
  \item all 3 worsening are for subtokenized architectures.
\end{itemize}


\begin{figure}
  \begin{minipage}{0.45\linewidth}
  \vspace{0pt}
    \includegraphics[width=1.0\linewidth]{images/XNLI_SRL_GRU_normal_200--01-11-2020_22-12-38_Accuracy.pdf}
  \end{minipage}
  \hfill
  \begin{minipage}{0.45\linewidth}
  \vspace{0pt}
    \includegraphics[width=1.0\linewidth]{images/XNLI_SRL_GRU_normal_200--01-11-2020_22-12-38_Loss.pdf}
  \end{minipage}
  \stepcounter{myfigure}
  \caption[Best Epoch Selection]{Example of selection of best epoch for XNLI SRL normal, GRU head. \textcolor{blue}{Blue}
                                = traning set, \textcolor{red}{red} = development set, \textcolor{green}{green} = test set.
                                On the \textbf{left}: The epoch with the best accuracy on the development
                                set is selected --- the horizontal drawn through line marks the development accuracy
                                of this epoch, the dashed through horizontal line marks the test accuracy
                                for the same epoch. On the \textbf{right}, the same lines mark the losses for
                                the best epoch.}
\end{figure}



To obtain as stable results as possible, I decided to train five models for each architecture
and configuration, all initialized with different random seeds. Additionally, I ensembled the
five models, achieving a performance gain of several percentage points (see example of PAWS-X,
table \ref{tab:pawsx}). In table \ref{tab:results} below, the test set ensemble results for
each architecture on the non-question answering data set are reported. The results for the
architecture for the question answering tasks are reported in table \ref{tab:results-qa}.

In a first step, I will discuss on the overall performance of models when the SRLs are added, compared to the same architectures without.

\begin{landscape}\centering
  % \vspace*{\fill}
  \tab{tab:results}{Test set accuracy ensemble results (per 5 models) on single sentence and sentence pair tasks.
                    \textbf{Bold} font marks the best result per line, \underline{underline} the second best, and \textit{italics} the poorest.
                    In the \emph{Scores} row, the afore mentioned positive extremes are accumulated for $-$SRL and $+$SRL;
                    note that if both +SRL configurations of an architecture achieved an extreme, it is only counted once.

                    The line marked with light gray --- PAWS-X $\beta$ --- is ``expanded'' in table \ref{tab:pawsx-beta} to illustrate
                    that each result in this table is actually a majority voting out of an ensembeling of fice models.}{
    \scalebox{0.90}{
      \begin{tabular}{llccc|ccc|ccc|ccc|ccc|ccc}
                                                      &           & \multicolumn{18}{c}{\large \textbf{Classification Data Sets}}  \\ \\
                                                      &           & \multicolumn{6}{c|}{\textbf{{[}CLS{]} Head}}                                                                                                                                                                                                                      & \multicolumn{6}{c|}{\textbf{FFNN Head}}                                                                                                                                                                                                & \multicolumn{6}{c}{\textbf{GRU Head}}                                                                                                                                                                                                                           \\ \cline{3-20}
                                                      &           & \multicolumn{3}{c|}{subtokenized}                                                                                               & \multicolumn{3}{c|}{subtokens merged}                                                                                           & \multicolumn{3}{c|}{subtokenized}                                                                                   & \multicolumn{3}{c|}{subtokens merged}                                                                            & \multicolumn{3}{c|}{subtokenized}                                                                                             & \multicolumn{3}{c}{subtokens merged}                                                                                            \\ \cline{3-20}
                                                      &           & \multicolumn{1}{c|}{$-$SRL}                  & \multicolumn{2}{c|}{+SRL}                                                        & \multicolumn{1}{c|}{$-$SRL}                & \multicolumn{2}{c|}{+SRL}                                                          & \multicolumn{1}{c|}{$-$SRL}                    & \multicolumn{2}{c|}{+SRL}                                          & \multicolumn{1}{c|}{$-$SRL}                    & \multicolumn{2}{c|}{+SRL}                                       & \multicolumn{1}{c|}{$-$SRL}                  & \multicolumn{2}{c|}{+SRL}                                                      & \multicolumn{1}{c|}{$-$SRL}                     & \multicolumn{2}{c}{+SRL}                                                      \\ % \cline{3-4}\cline{6-7}\cline{9-10}\cline{12-13}\cline{15-16}\cline{18-19}
                                                      &           & \multicolumn{1}{c|}{}                        & \multicolumn{1}{c}{zeros}              & \multicolumn{1}{c|}{dupl.}              & \multicolumn{1}{c|}{}                      & \multicolumn{1}{c}{zeros}                 & \multicolumn{1}{c|}{dupl.}             & \multicolumn{1}{c|}{}                          & \multicolumn{1}{c}{zeros}             & \multicolumn{1}{c|}{dupl.} & \multicolumn{1}{c|}{}                          & \multicolumn{1}{c}{zeros}          & dupl.                      & \multicolumn{1}{c|}{}                        & \multicolumn{1}{c}{zeros}             & dupl.                                  & \multicolumn{1}{c|}{}                           & \multicolumn{1}{c}{zeros}             & dupl.                                 \\ \hline\hline
         \multicolumn{1}{c}{\multirow{2}{*}{deISEAR}} & $\alpha$  & \multicolumn{1}{c|}{71.52}                   & \multicolumn{1}{c}{72.19}              & \multicolumn{1}{c|}{71.52}              & \multicolumn{1}{c|}{72.19}                 & \multicolumn{1}{c}{\textit{67.55}}        & \multicolumn{1}{c|}{72.19}             & \multicolumn{1}{c|}{70.86}                     & \multicolumn{1}{c}{\textbf{77.48}}    & \multicolumn{1}{c|}{72.85} & \multicolumn{1}{c|}{74.17}                     & \multicolumn{1}{c}{72.85}          & \multicolumn{1}{c|}{74.17} & \multicolumn{1}{c|}{70.20}                   & \multicolumn{1}{c}{\underline{74.83}} & \multicolumn{1}{c|}{74.17}             & \multicolumn{1}{c|}{73.51}                      & \multicolumn{1}{c}{70.20}             & \multicolumn{1}{c}{71.52}             \\
         \multicolumn{1}{c}{}                         & $\beta$   & \multicolumn{1}{c|}{71.52}                   & \multicolumn{1}{c}{\underline{74.83}}  & \multicolumn{1}{c|}{\underline{74.83}}  & \multicolumn{1}{c|}{70.86}                 & \multicolumn{1}{c}{70.86}                 & \multicolumn{1}{c|}{72.85}             & \multicolumn{1}{c|}{\underline{74.83}}         & \multicolumn{1}{c}{\textit{68.21}}    & \multicolumn{1}{c|}{70.20} & \multicolumn{1}{c|}{\underline{74.83}}         & \multicolumn{1}{c}{73.51}          & \multicolumn{1}{c|}{70.86} & \multicolumn{1}{c|}{73.51}                   & \multicolumn{1}{c}{\underline{74.83}} & \multicolumn{1}{c|}{72.19}             & \multicolumn{1}{c|}{\textbf{76.82}}             & \multicolumn{1}{c}{70.20}             & \multicolumn{1}{c}{\underline{74.83}} \\ \cline{3-20}
         \multicolumn{1}{c}{\multirow{2}{*}{SCARE}}   & $\alpha$  & \multicolumn{1}{c|}{\underline{85.61}}       & \multicolumn{1}{c}{82.58}              & \multicolumn{1}{c|}{83.71}              & \multicolumn{1}{c|}{83.33}                 & \multicolumn{1}{c}{83.71}                 & \multicolumn{1}{c|}{\underline{85.61}} & \multicolumn{1}{c|}{83.33}                     & \multicolumn{1}{c}{83.71}             & \multicolumn{1}{c|}{84.09} & \multicolumn{1}{c|}{84.09}                     & \multicolumn{1}{c}{\textit{81.44}} & \multicolumn{1}{c|}{84.47} & \multicolumn{1}{c|}{83.71}                   & \multicolumn{1}{c}{83.33}             & \multicolumn{1}{c|}{84.09}             & \multicolumn{1}{c|}{\textbf{85.98}}             & \multicolumn{1}{c}{84.09}             & \multicolumn{1}{c}{83.71}             \\
         \multicolumn{1}{c}{}                         & $\beta$   & \multicolumn{1}{c|}{\underline{86.36}}       & \multicolumn{1}{c}{84.85}              & \multicolumn{1}{c|}{84.47}              & \multicolumn{1}{c|}{85.23}                 & \multicolumn{1}{c}{85.23}                 & \multicolumn{1}{c|}{85.23}             & \multicolumn{1}{c|}{\textbf{86.74}}            & \multicolumn{1}{c}{85.98}             & \multicolumn{1}{c|}{85.23} & \multicolumn{1}{c|}{84.47}                     & \multicolumn{1}{c}{\textit{83.33}} & \multicolumn{1}{c|}{84.09} & \multicolumn{1}{c|}{\textbf{86.74}}          & \multicolumn{1}{c}{85.98}             & \multicolumn{1}{c|}{83.71}             & \multicolumn{1}{c|}{\underline{86.36}}          & \multicolumn{1}{c}{84.09}             & \multicolumn{1}{c}{85.23}             \\ \hline
         \multicolumn{1}{c}{\multirow{2}{*}{PAWS-X}}  & $\alpha$  & \multicolumn{1}{c|}{80.63}                   & \multicolumn{1}{c}{81.60}              & \multicolumn{1}{c|}{81.49}              & \multicolumn{1}{c|}{\textit{79.92}}        & \multicolumn{1}{c}{80.63}                 & \multicolumn{1}{c|}{82.51}             & \multicolumn{1}{c|}{81.19}                     & \multicolumn{1}{c}{80.78}             & \multicolumn{1}{c|}{80.07} & \multicolumn{1}{c|}{80.43}                     & \multicolumn{1}{c}{80.02}          & \multicolumn{1}{c|}{80.68} & \multicolumn{1}{c|}{82.26}                   & \multicolumn{1}{c}{82.77}             & \multicolumn{1}{c|}{82.77}             & \multicolumn{1}{c|}{82.82}                      & \multicolumn{1}{c}{\underline{82.87}} & \multicolumn{1}{c}{\textbf{83.53}}    \\
         \multicolumn{1}{c}{}                         & $\beta$   & \multicolumn{1}{g|}{87.49}                   & \multicolumn{1}{g}{\underline{88.05}}  & \multicolumn{1}{g|}{\textbf{88.21}}     & \multicolumn{1}{g|}{87.75}                 & \multicolumn{1}{g}{87.24}                 & \multicolumn{1}{g|}{88.00}             & \multicolumn{1}{g|}{86.83}                     & \multicolumn{1}{g}{87.39}             & \multicolumn{1}{g|}{87.09} & \multicolumn{1}{g|}{87.75}                     & \multicolumn{1}{g}{\textit{86.58}} & \multicolumn{1}{g|}{86.68} & \multicolumn{1}{g|}{87.60}                   & \multicolumn{1}{g}{87.60}             & \multicolumn{1}{g|}{87.90}             & \multicolumn{1}{g|}{88.00}                      & \multicolumn{1}{g}{88.00}             & \multicolumn{1}{g}{\underline{88.05}} \\ \cline{3-20}
         \multicolumn{1}{c}{\multirow{2}{*}{XNLI}}    & $\alpha$  & \multicolumn{1}{c|}{67.34}                   & \multicolumn{1}{c}{\textbf{67.52}}     & \multicolumn{1}{c|}{66.64}              & \multicolumn{1}{c|}{66.94}                 & \multicolumn{1}{c}{66.94}                 & \multicolumn{1}{c|}{\textit{66.26}}    & \multicolumn{1}{c|}{67.20}                     & \multicolumn{1}{c}{\underline{67.42}} & \multicolumn{1}{c|}{67.34} & \multicolumn{1}{c|}{66.38}                     & \multicolumn{1}{c}{67.08}          & \multicolumn{1}{c|}{66.92} & \multicolumn{1}{c|}{66.68}                   & \multicolumn{1}{c}{66.60}             & \multicolumn{1}{c|}{67.14}             & \multicolumn{1}{c|}{66.42}                      & \multicolumn{1}{c}{66.54}             & \multicolumn{1}{c}{66.26}             \\
         \multicolumn{1}{c}{}                         & $\beta$   & \multicolumn{1}{c|}{68.09}                   & \multicolumn{1}{c}{68.18}              & \multicolumn{1}{c|}{66.84}              & \multicolumn{1}{c|}{67.82}                 & \multicolumn{1}{c}{67.82}                 & \multicolumn{1}{c|}{\underline{68.36}} & \multicolumn{1}{c|}{66.31}                     & \multicolumn{1}{c}{65.60}             & \multicolumn{1}{c|}{66.40} & \multicolumn{1}{c|}{\textit{64.98}}            & \multicolumn{1}{c}{65.51}          & \multicolumn{1}{c|}{65.07} & \multicolumn{1}{c|}{66.84}                   & \multicolumn{1}{c}{67.82}             & \multicolumn{1}{c|}{67.02}             & \multicolumn{1}{c|}{67.64}                      & \multicolumn{1}{c}{66.31}             & \multicolumn{1}{c}{\textbf{68.53}}    \\ \hline\hline
         \multicolumn{1}{c}{Scores}                   &           & \multicolumn{1}{c|}{\underline{II}}          & \multicolumn{2}{c|}{\textbf{II} \underline{II}}                                  & \multicolumn{1}{c|}{}                      & \multicolumn{2}{c|}{\underline{II}}                                                & \multicolumn{1}{c|}{\textbf{I} \underline{I}}  & \multicolumn{2}{c|}{\textbf{I} \underline{I}}                      & \multicolumn{1}{c|}{\underline{I}}             & \multicolumn{2}{c|}{}                                           & \multicolumn{1}{c|}{\textbf{I}}              & \multicolumn{2}{c|}{\underline{II}}                                            & \multicolumn{1}{c|}{\textbf{II} \underline{I}}  & \multicolumn{2}{c}{\textbf{II} \underline{III}}                               \\ \cline{1-2}
         \multicolumn{1}{c}{+SRL}                     & \multicolumn{3}{l}{\textbf{5} \underline{10}} \\
         \multicolumn{1}{c}{$-$SRL}                   & \multicolumn{3}{l}{\textbf{4} \underline{5}}
      \end{tabular}
    }
  }{Results}

  \tab{tab:pawsx-beta}{The ``expanded'' PAWS-X $\beta$ results. The light gray line corresponds to the one in table \ref{tab:results}.
                       As can be seen, the fluctuations between single models is not too big, {\textcolor{red} which is an indicator that the architecture
                       is fairly stable.} Ensembeling reliably adds 1.26 percentage points on average.}{
    \scalebox{0.9}{
      \begin{tabular}{lccc|ccc|ccc|ccc|ccc|ccc}
                 & \multicolumn{18}{c}{\large \textbf{PAWS-X $\beta$}}  \\ \\
                 & \multicolumn{6}{c|}{\textbf{{[}CLS{]} Head}}                                                                                                                                                                                                                      & \multicolumn{6}{c|}{\textbf{FFNN Head}}                                                                                                                                                                                                         & \multicolumn{6}{c}{\textbf{GRU Head}}                                                                                                                                                                                                                           \\ \cline{2-19}
                 & \multicolumn{3}{c|}{subtokenized}                                                                                               & \multicolumn{3}{c|}{subtokens merged}                                                                                           & \multicolumn{3}{c|}{subtokenized}                                                                                            & \multicolumn{3}{c|}{subtokens merged}                                                                            & \multicolumn{3}{c|}{subtokenized}                                                                                             & \multicolumn{3}{c}{subtokens merged}                                                                                            \\ \cline{2-19}
                 & \multicolumn{1}{c|}{$-$SRL}                  & \multicolumn{2}{c|}{+SRL}                                                        & \multicolumn{1}{c|}{$-$SRL}                & \multicolumn{2}{c|}{+SRL}                                                          & \multicolumn{1}{c|}{$-$SRL}                    & \multicolumn{2}{c|}{+SRL}                                                   & \multicolumn{1}{c|}{$-$SRL}                    & \multicolumn{2}{c|}{+SRL}                                       & \multicolumn{1}{c|}{$-$SRL}                  & \multicolumn{2}{c|}{+SRL}                                                      & \multicolumn{1}{c|}{$-$SRL}                     & \multicolumn{2}{c}{+SRL}                                                      \\
                 & \multicolumn{1}{c|}{}                        & \multicolumn{1}{c}{zeros}              & \multicolumn{1}{c|}{dupl.}              & \multicolumn{1}{c|}{}                      & \multicolumn{1}{c}{zeros}                 & \multicolumn{1}{c|}{dupl.}             & \multicolumn{1}{c|}{}                          & \multicolumn{1}{c}{zeros}             & \multicolumn{1}{c|}{dupl.}          & \multicolumn{1}{c|}{}                          & \multicolumn{1}{c}{zeros}          & dupl.                      & \multicolumn{1}{c|}{}                        & \multicolumn{1}{c}{zeros}             & dupl.                                  & \multicolumn{1}{c|}{}                           & \multicolumn{1}{c}{zeros}             & dupl.                                 \\ \hline\hline
        Model 1  & \multicolumn{1}{c|}{85.41}                   & \multicolumn{1}{c}{85.36}              & \multicolumn{1}{c|}{86.02}              & \multicolumn{1}{c|}{86.43}                 & \multicolumn{1}{c}{86.53}                 & \multicolumn{1}{c|}{87.04}             & \multicolumn{1}{c|}{85.77}                     & \multicolumn{1}{c}{85.61}             & \multicolumn{1}{c|}{85.77}          & \multicolumn{1}{c|}{86.22}                     & \multicolumn{1}{c}{84.70}          & \multicolumn{1}{c|}{86.53} & \multicolumn{1}{c|}{87.54}                   & \multicolumn{1}{c}{87.49}             & \multicolumn{1}{c|}{86.43}             & \multicolumn{1}{c|}{85.51}                      & \multicolumn{1}{c}{86.93}             & \multicolumn{1}{c}{86.53}             \\
        Model 2  & \multicolumn{1}{c|}{86.07}                   & \multicolumn{1}{c}{86.83}              & \multicolumn{1}{c|}{86.99}              & \multicolumn{1}{c|}{85.87}                 & \multicolumn{1}{c}{86.32}                 & \multicolumn{1}{c|}{86.68}             & \multicolumn{1}{c|}{86.17}                     & \multicolumn{1}{c}{85.82}             & \multicolumn{1}{c|}{87.39}          & \multicolumn{1}{c|}{85.92}                     & \multicolumn{1}{c}{85.36}          & \multicolumn{1}{c|}{85.26} & \multicolumn{1}{c|}{87.04}                   & \multicolumn{1}{c}{86.99}             & \multicolumn{1}{c|}{85.87}             & \multicolumn{1}{c|}{87.19}                      & \multicolumn{1}{c}{86.07}             & \multicolumn{1}{c}{87.44}             \\
        Model 3  & \multicolumn{1}{c|}{86.07}                   & \multicolumn{1}{c}{87.49}              & \multicolumn{1}{c|}{86.99}              & \multicolumn{1}{c|}{87.14}                 & \multicolumn{1}{c}{85.26}                 & \multicolumn{1}{c|}{86.73}             & \multicolumn{1}{c|}{86.22}                     & \multicolumn{1}{c}{84.90}             & \multicolumn{1}{c|}{85.36}          & \multicolumn{1}{c|}{85.41}                     & \multicolumn{1}{c}{85.31}          & \multicolumn{1}{c|}{85.71} & \multicolumn{1}{c|}{86.12}                   & \multicolumn{1}{c}{85.66}             & \multicolumn{1}{c|}{86.83}             & \multicolumn{1}{c|}{86.48}                      & \multicolumn{1}{c}{87.09}             & \multicolumn{1}{c}{86.93}             \\
        Model 4  & \multicolumn{1}{c|}{87.39}                   & \multicolumn{1}{c}{86.32}              & \multicolumn{1}{c|}{86.58}              & \multicolumn{1}{c|}{86.38}                 & \multicolumn{1}{c}{84.04}                 & \multicolumn{1}{c|}{87.65}             & \multicolumn{1}{c|}{86.53}                     & \multicolumn{1}{c}{86.73}             & \multicolumn{1}{c|}{85.61}          & \multicolumn{1}{c|}{85.82}                     & \multicolumn{1}{c}{85.61}          & \multicolumn{1}{c|}{86.38} & \multicolumn{1}{c|}{84.99}                   & \multicolumn{1}{c}{86.02}             & \multicolumn{1}{c|}{87.70}             & \multicolumn{1}{c|}{86.99}                      & \multicolumn{1}{c}{86.68}             & \multicolumn{1}{c}{86.88}             \\
        Model 5  & \multicolumn{1}{c|}{86.63}                   & \multicolumn{1}{c}{86.43}              & \multicolumn{1}{c|}{86.58}              & \multicolumn{1}{c|}{86.99}                 & \multicolumn{1}{c}{85.26}                 & \multicolumn{1}{c|}{85.56}             & \multicolumn{1}{c|}{86.18}                     & \multicolumn{1}{c}{87.09}             & \multicolumn{1}{c|}{84.75}          & \multicolumn{1}{c|}{87.04}                     & \multicolumn{1}{c}{85.77}          & \multicolumn{1}{c|}{85.82} & \multicolumn{1}{c|}{86.12}                   & \multicolumn{1}{c}{85.82}             & \multicolumn{1}{c|}{86.73}             & \multicolumn{1}{c|}{87.34}                      & \multicolumn{1}{c}{86.73}             & \multicolumn{1}{c}{86.58}             \\ \hline\hline %\hhline{~==================}
        Average  & \multicolumn{1}{c|}{86.31}                   & \multicolumn{1}{c}{86.49}              & \multicolumn{1}{c|}{86.63}              & \multicolumn{1}{c|}{86.56}                 & \multicolumn{1}{c}{85.48}                 & \multicolumn{1}{c|}{\underline{86.81}} & \multicolumn{1}{c|}{86.22}                     & \multicolumn{1}{c}{86.03}             & \multicolumn{1}{c|}{\textit{85.78}} & \multicolumn{1}{c|}{86.08}                     & \multicolumn{1}{c}{85.35}          & \multicolumn{1}{c|}{85.94} & \multicolumn{1}{c|}{86.35}                   & \multicolumn{1}{c}{86.40}             & \multicolumn{1}{c|}{86.71}             & \multicolumn{1}{c|}{86.70}                      & \multicolumn{1}{c}{86.70}             & \multicolumn{1}{c}{\textbf{86.87}}    \\
        Ensemble & \multicolumn{1}{g|}{87.49}                   & \multicolumn{1}{g}{\underline{88.05}}  & \multicolumn{1}{g|}{\textbf{88.21}}     & \multicolumn{1}{g|}{87.75}                 & \multicolumn{1}{g}{87.24}                 & \multicolumn{1}{g|}{88.00}             & \multicolumn{1}{g|}{86.83}                     & \multicolumn{1}{g}{87.39}             & \multicolumn{1}{g|}{87.09}          & \multicolumn{1}{g|}{87.75}                     & \multicolumn{1}{g}{\textit{86.58}} & \multicolumn{1}{g|}{86.68} & \multicolumn{1}{g|}{87.60}                   & \multicolumn{1}{g}{87.60}             & \multicolumn{1}{g|}{87.90}             & \multicolumn{1}{g|}{88.00}                      & \multicolumn{1}{g}{88.00}             & \multicolumn{1}{g}{\underline{88.05}} \\ \cline{1-19}
        Gain     & \multicolumn{1}{c|}{1.18}                    & \multicolumn{1}{c}{1.56}               & \multicolumn{1}{c|}{1.58}               & \multicolumn{1}{c|}{1.19}                  & \multicolumn{1}{c}{1.76}                  & \multicolumn{1}{c|}{1.19}              & \multicolumn{1}{c|}{.65}                       & \multicolumn{1}{c}{1.36}              & \multicolumn{1}{c|}{1.31}           & \multicolumn{1}{c|}{1.67}                      & \multicolumn{1}{c}{1.23}           & \multicolumn{1}{c|}{.74}   & \multicolumn{1}{c|}{1.25}                    & \multicolumn{1}{c}{1.20}              & \multicolumn{1}{c|}{1.19}              & \multicolumn{1}{c|}{1.30}                       & \multicolumn{1}{c}{1.30}              & \multicolumn{1}{c}{1.18}              \\ \cline{1-1}
        Average  & 1.26 ($\sigma$ 0.28)
      \end{tabular}
    }
  }{Gains Ensemble vs Average}
  \vfill
\end{landscape}


\tab{tab:token-vs-merged}{Performance of architectures when BERT \customcolorbox{subtokenized}{yellow} vs. \customcolorbox{merged}{purple}. Both SRL implementations were
                          compared pairwise. Example case, upper left 4.64**: Comparison of deISEAR $\alpha$, +SRL zero
                          implementation, [CLS] Head, subtokenized ensemble (72.19\% accuracy) and the merged ensemble (67.55\%
                          accuracy) --- the subtokenized ensemble performed 4.64\% better, apparently with very high significance (p $<$ 1\%).}{
  \scalebox{0.90}{
    \begin{tabular}{llcc|cc|cc}
                                                 &           & \multicolumn{2}{c|}{\textbf{{[}CLS{]} Head}}                                                     & \multicolumn{2}{c|}{\textbf{FFNN Head}}                                                            & \multicolumn{2}{c}{\textbf{GRU Head}}                                                           \\ \cline{3-8}
                                                 &           & \multicolumn{1}{c|}{zeros}                      & \multicolumn{1}{c|}{dupl.}                     & \multicolumn{1}{c|}{zeros}                        & \multicolumn{1}{c|}{dupl.}                     & \multicolumn{1}{c|}{zeros}                    & dupl.                                           \\ \hline\hline
    \multicolumn{1}{c}{\multirow{2}{*}{deISEAR}} & $\alpha$  & \multicolumn{1}{c|}{\cellcolor{yellow} 4.64**}  & \multicolumn{1}{c|}{\cellcolor{purple} .67}    & \multicolumn{1}{c|}{\cellcolor{yellow} 4.63*}     & \multicolumn{1}{c|}{\cellcolor{purple} 1.32}   & \multicolumn{1}{c|}{\cellcolor{yellow} 4.63*} & \multicolumn{1}{c}{\cellcolor{yellow} 2.65}     \\
    \multicolumn{1}{c}{}                         & $\beta$   & \multicolumn{1}{c|}{\cellcolor{yellow} 3.97*}   & \multicolumn{1}{c|}{\cellcolor{yellow} 1.98}   & \multicolumn{1}{c|}{\cellcolor{purple} 5.30*}     & \multicolumn{1}{c|}{\cellcolor{purple} .66}    & \multicolumn{1}{c|}{\cellcolor{yellow} 4.63*} & \multicolumn{1}{c}{\cellcolor{purple} 2.64}     \\ \cline{3-8}
    \multicolumn{1}{c}{\multirow{2}{*}{SCARE}}   & $\alpha$  & \multicolumn{1}{c|}{\cellcolor{purple} 1.13}    & \multicolumn{1}{c|}{\cellcolor{purple} 1.90}   & \multicolumn{1}{c|}{\cellcolor{yellow} 2.27*}     & \multicolumn{1}{c|}{\cellcolor{purple} .38}    & \multicolumn{1}{c|}{\cellcolor{purple} .76}   & \multicolumn{1}{c}{\cellcolor{yellow} .38}      \\
    \multicolumn{1}{c}{}                         & $\beta$   & \multicolumn{1}{c|}{\cellcolor{purple} .38}     & \multicolumn{1}{c|}{\cellcolor{purple} .76}    & \multicolumn{1}{c|}{\cellcolor{yellow} 2.65**}    & \multicolumn{1}{c|}{\cellcolor{yellow} 1.14}   & \multicolumn{1}{c|}{\cellcolor{yellow} 1.89}  & \multicolumn{1}{c}{\cellcolor{purple} 1.52}     \\ \hline
    \multicolumn{1}{c}{\multirow{2}{*}{PAWS-X}}  & $\alpha$  & \multicolumn{1}{c|}{\cellcolor{yellow} .97*}    & \multicolumn{1}{c|}{\cellcolor{purple} 1.02**} & \multicolumn{1}{c|}{\cellcolor{yellow} .76}       & \multicolumn{1}{c|}{\cellcolor{purple} .61}    & \multicolumn{1}{c|}{\cellcolor{purple} .10}   & \multicolumn{1}{c}{\cellcolor{purple} .76}      \\
    \multicolumn{1}{c}{}                         & $\beta$   & \multicolumn{1}{c|}{\cellcolor{yellow} .81**}   & \multicolumn{1}{c|}{\cellcolor{yellow} .21}    & \multicolumn{1}{c|}{\cellcolor{yellow} .81*}      & \multicolumn{1}{c|}{\cellcolor{yellow} .41}    & \multicolumn{1}{c|}{\cellcolor{purple} .40}   & \multicolumn{1}{c}{\cellcolor{purple} .15}      \\ \cline{3-8}
    \multicolumn{1}{c}{\multirow{2}{*}{XNLI}}    & $\alpha$  & \multicolumn{1}{c|}{\cellcolor{yellow} .58**}   & \multicolumn{1}{c|}{\cellcolor{yellow} .38}    & \multicolumn{1}{c|}{\cellcolor{yellow} .34}       & \multicolumn{1}{c|}{\cellcolor{yellow} .42}    & \multicolumn{1}{c|}{\cellcolor{yellow} .06}   & \multicolumn{1}{c}{\cellcolor{yellow} .88**}    \\
    \multicolumn{1}{c}{}                         & $\beta$   & \multicolumn{1}{c|}{\cellcolor{yellow} .36}     & \multicolumn{1}{c|}{\cellcolor{purple} 1.52*}  & \multicolumn{1}{c|}{\cellcolor{yellow} .09}       & \multicolumn{1}{c|}{\cellcolor{yellow} 1.33*}  & \multicolumn{1}{c|}{\cellcolor{yellow} 1.51*} & \multicolumn{1}{c}{\cellcolor{purple} 1.51*}    \\
    \end{tabular}
  }
}{Tokenized vs. Merged wo QA}

\tab{tab:configs}{The different hyperparameter configurations for each data set.}{
  \scalebox{0.9}{
    \begin{tabular}{ll|ccccc}
                                                   &             & \# of epochs                           & split set up                           & batch size                               & maximum length                 & SRL implementation        \\ \hline
    \multicolumn{1}{c}{\multirow{2}{*}{deISEAR}}   &  $\alpha$   & 100                                    & normal                                 & 16                                       & 40                             & normal                    \\
    \multicolumn{1}{c}{}                           &  $\beta$    & 100                                    & normal                                 & 16                                       & 200                            & normal                    \\
    \multicolumn{1}{c}{\multirow{2}{*}{SCARE}}     &  $\alpha$   & 50                                     & normal                                 & 16                                       & 50                             & normal                    \\
    \multicolumn{1}{c}{}                           &  $\beta$    & 50                                     & normal                                 & 16                                       & 100                            & normal                    \\ \hline
    \multicolumn{1}{c}{\multirow{2}{*}{PAWS-X}}    &  $\alpha$   & 20                                     & normal                                 & 16                                       & 16                             & normal                    \\
    \multicolumn{1}{c}{}                           &  $\beta$    & 20                                     & normal                                 & 16                                       & 16                             & normal                    \\
    \multicolumn{1}{c}{\multirow{2}{*}{XNLI}}      &  $\alpha$   & 50                                     & normal                                 & 16                                       & 100                            & normal                    \\
    \multicolumn{1}{c}{}                           &  $\beta$    & 50                                     & re-split                               & 16                                       & 100                            & normal                    \\ \hline
    \multicolumn{1}{c}{\multirow{2}{*}{MLQA}}      &  $\alpha$   & \colorbox{blue}{40}                    & \colorbox{blue}{200}                   & \colorbox{blue}{50}                      & \colorbox{blue}{100}           & normal                    \\
    \multicolumn{1}{c}{}                           &  $\beta$    & \colorbox{blue}{40}                    & \colorbox{blue}{200}                   & \colorbox{blue}{50}                      & \colorbox{blue}{100}           & normal                    \\
    \multicolumn{1}{c}{\multirow{2}{*}{XQuAD}}     &  $\alpha$   & \colorbox{blue}{40}                    & \colorbox{blue}{200}                   & \colorbox{blue}{50}                      & \colorbox{blue}{100}           & 100                       \\
    \multicolumn{1}{c}{}                           &  $\beta$    & \colorbox{blue}{40}                    & \colorbox{blue}{200}                   & \colorbox{blue}{50}                      & \colorbox{blue}{100}           & 100                       \\
    \end{tabular}
  }
}{Data Set specific Configs}

\tab{tab:hyper-configs}{General hyperparameter configurations.}{
  \scalebox{0.9}{
    \begin{tabular}{ll}
    learning rate              & 2e-05                         \\
    SRL embedding dimensions   & 20                            \\
    SRL GRU hidden size        & 32                            \\
    SRL number of layers       & 2                             \\
    SRL bias                   & True                          \\
    SRL bidirectional          & True                          \\
    SRL dropout                & 0.1                           \\
    \end{tabular}
  }
}{Stable Hyperparameter Configs}



\tab{tab:gain-loss}{Ensemble percentage points gains (positive numbers) / losses (negative numbers) for +SRL
                      over $-$SRL for each configuration from table \ref{tab:results}. The better of the +SRL configurations was taken into account:
                      \customcolorbox{zeros}{blue},
                      \customcolorbox{duplicate}{dark-blue}.
                      Light blue denotes that both architectures performed
                      \customcolorbox{equally}{llight-blue}
                      (in which case both ensembles were controlled for significance). One asterisk signifies
                      a $p$-value $<$ 10\%, two stand for $p <$ 5\% and three for $p <$ 1\%.}{
    \scalebox{1}{
      \begin{tabular}{llcccccc}
                                                   &           & \multicolumn{2}{c|}{\textbf{{[}CLS{]} Head}}                                                                 & \multicolumn{2}{c|}{\textbf{FFNN Head}}                                                                        & \multicolumn{2}{c}{\textbf{GRU Head}}                                                                     \\ \cline{3-8}
                                                   &           & \multicolumn{1}{c|}{subtok.}                          & \multicolumn{1}{c|}{merged}                          & \multicolumn{1}{c|}{subtok.}                          & \multicolumn{1}{c|}{merged}                            & \multicolumn{1}{c|}{subtok.}                        & \multicolumn{1}{c}{merged}                          \\ \hline\hline
      \multicolumn{1}{c}{\multirow{2}{*}{deISEAR}} & $\alpha$  & \multicolumn{1}{c|}{\cellcolor{blue} .67}             & \multicolumn{1}{c|}{\cellcolor{dark-blue} .00}       & \multicolumn{1}{c|}{\cellcolor{blue} 6.62**}          & \multicolumn{1}{c|}{\cellcolor{dark-blue} .00}         & \multicolumn{1}{c|}{\cellcolor{blue} 4.63**}        & \multicolumn{1}{c}{\cellcolor{dark-blue} $-1.99$}   \\
      \multicolumn{1}{c}{}                         & $\beta$   & \multicolumn{1}{c|}{\cellcolor{llight-blue} 3.31}     & \multicolumn{1}{c|}{\cellcolor{dark-blue} 1.99}      & \multicolumn{1}{c|}{\cellcolor{dark-blue} $-4.63$*}   & \multicolumn{1}{c|}{\cellcolor{blue} $-1.32$}          & \multicolumn{1}{c|}{\cellcolor{blue} 1.32}          & \multicolumn{1}{c}{\cellcolor{dark-blue} $-1.99$}   \\ \cline{3-8}
      \multicolumn{1}{c}{\multirow{2}{*}{SCARE}}   & $\alpha$  & \multicolumn{1}{c|}{\cellcolor{dark-blue} $-1.90$*}   & \multicolumn{1}{c|}{\cellcolor{dark-blue} 2.28*}     & \multicolumn{1}{c|}{\cellcolor{dark-blue} .76}        & \multicolumn{1}{c|}{\cellcolor{dark-blue} .38}         & \multicolumn{1}{c|}{\cellcolor{dark-blue} .38}      & \multicolumn{1}{c}{\cellcolor{blue} $-1.89$}        \\
      \multicolumn{1}{c}{}                         & $\beta$   & \multicolumn{1}{c|}{\cellcolor{blue} $-1.51$**}       & \multicolumn{1}{c|}{\cellcolor{llight-blue} .00}     & \multicolumn{1}{c|}{\cellcolor{blue} $-.76$}          & \multicolumn{1}{c|}{\cellcolor{dark-blue} $-.38$}      & \multicolumn{1}{c|}{\cellcolor{blue} .76}           & \multicolumn{1}{c}{\cellcolor{dark-blue} $-1.13$}   \\ \hline
      \multicolumn{1}{c}{\multirow{2}{*}{PAWS-X}}  & $\alpha$  & \multicolumn{1}{c|}{\cellcolor{blue} .97*}            & \multicolumn{1}{c|}{\cellcolor{dark-blue} 2.59***}   & \multicolumn{1}{c|}{\cellcolor{blue} $-.41$}          & \multicolumn{1}{c|}{\cellcolor{dark-blue} .25}         & \multicolumn{1}{c|}{\cellcolor{llight-blue} .51}     & \multicolumn{1}{c}{\cellcolor{dark-blue} .71}       \\
      \multicolumn{1}{c}{}                         & $\beta$   & \multicolumn{1}{c|}{\cellcolor{dark-blue} .72*}       & \multicolumn{1}{c|}{\cellcolor{dark-blue} .25}       & \multicolumn{1}{c|}{\cellcolor{blue} .56}             & \multicolumn{1}{c|}{\cellcolor{dark-blue} $-1.07$***}  & \multicolumn{1}{c|}{\cellcolor{dark-blue} .30}      & \multicolumn{1}{c}{\cellcolor{dark-blue} .05}       \\ \cline{3-8}
      \multicolumn{1}{c}{\multirow{2}{*}{XNLI}}    & $\alpha$  & \multicolumn{1}{c|}{\cellcolor{blue} .18}             & \multicolumn{1}{c|}{\cellcolor{blue} $-.22$}         & \multicolumn{1}{c|}{\cellcolor{blue} .22}             & \multicolumn{1}{c|}{\cellcolor{blue} .70*}             & \multicolumn{1}{c|}{\cellcolor{dark-blue} .46}      & \multicolumn{1}{c}{\cellcolor{dark-blue} .12}       \\
      \multicolumn{1}{c}{}                         & $\beta$   & \multicolumn{1}{c|}{\cellcolor{blue} .09}             & \multicolumn{1}{c|}{\cellcolor{dark-blue} .44}       & \multicolumn{1}{c|}{\cellcolor{dark-blue} .09}        & \multicolumn{1}{c|}{\cellcolor{blue} .53}              & \multicolumn{1}{c|}{\cellcolor{blue} .98}           & \multicolumn{1}{c}{\cellcolor{dark-blue} .89}       \\ % \hline\hline
      % & \multicolumn{2}{l}{Accumulation}                  \\ \cline{2-4}
      % & \multicolumn{2}{l}{\textbf{Gains} (significant)}  & 32 (5) \\
      % & \multicolumn{2}{l}{\textbf{Losses} (significant)} & 15 (3) \\
     \end{tabular}
    }
  }{Gain-Loss}

\begin{figure}
  \begin{minipage}{0.45\linewidth}
  \vspace{0pt}
    \includegraphics[width=1.0\linewidth]{images/gain_losses_accumulated.pdf}
  \end{minipage}
  \hfill
  \begin{minipage}{0.45\linewidth}
  \vspace{0pt}
    \includegraphics[width=1.0\linewidth]{images/gain_losses_SRL.pdf}
  \end{minipage}
  \stepcounter{myfigure}
  \caption[Accumulated Gains and Losses.]{Accumulated scores from table \ref{tab:gain-loss}. \textbf{Left}: Which SRL mode performed stronger out of a total of 48 settings. The bars indicate a slight outperforming of duplicating SRLs instead of adding zeros. \textbf{Right}: Counts for Gains and Losses in all 48 settings. Darker shades indicate significant results. The light green tip stands for settings where the gain equals 0.00.}
\end{figure}

\fig{images/all_datasets_gain_dup.pdf}{fig:each-dataset-gains}{Accumulation of statistics for each classification
                                       datasetclassification. \textbf{Left}: Which SRL architecture performed better.
                                       \textbf{Right}: Comparison of accuray points gained/lost after adding SRLs.}{15}{Results Accumulation for each Data Set}

\tab{tab:confusion-scare}{Confusion matrix for SCARE $\alpha$ merged, +SRL duplicated. (macro F1: 73.52).}{
  \begin{tabular}{|ll|ccc|}
    \hline
                                                    &           & \multicolumn{3}{c|}{Predicted} \\
                                                    &           & Positive & Neutral & Negative  \\ \hline
    \multirow{3}{*}{\rotatebox[origin=c]{90}{True}} & Positive  & 149      & 12      & 5         \\
                                                    & Neutral   & 4        & 11      & 6         \\
                                                    & Negative  & 3        & 8       & 66        \\ \hline
  \end{tabular}
}{Confusion matrix for one SCARE +SRL ensemble}


\tab{tab:results-qa}{Test set accuracy ensemble results (per 5 models) on question answering tasks. \textbf{Bold} font marks the best result per line, \underline{underline} the second best, and \textit{italics} the poorest.}{
  \scalebox{1}{
    \begin{tabular}{llccc|ccc}
                                               &           & \multicolumn{6}{c}{\large \textbf{Q\&A Data Sets}}  \\ \\
                                               &           & \multicolumn{6}{c}{\textbf{Span Prediction Head}}                                                                                                                                                                                                \\ \cline{3-8}
                                               &           & \multicolumn{3}{c|}{subtokenized}                                                                                                & \multicolumn{3}{c}{subtokens merged}                                                                          \\ \cline{3-8}
                                               &           & \multicolumn{1}{c|}{$-$SRL}                     & \multicolumn{2}{c|}{+SRL}                                                      & \multicolumn{1}{c|}{$-$SRL}         & \multicolumn{2}{c}{+SRL}                                                \\ %\cline{3-4}\cline{6-7}
                                               &           & \multicolumn{1}{c|}{}                           & \multicolumn{1}{c}{zeros}             & \multicolumn{1}{c|}{dupl.}             & \multicolumn{1}{c|}{}               & \multicolumn{1}{c}{zeros}          & dupl.                              \\ \hline\hline
    \multicolumn{1}{c}{\multirow{2}{*}{MLQA}}  & $\alpha$  & \multicolumn{1}{c|}{\textbf{30.69}}             & \multicolumn{1}{c}{\underline{29.68}} & \multicolumn{1}{c|}{\underline{29.68}} & \multicolumn{1}{c|}{21.92}          & \multicolumn{1}{c}{21.92}          & \multicolumn{1}{c}{\textit{21.81}} \\
    \multicolumn{1}{c}{}                       & $\beta$   & \multicolumn{1}{c|}{\textbf{44.75}}             & \multicolumn{1}{c}{\underline{44.55}} & \multicolumn{1}{c|}{43.41}             & \multicolumn{1}{c|}{\textit{41.66}} & \multicolumn{1}{c}{41.86}          & \multicolumn{1}{c}{41.79}          \\ \cline{3-8}
    \multicolumn{1}{c}{\multirow{2}{*}{XQuAD}} & $\alpha$  & \multicolumn{1}{c|}{\textbf{42.01}}             & \multicolumn{1}{c}{\underline{41.42}} & \multicolumn{1}{c|}{41.12}             & \multicolumn{1}{c|}{37.87}          & \multicolumn{1}{c}{36.98}          & \multicolumn{1}{c}{\textit{35.50}} \\
    \multicolumn{1}{c}{}                       & $\beta$   & \multicolumn{1}{c|}{\underline{46.57}}          & \multicolumn{1}{c}{45.43}             & \multicolumn{1}{c|}{\textbf{46.86}}    & \multicolumn{1}{c|}{37.43}          & \multicolumn{1}{c}{\textit{37.14}} & \multicolumn{1}{c}{39.14}          \\ \hline\hline
    \multicolumn{1}{c}{Scores}                 &           & \multicolumn{1}{c|}{\textbf{III} \underline{I}} & \multicolumn{2}{c|}{\textbf{I} \underline{III}}                                & \multicolumn{1}{c|}{}               & \multicolumn{2}{c}{}                                                    \\ \cline{1-2}
    \multicolumn{1}{c}{+SRL}                   & \multicolumn{3}{l}{\textbf{1} \underline{3}} \\
    \multicolumn{1}{c}{$-$SRL}                 & \multicolumn{3}{l}{\textbf{3} \underline{1}}

    \end{tabular}
  }
}{Results-QA}

\tab{tab:qa-gain-loss-token-merged}{\textbf{Left part}: Ensemble percentage points gains (positive numbers) / losses (negative numbers) for +SRL
                       over $-$SRL for the Span Prediction Head from table \ref{tab:results-qa}. The better of the +SRL configurations was taken into
                       account: \customcolorbox{zeros}{blue},
                       \customcolorbox{duplicate}{dark-blue}.
                       Light blue denotes that both architectures performed \customcolorbox{equally}{llight-blue}
                       (in which case both ensembles were controlled for significance).
                       One asterisk signifies a $p$-value $<$ 10\%, two stand for $p <$ 5\% and three for $p <$ 1\%.
                       \textbf{Right part}: Performance of architectures when BERT \customcolorbox{subtokenized}{yellow} vs. \customcolorbox{merged}{purple}. Both SRL implementations were
                          compared pairwise..}{
  \scalebox{1}{
    \begin{tabular}{llP{2cm}P{2cm}cP{2cm}P{2cm}}
                                                 &           & \multicolumn{5}{c}{\textbf{Span Prediction Head}}                                                                                                                                                                   \\
                                                 &           & \multicolumn{2}{c}{Gains/Losses}                                                                            &  & \multicolumn{2}{c}{subtokenized/merged}                                                            \\ \cline{3-4} \cline{6-7}
                                                 &           & \multicolumn{1}{P{1.5cm}|}{subtok.}                            & \multicolumn{1}{P{1.5cm}}{merged}                        &  & \multicolumn{1}{P{1.5cm}|}{zeros}                       & \multicolumn{1}{P{1.5cm}}{dupl.}                       \\ \hline\hline
    \multicolumn{1}{P{1.5cm}}{\multirow{2}{*}{MLQA}}    & $\alpha$  & \multicolumn{1}{P{1.5cm}|}{\cellcolor{llight-blue} $-$1.01***} & \multicolumn{1}{P{1.5cm}}{\cellcolor{blue} .00}          &  & \multicolumn{1}{P{1.5cm}|}{\cellcolor{yellow} 7.76***}  & \multicolumn{1}{P{1.5cm}}{\cellcolor{yellow} 7.86***}  \\
    \multicolumn{1}{P{1.5cm}}{}                         & $\beta$   & \multicolumn{1}{P{1.5cm}|}{\cellcolor{blue} $-$.20}            & \multicolumn{1}{P{1.5cm}}{\cellcolor{blue} .20}          &  & \multicolumn{1}{P{1.5cm}|}{\cellcolor{yellow} 2.69}     & \multicolumn{1}{P{1.5cm}}{\cellcolor{yellow} 1.62}     \\ \cline{3-4} \cline{6-7}
    \multicolumn{1}{P{1.5cm}}{\multirow{2}{*}{XQuAD}}   & $\alpha$  & \multicolumn{1}{P{1.5cm}|}{\cellcolor{blue} $-$.59}            & \multicolumn{1}{P{1.5cm}}{\cellcolor{blue} -.89}         &  & \multicolumn{1}{P{1.5cm}|}{\cellcolor{yellow} 4.44**}   & \multicolumn{1}{P{1.5cm}}{\cellcolor{yellow} 5.62***}  \\
    \multicolumn{1}{P{1.5cm}}{}                         & $\beta$   & \multicolumn{1}{P{1.5cm}|}{\cellcolor{dark-blue} .29}          & \multicolumn{1}{P{1.5cm}}{\cellcolor{dark-blue} 1.71}    &  & \multicolumn{1}{P{1.5cm}|}{\cellcolor{yellow} 8.29***}  & \multicolumn{1}{P{1.5cm}}{\cellcolor{yellow} 7.72***}  \\
   \end{tabular}
  }
}{QA Gain-Loss / QA Tokenized vs. merged}


\begin{figure}
  \begin{minipage}{0.45\linewidth}
  \vspace{0pt}
    \includegraphics[width=1.0\linewidth]{images/QA_gain_losses_accumulated.pdf}
  \end{minipage}
  \hfill
  \begin{minipage}{0.45\linewidth}
  \vspace{0pt}
    \includegraphics[width=1.0\linewidth]{images/QA_gain_losses_SRL2.pdf}
  \end{minipage}
  \stepcounter{myfigure}
  \caption[Accumulated Gains and Losses.]{Accumulated scores from table \ref{tab:qa-gain-loss}. \textbf{Left}: Which SRL mode performed stronger out of a total of 8 settings. The bars indicate a slight outperforming of duplicating SRLs instead of adding zeros. \textbf{Right}: Counts for Gains and Losses in all 8 settings. Darker shades indicate significant results. The light green tip stands for settings where the gain equals 0.00.}
\end{figure}


\subsection{Testing for Statistical Significance}

``if we rely on empirical evaluation to validate our hypotheses and reveal the correct language processing mechanisms, we better be sure that our results are not coincidental.'' \citep{dror2018hitchhiker}


$\delta(X) = M(A, X) - M(B, X)$

$H_0:\delta(X) \leq 0$

$H_1:\delta(X) > 0$

``It is important to have a method at hand that gives us assurances that the
observed increase in the test score on a test set reflects true improvement in system
quality.'' \citep{koehn2004statistical}

\citet{koehn2004statistical} focus strongly on significance testing in the context of evaluating
on a sub-sample of the test set --- due to expensiveness of testing on the whole set ---
and making statements about the reliability of this subset sample:

``Given a test result of \emph{m} BLEU, we want to compute with a confidence \emph{q} (or
p-level P = 1 - \emph{q}) that the rue BLEU score lies in an interval [\emph{m} - \emph{d},
\emph{m} + \emph{d}].'' \citep{koehn2004statistical}

Since the systems under review here predict on the exact same test set, the assumed independence
of the predictions of the two models holds no longer. \citet{morgan2005statistical} propose
the following algorithm for testing difference significance:

``When the results are better with the new technique, a question arises as to whether
these result differences are due to the new technique actually being better or just due to
chance. Unfortunately, one usually cannot directly answer the question “what is the probability
that the new technique is better given the results on the test data set”'' \citep{yeh2000more}

``But with statistics, one can answer the following proxy question: if the new technique was
actually no different than the old technique (the null hypothesis), what is the probability
that the results on the test set would be at least this skewed in the new technique’s
favor?'' \citep{yeh2000more}

Many evaluation metrics ``have a tendency to underestimate the significance of the results'',
due to their inherent assumption that the compared systems ``produce independent results''
when in reality, they tend to produce ``positively correlated results''. \citep{yeh2000more}


\begin{algorithm}
\caption{Approximate Randomization Algorithm}
\label{alg:approximate-randomization}
	\begin{algorithmic}[1]
    \STATE $p(M,x) =$ prediction of model $M$ on example $x$
    \STATE $A, B =$ Two different models
    \STATE $O = \{x_1, \dotsc, x_n\} =$ test set
    \STATE $O_A = \{p(A,x_1), \dotsc, p(A,x_n)\}$
    \STATE $O_B = \{p(B,x_1), \dotsc, p(B,x_n)\}$
    \STATE $O_{gold} =$ gold labels for $\{x_1, \dotsc, x_n\}$
    \STATE $e(\hat{Y},Y) =$ evaluation function for gold labels $\hat{Y}$ and predictions $Y$
    \STATE $t_{original} =\ \mid e(O_{gold},O_A) - e(O_{gold},O_B) \mid$
    \STATE $rand() =$ returns $0$ or $1$, randomly
    \STATE $swap(x,y) =$ exchanges elements $x \in A,y \in B$ such that $y \in A, x \in B$
    \STATE $r \leftarrow 0$
    \STATE $R \leftarrow 0$
    \STATE $threshold \leftarrow 1,000$
    \STATE $p \leftarrow 0.05$
    \WHILE{$R < threshold$}
      \FORALL{$(a_i, b_i) \in O_A \times O_B \mid i \in I$}
        \IF{$rand() = 0$}
          \STATE $swap(a_i,b_i)$
        \ENDIF
      \ENDFOR
      \STATE $t_{permute} =\ \mid e(O_{gold},O_A') - e(O_{gold},O_B') \mid$
      \IF{$t_{permute} \geq t_{original}$}
        \STATE $r \mathrel{+}= 1$
      \ENDIF
      \STATE $R \mathrel{+}= 1$
    \ENDWHILE
    \IF{$\frac{r+1}{R+1} < p$}
      \STATE system $A$ truly better than system $B$
    \ENDIF
  \end{algorithmic}
\end{algorithm}


\subsubsection{Example Case for XNLI}


Let's consider the case for the non-merged subtokens setting in the resampled XNLI data set. The test
set contains 1,125 sentence pairs for which textual entailment must be predicted. From these 1,125
sentence pairs, 398 bear the gold label \emph{contradiction}, 357 are labeled \emph{entailment},
and 370 are \emph{neutral}; so, the class distribution of the set is fairly balanced.

I trained and optimized five systems for two architectures on the training and development set
of XNLI: One architecture is the plain ``vanilla'' GRU classifier described in section XXX, the
other is the same GRU architecture enriched with embedded SRLs (implementing the duplication
approach, described in section XXX). The ``vanilla'' system ensemble achieved an accuracy of
66,58\% on the XNLI test set, while the SRL enriched ensemble scored a 68,27\% --- in other
words, the SRL enchriched ensemble performed 1,69\% better than the ``vanilla'' ensemble.

To check if this difference truly measures the supremacy of the latter model over the first, I
apply the above described algorithm \ref{alg:approximate-randomization} for testing significance
by permuting the actual ensemble predictions. Note that both ensemble models were equally
right or wrong in 1,018 cases out of 1,125.  From this follows, in consequence, that in 90,49\%
of the cases the flipping of predictions between the ensemble models will have no effect.

Result p = 4.80\%

In contrary, if we compare this results to the zero implementation of SRLs, we observe something
different: The accuracy of this ensemble was slightly lower than the duplicate architecture;
namely 67,73\% or, speaking in differences, 1.15\% better than the vanilly ensemble. The
number of equally right or wrong examples was also slightly lower --- 1,010 examples were
equally wrong or correct predicted by the systems.

Result zeros p = 14.09\%

In summary it is safe to say that although there is a positive effect of injecting
SRL information during training over all data sets and architectures, this effect is
arguably quite small and unsteady. {\color{red} this is especially in yontrast to
\cite{zhang2019semantics}, who report more stable and higher effects} In the next
sections I will try to give an answer as to what are the reasons for these, honestly
spoken, moderate results. Concretely, I will argue that this is mainly due to noise,
present in differing intensities and at various levels in the data I acquired, that
the model has to cope with:


\fig{images/all_data_sets_token_stats.pdf}{fig:all-data-sets-token-stats}{Percentages of token-types in all data sets.
                                                                          \#\# Subtokens represent the amount of tokens that
                                                                          get re-merged in the merged settings (e.g. ``Master''
                                                                          ``\#\#arbeit'' $\rightarrow$ ``Masterarbeit''. The amount
                                                                          of tokens that lie outside of the German BERT vocabulary is
                                                                          in all data sets extremely small (for deISEAR and XNLI
                                                                          there are no [UNK]s at all); the largest shares of such tokens
                                                                          are present in MLQA and XQuAD with .79\% and .73\%,
                                                                          respectively.}{14}{Token Types all Data Sets}


\begin{itemize}
  \item[\textbf{register noise}] The textual styles vary greatly from iutilizing complex,
                                 hypotactic sentence structures (e.g. XQuAD), to highly
                                 informal, elliptic --- even erratic --- structures
                                 (e.g. SCARE).
 \item[\textbf{label noise}] Many of my data sets were constructed either automatically
                             (e.g. scrambling text automatically to create paraphrase pairs)
                             or employing crowd-sourcing techniques. Either way, the process
                             is prone to erros. There are, e.g., 84 sentence pairs in the trainig
                             set of PAWS-X that are 100\% identical, yet labelled as non-paraphrases.
 \item[\textbf{translation noise}] Due to the mostly employed semi-automatic translation
                                   approach for creating the various data sets, errors
                                   have been introduced into the data ranging from typical
                                   translation errors (e.g. English ``bishop'' in the clerical
                                   context translated to the German chess figure counterpart
                                   ``Läufer'', not ``Bischof'') to eventually wrongly copied
                                   labels, since the overall meaning changed during
                                   the tranlsation process (e.g. a sentence pair is no more
                                   contradictive but neutral).
 \item[\textbf{SRL noise}] The SRLs obtained from DAMESRL are, conservatively formulated,
                           questionable in their quality (e.g. modifiers are completely missing).
\end{itemize}

In short --- the old GIGO concept from informatics holds mutatis mutandis also in NLP.

\section{Register Noise}
\label{sec:register-noise}

\section{Label Noise}
\label{sec:label-noise}

As \citep{caswell2021quality} point out,


PAWS-X sentence number 45061, labelled as non-paraphrases:

Riverton was a parliamentary electorate in the New Zealand region of Southland .
Riverton was a parliamentary electorate in the New Zealand region of Southland .

Riverton war ein Parlamentswähler in der neuseeländischen Region Southland.
Riverton war ein Parlamentswähler in der neuseeländischen Region Southland.


\subsubsection{Re-annotation}

\textbf{PAWS-X}

2 human annotators re-label 20 examples of PAWS-X where gold != predicted.
Fleiss' $\kappa$ between 2 annotators: 0.68
Fleiss' $\kappa$ between 2 annotators and gold: 0.3541

6 examples where both annotators agree with predictions, disagree with gold:

\begin{examples}
  \item Der NVIDIA TITAN V wurde von Nvidia am 7. Dezember 2017 offiziell angekündigt.\\
        Am 07. Dezember 2017, verkündete NVIDIA offiziell Nvidia TITAN V.

        humans \& model: False, Gold: True
  \item Die Schäfte sind sehr kurz oder oft nicht vorhanden.\\
        Es sind entweder wenig Landschaften vorhanden oder sie fehlen in den meisten Fällen.

        humans \& model: False, Gold: True
  \item 1963 trat Roy der Kommunistischen Partei Indiens bei und leitete Gewerkschaftsbewegungen in Bansdroni in Kalkutta.\\
        Roy trat 1963 der Kommunistischen Partei Indiens bei und leitete Gewerkschaftsbewegungen im Kolkata-Gebiet von Bansdroni.

        humans \& model: True, Gold:False
  \item Der Kanal ist einer der ältesten schiffbaren Kanäle Europas und sogar Belgiens.\\
        Der Kanal ist einer der ältesten befahrbaren Kanäle in Belgien und Europa.

        humans \& model: True, Gold:False
  \item Propilidium pelseneeri ist eine Art der Meeresschnecken, eine wahre Napfschnecke und Gastropoden-Mollusk in der Familie der Lepetidae.\\
        Propilidium pelseneeri ist eine Art der Meeresschnecken, eine wahre Napfschnecke und Meeres-Gastropoden-Mollusk der Familie der Lepetidae.

        humans \& model: True, Gold:False
  \item Die Chicago Bears sanken auf die Giants 27:21, und verloren 0:6 zum ersten Mal seit 1976.\\
        Die Chicago Bears verloren 21:27 gegen die Bears und standen erstmals seit 1976 bei 0:6.

        humans \& model: False, Gold:True
\end{examples}


\textbf{XNLI}

2 human annotators re-label 20 examples of XNLI where gold != predicted.
Fleiss' $\kappa$ between 2 annotators: 0.36
Fleiss' $\kappa$ between 2 annotators and gold: 0.4787

1 example where 2 humans == model and 2 humans != Gold

Bato ist ein Jahrhunderte altes Wort, das man als Kerl oder Kumpel übersetzen kann.
Bato (oder Vato) ist ein spanisches Wort, das Typ oder Typ bedeutet.

humans \& model: neutral, Gold: Entailment

1 example where 2 humans != model and 2 humans != Gold

Oh, ich sehe oh der Staat braucht es nicht gut, das ist eher das, das ist eher ungewöhnlich, nicht wahr?
Das macht Sinn, dass der Staat es benötigt.

humans: neutral, model: entailment, Gold: contradiction


\section{Translation Noise}
\label{sec:translation-noise}

XNLI labelled as entailment

and that's a lot of it is due to the fact that the mothers are on drugs
The mothers take drugs.

Und vieles davon liegt daran, dass die Mütter Medikamente nehmen.
Die Mütter nehmen Drogen.

PAWS-X; different repair-strategies $\rightarrow$ different labels (gold: false)

Sawyers autorisierte Biografie wurde 2014 von Huston Smith veröffentlicht.
Im Jahr 2014 wurde Huston Smith eine autorisierte Biographie von Sawyer veröffentlicht.


Im Jahr 2014 wurde {\color{red} «}Huston Smith{\color{red} », } eine autorisierte Biographie von Sawyer{\color{red} ,} veröffentlicht.

Im Jahr 2014 wurde {\color{red} von|für|durch|trotz|wegen} Huston Smith eine autorisierte Biographie von Sawyer veröffentlicht.

Im Jahr 2014 wurde Huston Smith eine autorisierte Biographie von Sawyer veröffentlicht.


\section{SRL Noise}
\label{sec:srl-noise}

A major question arising in the context of using automatically assigned Semantic Roles in
downstream tasks, is how good these Semantic Roles are. Since there is no gold standard
available for Semantic Role Labels for the data sets I use in my experiments, there is no
straight-forward way to evaluate their quality {\color{red} automatically}. In contrast to
other tagging tasks like POS prediction or NER, Semantic Roles are not as black and white:
While it is relatively easy to decide if a predicted POS tag is correct or incorrect, it
is more a scale concerning SRLs.

\fig{images/SRL_assessment.pdf}{fig:SRL-assessment}{Independent evaluation of SRL quality by three people. Regardless of the label attributed to each example, it is obvious, that the total amount of sentences for which the annotators evaluated the corresponding semanti roles as \emph{helpful}, is relatively stable.}{11}{SRL assessment}

\fig{images/SRL_assessment_data_set.pdf}{fig:data-set-SRL-assessment}{Estimated quality of SRLs per data set.}{15}{SRL assessment per data sets}


Fleiss' $\kappa$ = 0.2048 --- this slightly above the threshold of «fair agreement», as defined by \citep{landis1977measurement} (0.20).

The $\kappa$ for helpful vs. other is even worse: 0.1944

for individual datasets:

deISEAR: 0.0814

SCARE: 0.2401

PAWS-X: 0.1245

XNLI: 0.2475

MLQA: -0.3636

XQuAD: -0.5



\cite{do2018flexible}


\section{Ablation study}
\label{sec:ablation}

To be able to make substantial claims about the positive influence about a new algorithm over an
established one, it is common ground to conduct an ablation study. In such a study, one tries to
determine which aspects of the proposed architecture contribute how much to the overall performance
gain (or loss, respectively).

In my case, i.e. the attempt to improve the performance of BERT regarding NLU tasks, the
following question would need some ablation experiments to be answered: What part of the
SRLs is most responsible for the performance boost? To be able to formulate this in a
matter which can be experimentally tested, I identify two easily separatable and testable
aspects of SRLs: Firstly, the information what parts of a sentence are the predicates.
The intuition behind this is that maybe the head relies mostly on the information as to
which tokens carry information about the events that happen in a given sentence. To test
this, I simply drpo the information about all SRLs, except the information that a token
is a predicate. In the second case, the hypothesis is reversed: Maybe the head is able to
get the most useful hints about the information which inidcates what role certain token
groups play in a given sentence. To test for this all information about predicates is
dropped and only information about arguments is preserved.

\begin{minipage}{1.0\linewidth}
  \begin{srl}
  \centering
    \begin{BVerbatim}[commandchars=\\\{\}, fontsize=\footnotesize]
      Ich        \colorbox{llight-blue}{B-A0}         \colorbox{white}{O}
      weiß       \colorbox{blue}{B-V}          \colorbox{white}{O}
      nicht      \colorbox{white}{O}            \colorbox{white}{O}
      ob         \colorbox{llight-blue}{B-A1}         \colorbox{white}{O}
      er         \colorbox{llight-blue}{I-A1}         \colorbox{llight-blue}{B-A0}
      danach     \colorbox{llight-blue}{I-A1}         \colorbox{white}{O}
      in         \colorbox{llight-blue}{I-A1}         \colorbox{llight-blue}{B-A1}
      Augusta    \colorbox{llight-blue}{I-A1}         \colorbox{llight-blue}{I-A1}
      geblieben  \colorbox{llight-blue}{I-A1}         \colorbox{blue}{B-V}
      ist        \colorbox{llight-blue}{I-A1}         \colorbox{white}{O}
      .          \colorbox{white}{O}            \colorbox{white}{O}
      ==============================
      Er         \colorbox{llight-blue}{B-A0}
      wohnte     \colorbox{blue}{B-V}
      weiterhin  \colorbox{white}{O}
      in         \colorbox{llight-blue}{B-A1}
      Augusta    \colorbox{llight-blue}{I-A1}
      .          \colorbox{white}{O}
    \end{BVerbatim}
    \caption{Normal SRLs.}
  \end{srl}
\end{minipage}

\begingroup
\begin{srl}[!h]
\centering
  \begin{minipage}{0.45\linewidth}
  \vspace{0pt}
    \begin{BVerbatim}[commandchars=\\\{\}, fontsize=\footnotesize]
    Ich        \colorbox{white}{O}           \colorbox{white}{O}
    weiß       \colorbox{blue}{B-V}         \colorbox{white}{O}
    nicht      \colorbox{white}{O}           \colorbox{white}{O}
    ob         \colorbox{white}{O}           \colorbox{white}{O}
    er         \colorbox{white}{O}           \colorbox{white}{O}
    danach     \colorbox{white}{O}           \colorbox{white}{O}
    in         \colorbox{white}{O}           \colorbox{white}{O}
    Augusta    \colorbox{white}{O}           \colorbox{white}{O}
    geblieben  \colorbox{white}{O}           \colorbox{blue}{B-V}
    ist        \colorbox{white}{O}           \colorbox{white}{O}
    .          \colorbox{white}{O}           \colorbox{white}{O}
    ==============================
    Er         \colorbox{white}{O}
    wohnte     \colorbox{blue}{B-V}
    weiterhin  \colorbox{white}{O}
    in         \colorbox{white}{O}
    Augusta    \colorbox{white}{O}
    .          \colorbox{white}{O}
    \end{BVerbatim}
  \end{minipage}
  \hfill
  \begin{minipage}{0.45\linewidth}
  \vspace{0pt}
    \begin{BVerbatim}[commandchars=\\\{\}, fontsize=\footnotesize]
      Ich        \colorbox{llight-blue}{B-A0}         \colorbox{white}{O}
      weiß       \colorbox{white}{O}            \colorbox{white}{O}
      nicht      \colorbox{white}{O}            \colorbox{white}{O}
      ob         \colorbox{llight-blue}{B-A1}         \colorbox{white}{O}
      er         \colorbox{llight-blue}{I-A1}         \colorbox{llight-blue}{B-A0}
      danach     \colorbox{llight-blue}{I-A1}         \colorbox{white}{O}
      in         \colorbox{llight-blue}{I-A1}         \colorbox{llight-blue}{B-A1}
      Augusta    \colorbox{llight-blue}{I-A1}         \colorbox{llight-blue}{I-A1}
      geblieben  \colorbox{llight-blue}{I-A1}         \colorbox{white}{O}
      ist        \colorbox{llight-blue}{I-A1}         \colorbox{white}{O}
      .          \colorbox{white}{O}            \colorbox{white}{O}
      ==============================
      Er         \colorbox{llight-blue}{B-A0}
      wohnte     \colorbox{white}{O}
      weiterhin  \colorbox{white}{O}
      in         \colorbox{llight-blue}{B-A1}
      Augusta    \colorbox{llight-blue}{I-A1}
      .          \colorbox{white}{O}
    \end{BVerbatim}
  \end{minipage}
\end{srl}
\captionof{srl}{\textbf{Left}: Only predicate SRLs. \textbf{Right}:  Only argument SRLs.}
\endgroup


\tab{tab:pawsx-abla}{Ablation on Effect of PREDs and ARGs isolated. note that PRED/ARG SRL not significant (SCARE, XNLI ARGs almost, ca. 11\%)}{
  \scalebox{0.9}{
    \begin{tabular}{llcccc}
                     &                               & \multicolumn{1}{c|}{$-$SRL}                  & \multicolumn{3}{c}{+SRL}                                                                                                 \\
                     &                               & \multicolumn{1}{c|}{}                        & \multicolumn{1}{c}{only PREDs}         & \multicolumn{1}{c}{only ARGs}           & \multicolumn{1}{c}{normal}            \\ \cline{3-6}
    deISEAR $\alpha$ & FFNN Head subtok. zeros       & \multicolumn{1}{c|}{\textit{70.86}}          & \multicolumn{1}{c}{72.19}              & \multicolumn{1}{c}{\underline{75.50**}} & \multicolumn{1}{c}{\textbf{77.48**}}  \\
    SCARE $\alpha$   & [CLS] Head merged duplicate   & \multicolumn{1}{c|}{\textit{83.33}}          & \multicolumn{1}{c}{84.47}              & \multicolumn{1}{c}{\underline{85.23}}   & \multicolumn{1}{c}{\textbf{85.61*}}   \\
    PAWS-X $\beta$   & [CLS] Head merged duplicate   & \multicolumn{1}{c|}{\textit{79.92}}          & \multicolumn{1}{c}{80.53}              & \multicolumn{1}{c}{\underline{80.68}}   & \multicolumn{1}{c}{\textbf{82.51***}} \\
    XNLI $\beta$     & GRU Head subtok. zeros        & \multicolumn{1}{c|}{\textit{66.84}}          & \multicolumn{1}{c}{67.02}              & \multicolumn{1}{c}{\textbf{68.00}}      & \multicolumn{1}{c}{\underline{67.82}} \\
    \end{tabular}
  }
}{Ablation Study}
%%\section{BLEU Scores}
%\label{sec:5_bleuscores}
%
%Table \ref{bleuresults} shows how to use the predefined tab command to have it listed.
%%\tab{#1: label}{#2: long caption}{#3: the table content}{#4: short caption}
%\tab{bleuresults}{BLEU scores of different MT systems}
%{\begin{tabular}{ll|ccc|c}
%language pair		& ABC	& YYY	\\
%\hline
%EN$\rightarrow$DE	& 20.56	& 32.53 \\
%DE$\rightarrow$EN	& 43.35	& 52.53 \\
%\hline
%\end{tabular}
%}{ABC BLEU scores}
%
%And we can reference the large table in the appendix as Table \ref{appendixTable}
%
%\section{Evaluation}
%\label{sec:5_evaluation}
%We saw in section \ref{sec:5_bleuscores}
%
%We will see in subsection \ref{subsec:5_moreeval} some more evaluations.
%
%\subsection{More evaluation}
%\label{subsec:5_moreeval}
%
%
%\section{Citations}
%Although BLEU scores should be taken with caution (see \citet{Callison-Burch2006})
%or if you prefer to cite like this: \citep{Callison-Burch2006} \ldots
%
%to cite: \cite[30-31]{Koehn2005} \\
%to cite within parentheses/brackets: \citep{Koehn2005}, \citep[30-32]{Koehn2005}\\ %\usepackage[square]{natbib} => square brackets
%
%to cite within the text: \citet{Koehn2005}, \citet[37]{Koehn2005}\\
%only the author(s): \citeauthor{Callison-Burch2006}\\
%only the year: \citeyear{Callison-Burch2006}\\
%
%\section{Graphics}
%
%To include a graphic that appears in the list of figures, use the predefined fig command:\\
%%\fig{#1: filename}{#2: label}{#3: long caption}{#4: width}{#5: short caption}
%\fig{images/Rosetta_Stone.jpg}{fig:rosetta}{The Rosetta Stone}{10}{Rosetta}
%
%%\reffig{#1: label}
%And then reference it as \reffig{fig:rosetta} is easy.
%
%\section{Some Linguistics}
%
%(With the package 'covington')\\
%
%Gloss:
%
%\begin{examples}
% \item \gll The cat sits on the table.
%	    die Katze sitzt auf dem Tisch
%	\glt 'Die Katze sitzt auf dem Tisch.'
%    \glend
%\end{examples}
%
%Gloss with morphology:
%
%\begin{examples}
% \item \gll La gata duerm -e en la cama.
%	    Art.Fem.Sg Katze schlaf -3.Sg in Art.Fem.Sg Bett
%	\glt 'Die Katze schl\"aft im Bett.'
%    \glend
%\end{examples}
%
