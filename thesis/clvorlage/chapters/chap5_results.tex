\newchap{Results}
\label{chap:5_results}

% \epigraph{Yada, yada, yada}{\textit{Someone, somewhere}}

In this chapter I report the outcomes of my experiments on the GerGLUE dataset. Because there
were several different implementations of various parameters tested against each other (SRLs duplicated vs.
SRLs zeroed, BERT tokens merged vs. SRLs split, etc.), I provide several aggregated ``views'' on the
plain numerical results, where one or more of these implementations is paid attention to. Further, I
document the outcomings of a human assessment of the quality of the DAMESRL produced SRLs to be able to
make statements about the usefulness of those and their effect on the results. Similar to this, the
general data quality of random samples of two datasets is manually reviewed, for the same reasons.
Eventually, I carry out a small ablation study, controlling the effect of ghosting out different
SRL structure parts to show that the actual, semanticity providing power of SRLs lies in the combination
of these structures.

For each dataset, two
rounds of experiments where computed with one hyperparameter difference, resulting in
an $\alpha$ and $\beta$ for each dataset. The hyperparameters which were kept stable
over all experiments are listed in table \ref{tab:hyper-configs}.

\tab{tab:hyper-configs}{Hyperparameter configurations that were kept fixed over all experiments.}{
  \scalebox{0.9}{
    \begin{tabular}{ll}
    Learning Rate              & 2e-05                         \\
    SRL Embedding Dimensions   & 20                            \\
    SRL GRU Hidden Size        & 32                            \\
    SRL Number of Layers       & 2                             \\
    SRL Bias                   & True                          \\
    SRL Bidirectional          & True                          \\
    SRL Dropout                & 0.1                           \\
    \# of Epochs               & 50 (deISEAR, SCARE, XNLI, MLQA, XQuAD), 20 (PAWS-X) \\
    Batch Size                 & 16 (classification datasets) 8 (question answering datasets) \\
    Maximum Sequence Length    & 100 (PAWS-X, XNLI) 512 (MLQA, XQuAD)
    \end{tabular}
  }
}{Fixed Hyperparameter Configs}

In table \ref{tab:alpha-beta}
the different hyperparameter for the two runs is listed for each dataset.

\tab{tab:alpha-beta}{Differences between $\alpha$ and $\beta$ runs for each dataset.}{
  \scalebox{0.9}{
    \begin{tabular}{lll}
                               & $\alpha$                   & $\beta$                          \\ \hline
    deISEAR                    & Maximum Length: 40         & Maximum Length: 200              \\
    SCARE                      & Maximum Length: 50         & Maximum Length: 100              \\
    PAWS-X                     & Examples Train Set: 8,000  & Examples Train Set: 48,977 (all) \\
    XNLI                       & Predefined Splits          & New Splits 70:15:15              \\
    MLQA                       & Predefined Splits          & New Splits 70:15:15              \\
    XQuAD                      & SRLs Encoded Sentence-wise & Regular GliBERT SRL Encoding     \\
    \end{tabular}
  }
}{$\alpha$-$\beta$ differences}

Since the different dataset types were tested with different heads, I group them the following
way: Results for classification datasets (deISEAR, SCARE, PAWS-X, XNLI) are reported together, and
the results for question answering datasets (MLQA, XQuAD), are grouped together. Since the latter
group consists of only two datasets addressing the same task, and bearing in mind that there were
only experiments for one head in this group, the Span Prediction Head, the discussion analyzing
these results will not be as interesting and substantial as for the first group. Because of the
limited scope of this thesis, and for reasons of space, only test set results are
reported in the tables. However, model selection was always based on development set results as is
visualized in figure \ref{fig:acc-loss}:
The epoch having the best development set results is selected and the
corresponding accuary on the test set is reported. The vertical line in
those three example experiments\myfootnote{\textbf{deISEAR} $\beta$ +SRL
normal duplicate [CLS] head, \textbf{SCARE} $\beta$ +SRL merge zeros GRU
head, \textbf{XNLI} $\alpha$ +SRL merge zeros [CLS] head.} marks this
epoch, the horizontal drawn through line marks the development accuarcy
(the global maximum of the red curve), and the horizontal dotted line
marks the correpsonding test accuracy. On the right the corresponding
losses for the same epoch are marked in the same manner.

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=11cm]{images/acc_loss-3_sets.pdf}
  \end{center}
  \stepcounter{myfigure}
  \caption[Accuracy/Loss plots of three experiments]{Accuracy and loss plots for three
    experiments (\textcolor{std-blue}{training}, \textcolor{std-red}{development}, and
    \textcolor{std-green}{test} set): The vertical lines mark the epoch with the highest
    development set accuracy; the test set accuracy for this epoch is reported as result. The
    corresponding losses for that epoch are marked in the plots to the right.}
  \label{fig:acc-loss}
\end{figure}

In the deISEAR plot, we see that early stopping
was triggered after the loss on the development set increased for four contiguous
epochs --- indicating an overfitting of the model on the training set. Both SCARE
and XNLI experiments ran until the maximum number of epochs were reached without
triggering early stopping. However, the SCARE accuracy and loss progressions show
much more fluctuations and erratic behavior than the XNLI ones, indicating the
noisiness of the data and the randomness of the function that needs to be learned
by the model.

\begin{tcolorbox}[
  colback=blue!5!white,
  colframe=blue!75!black,
  title={\centering Code}]

  To evaluate several models in ensemble mode, run the following command:

  \begin{verbatim}
    $ python ensemble_predict.py -r  \
                       <file_1>.json \
                       <file_2>.json \
                       ...
                       <file_n>.json
  \end{verbatim}

\end{tcolorbox}


% (see figure
% \ref{fig:acc-loss} for visualizations of example\myfootnote{\textbf{deISEAR} $\beta$
% +SRL normal duplicate [CLS] head, \textbf{SCARE} $\beta$ +SRL merge zeros GRU head,
% \textbf{XNLI} $\alpha$ +SRL merge zeros [CLS] head.} accuracy and loss progression on
% development and test results compared).


\section{Testing for Statistical Significance}

Before reporting the actual results, I briefly elaborate on my strategy for
assessing statistical significance, since this is a crucial aspect of empirical
analysis in general, and in attributing superiority of one model over the other
on the basis of measured performance on a dataset in particular:
``if we rely on empirical evaluation to validate our hypotheses and reveal the
correct language processing mechanisms, we better be sure that our results are
not coincidental'' \citep{dror2018hitchhiker}. In a formalized way, if we have a
performance measure $M$ --- in my case accuracy --- to evaluate the predictions
of a Model $A$ on a dataset $X$, we want to assess, if this performance was better
than the one of a model $B$:

$\delta(X) = M(A, X) - M(B, X)$

where $\delta(X)$ refers to the test statistic, which will be shortly explained.
Thus, the null-hypothesis which needs to be rejected, and the corresponding
alternative hypothesis (that model $A$ \emph{is} in fact truly superior to model
$B$) are formulated:

$H_0:\delta(X) \leq 0$

$H_1:\delta(X) > 0$

However, it is normally not the case that one is able to answer this question --- is the observed
performance measurement due to true model improvement or simply due to chance --- by simply looking
at the pure results; ``[b]ut with statistics, one can answer the following proxy question: if the
new technique was actually no different than the old technique (the null hypothesis), what is the
probability that the results on the test set would be at least this skewed in the new technique’s
favor?'' \citep{yeh2000more}

In other words, to be able to reject the above defined null-hypothesis, usually a $p$-value test is
computed, where $p$ is defined ``as the probability, under the null hypothesis $H_0$, of obtaining
a result equal to or more extreme than what was actually observed'' \citep[p.~1384]{dror2018hitchhiker}:

$Pr(\delta(X) \geq \delta_{observed} | H_0)$.

There are different approaches as to how this probability can
be computed: \citep{koehn2004statistical}, e.g., proposes a
``paired bootstrap resampling'' algorithm he applies to machine
translation systems. I follow here \cite{morgan2005statistical}
who argues in favour of an ``approximate randomization'' approach,
which makes less assumptions about the distribution of the sample
distributions.\myfootnote{ \citeauthor{koehn2004statistical}, for
example, presupposes that the results to be compared stem from
different subsamples of the original test set which get re-drawn
repeatedly with replacement. In my case, this does not hold since
the predictions are always performed on the whole test set.}

% ``It is important to have a method at hand that gives us assurances that the
% observed increase in the test score on a test set reflects true improvement in system
% quality.'' \citep{koehn2004statistical}

% \citet{koehn2004statistical} focus strongly on significance testing in the context of evaluating
% on a sub-sample of the test set --- due to expensiveness of testing on the whole set ---
% and making statements about the reliability of this subset sample:

% ``Given a test result of \emph{m} BLEU, we want to compute with a confidence \emph{q} (or
% p-level P = 1 - \emph{q}) that the rue BLEU score lies in an interval [\emph{m} - \emph{d},
% \emph{m} + \emph{d}].'' \citep{koehn2004statistical}

% Since the systems under review here predict on the exact same test set, the assumed independence
% of the predictions of the two models holds no longer. \citet{morgan2005statistical} propose
% the following algorithm for testing difference significance:

% ``When the results are better with the new technique, a question arises as to whether
% these result differences are due to the new technique actually being better or just due to
% chance. Unfortunately, one usually cannot directly answer the question “what is the probability
% that the new technique is better given the results on the test dataset'' \citep{yeh2000more}

% ``But with statistics, one can answer the following proxy question: if the new technique was
% actually no different than the old technique (the null hypothesis), what is the probability
% that the results on the test set would be at least this skewed in the new technique’s
% favor?'' \citep{yeh2000more}

% Many evaluation metrics ``have a tendency to underestimate the significance of the results'',
% due to their inherent assumption that the compared systems ``produce independent results''
% when in reality, they tend to produce ``positively correlated results''. \citep{yeh2000more}

The basic idea is to go through the paired predictions of two models (on
the same test set) and
randomly (i.e. with a 50\% chance) flip the predictions between them. When
this has been completed for the whole test set, the difference $\delta_{perm}$ of the
evaluation metric between the recomputed performance of them is calculated again
and it is checked whether $\delta_{perm} \geq \delta_{orig}$. This process is repeated
$R$ times and if the number of times $\delta_{perm}$ was greater/equal is small
enough, in statistics, normally 5\%, for a large enough number $R$, we can reject
the null-hypothesis with good confidence. See the following algorithm \ref{alg:approximate-randomization}
for a more detailed formulation of this process:

\begin{algorithm}
\caption{Approximate Randomization Algorithm}
\label{alg:approximate-randomization}
	\begin{algorithmic}[1]
    \STATE $p(M,x) =$ prediction of model $M$ on example $x$
    \STATE $A, B =$ Two different models
    \STATE $O = \{x_1, \dotsc, x_n\} =$ test set
    \STATE $O_A = \{p(A,x_1), \dotsc, p(A,x_n)\}$
    \STATE $O_B = \{p(B,x_1), \dotsc, p(B,x_n)\}$
    \STATE $O_{gold} =$ gold labels for $\{x_1, \dotsc, x_n\}$
    \STATE $e(\hat{Y},Y) =$ evaluation function for gold labels $\hat{Y}$ and predictions $Y$
    \STATE $t_{original} =\ \mid e(O_{gold},O_A) - e(O_{gold},O_B) \mid$
    \STATE $rand() =$ returns $0$ or $1$, randomly
    \STATE $swap(x,y) =$ exchanges elements $x \in A, y \in B$ such that $y \in A, x \in B$
    \STATE $r \leftarrow 0$
    \STATE $R \leftarrow 0$
    \STATE $threshold \leftarrow 1,000$
    \STATE $p \leftarrow 0.05$
    \WHILE{$R < threshold$}
      \FORALL{$(a_i, b_i) \in O_A \times O_B \mid i \in I$}
        \IF{$rand() = 0$}
          \STATE $swap(a_i,b_i)$
        \ENDIF
      \ENDFOR
      \STATE $t_{permute} =\ \mid e(O_{gold},O_A') - e(O_{gold},O_B') \mid$
      \IF{$t_{permute} \geq t_{original}$}
        \STATE $r \mathrel{+}= 1$
      \ENDIF
      \STATE $R \mathrel{+}= 1$
    \ENDWHILE
    \IF{$\frac{r+1}{R+1} < p$}
      \STATE system $A$ significantly better than system $B$
    \ENDIF
  \end{algorithmic}
\end{algorithm}


\begin{tcolorbox}[
  colback=blue!5!white,
  colframe=blue!75!black,
  title={\centering Code}]

  To control for two models, or ensembles, if the differences between the control (\texttt{-c}) and test
  (\texttt{-t}) one are statistically significant, run the following command:

  \begin{verbatim}
    $ python test_significanc.py -c          \
                       <control_file_1>.json \
                       <control_file_2>.json \
                       ...
                       <control_file_n>.json \
                       -t                    \
                       <test_file_1>.json    \
                       <test_file_2>.json    \
                       ...
                       <test_file_n>.json
  \end{verbatim}

\end{tcolorbox}

% \subsubsection{Example Case for XNLI}

% Let's consider the case for the non-merged subtokens setting in the resampled XNLI dataset. The test
% set contains 1,125 sentence pairs for which textual entailment must be predicted. From these 1,125
% sentence pairs, 398 bear the gold label \emph{contradiction}, 357 are labeled \emph{entailment},
% and 370 are \emph{neutral}; so, the class distribution of the set is fairly balanced.

% I trained and optimized five systems for two architectures on the training and development set
% of XNLI: One architecture is the plain ``vanilla'' GRU classifier described in section XXX, the
% other is the same GRU architecture enriched with embedded SRLs (implementing the duplication
% approach, described in section XXX). The ``vanilla'' system ensemble achieved an accuracy of
% 66,58\% on the XNLI test set, while the SRL enriched ensemble scored a 68,27\% --- in other
% words, the SRL enchriched ensemble performed 1,69\% better than the ``vanilla'' ensemble.

% To check if this difference truly measures the supremacy of the latter model over the first, I
% apply the above described algorithm \ref{alg:approximate-randomization} for testing significance
% by permuting the actual ensemble predictions. Note that both ensemble models were equally
% right or wrong in 1,018 cases out of 1,125.  From this follows, in consequence, that in 90,49\%
% of the cases the flipping of predictions between the ensemble models will have no effect.

% Result p = 4.80\%

% In contrary, if we compare this results to the zero implementation of SRLs, we observe something
% different: The accuracy of this ensemble was slightly lower than the duplicate architecture;
% namely 67,73\% or, speaking in differences, 1.15\% better than the vanilly ensemble. The
% number of equally right or wrong examples was also slightly lower --- 1,010 examples were
% equally wrong or correct predicted by the systems.

% Result zeros p = 14.09\%

% In summary it is safe to say that although there is a positive effect of injecting
% SRL information during training over all datasets and architectures, this effect is
% arguably quite small and unsteady. {\color{red} this is especially in yontrast to
% \cite{zhang2019semantics}, who report more stable and higher effects} In the next
% sections I will try to give an answer as to what are the reasons for these, honestly
% spoken, moderate results. Concretely, I will argue that this is mainly due to noise,
% present in differing intensities and at various levels in the data I acquired, that
% the model has to cope with:

% Afterwards, statistical signifance is indicated by appending one, two, or three asterisks
% to a result that was compared with one or several others: * stands for $p < 10\%$, ** for
% $p < 5\%$, and *** indicates very high significance of $p < 1\%$.



\section{Classification Dataset Results}
\label{sec:classification-results}

To obtain as stable results as possible, I decided to train five models for each architecture
and configuration, all initialized with different random seeds. Additionally, I ensembled the
five models, achieving a performance gain of several percentage points (see example of PAWS-X,
table \ref{tab:pawsx-beta}).\myfootnote{For ensembling, a straightforward majority vote of the five models
is implemented.} In table \ref{tab:results} below, the test set ensemble results for
each architecture on the classification datasets are reported.

The best, second best and worst performance over all three heads and SRL configurations
are marked. In the last row, best and second best results per head and +/$-$SRL for each
subtokenization settings are accumulated. As the scores indicate, the results taken without
controlling for significance,\myfootnote{ Here I don't test for statistical significance
because it is not clear, against what model(s) should a peak performance be controlled
for: Take the 77.45\% accuracy of the deISEAR $\alpha$, achieved by the +SRL, BERT tokens
subtokenized, FFNN head: Against what should this results be compared to? To all other $-$SRL
results? Only to the corresponding $-$SRL results for the same head?} the +SRL configurations
achieved 5 times the best, and 10 times the second best results --- compared to 4 best and
5 second best results for $-$SRL. This is a first indicator that adding SRL information seems
to be able to improve plain BERT embeddings for classification tasks.

The accumulations in roman numerals also indicate a further, observable trend: The GRU head
accumulated 5 best and 6 second best results, independent of with or without SRLs, appears to be better
suited for classification tasks than the [CLS] (2 best, 6 second best) and FFNN (2 best, 3 second best) heads.
Supporting this, the GRU head never was responsible for the worst performance, which were all
computed by the FFNN and [CLS] heads.

In the following table \ref{tab:pawsx-beta}, the ensembling of each of the results in the main table is
disaggregated as an example. Each head-SRL configuration is randomly initialized 5 times, resulting in
5 models achieving slightly different performances on the dataset. By ensembling them, a steady gain of, in this case,
1.26\% compared to the average of the 5 models is achieved. Similar ensembling improvements where
observed for all other datasets.

% In a first step, I will discuss on the overall performance of models when the SRLs are added,
% compared to the same architectures without.

\begin{landscape}\centering
  % \vspace*{\fill}
  \tab{tab:results}{Test set accuracy ensemble results (per 5 models) on single sentence and sentence pair tasks.
                    \textbf{Bold} font marks the best result per line, \underline{underline} the second best, and \textit{italics} the poorest.
                    In the \emph{Scores} row, the aforementioned positive extremes are accumulated for $-$SRL and $+$SRL;
                    note that if both +SRL configurations of an architecture achieved an extreme, it is only counted once.

                    The line marked with light gray --- PAWS-X $\beta$ --- is ``expanded'' in table \ref{tab:pawsx-beta} to illustrate
                    that each result in this table is actually a majority voting out of an ensembeling of five models.}{
    \scalebox{0.90}{
      \begin{tabular}{llccc|ccc|ccc|ccc|ccc|ccc}
                                                      &           & \multicolumn{18}{c}{\large \textbf{Classification Datasets}}  \\ \\
                                                      &           & \multicolumn{6}{c|}{\textbf{{[}CLS{]} Head}}                                                                                                                                                                                                                      & \multicolumn{6}{c|}{\textbf{FFNN Head}}                                                                                                                                                                                                & \multicolumn{6}{c}{\textbf{GRU Head}}                                                                                                                                                                                                                           \\ \cline{3-20}
                                                      &           & \multicolumn{3}{c|}{subtokenized}                                                                                               & \multicolumn{3}{c|}{subtokens merged}                                                                                           & \multicolumn{3}{c|}{subtokenized}                                                                                   & \multicolumn{3}{c|}{subtokens merged}                                                                            & \multicolumn{3}{c|}{subtokenized}                                                                                             & \multicolumn{3}{c}{subtokens merged}                                                                                            \\ \cline{3-20}
                                                      &           & \multicolumn{1}{c|}{$-$SRL}                  & \multicolumn{2}{c|}{+SRL}                                                        & \multicolumn{1}{c|}{$-$SRL}                & \multicolumn{2}{c|}{+SRL}                                                          & \multicolumn{1}{c|}{$-$SRL}                    & \multicolumn{2}{c|}{+SRL}                                          & \multicolumn{1}{c|}{$-$SRL}                    & \multicolumn{2}{c|}{+SRL}                                       & \multicolumn{1}{c|}{$-$SRL}                  & \multicolumn{2}{c|}{+SRL}                                                      & \multicolumn{1}{c|}{$-$SRL}                     & \multicolumn{2}{c}{+SRL}                                                      \\ % \cline{3-4}\cline{6-7}\cline{9-10}\cline{12-13}\cline{15-16}\cline{18-19}
                                                      &           & \multicolumn{1}{c|}{}                        & \multicolumn{1}{c}{zeros}              & \multicolumn{1}{c|}{dupl.}              & \multicolumn{1}{c|}{}                      & \multicolumn{1}{c}{zeros}                 & \multicolumn{1}{c|}{dupl.}             & \multicolumn{1}{c|}{}                          & \multicolumn{1}{c}{zeros}             & \multicolumn{1}{c|}{dupl.} & \multicolumn{1}{c|}{}                          & \multicolumn{1}{c}{zeros}          & dupl.                      & \multicolumn{1}{c|}{}                        & \multicolumn{1}{c}{zeros}             & dupl.                                  & \multicolumn{1}{c|}{}                           & \multicolumn{1}{c}{zeros}             & dupl.                                 \\ \hline\hline
         \multicolumn{1}{c}{\multirow{2}{*}{deISEAR}} & $\alpha$  & \multicolumn{1}{c|}{71.52}                   & \multicolumn{1}{c}{72.19}              & \multicolumn{1}{c|}{71.52}              & \multicolumn{1}{c|}{72.19}                 & \multicolumn{1}{c}{\textit{67.55}}        & \multicolumn{1}{c|}{72.19}             & \multicolumn{1}{c|}{70.86}                     & \multicolumn{1}{c}{\textbf{77.48}}    & \multicolumn{1}{c|}{72.85} & \multicolumn{1}{c|}{74.17}                     & \multicolumn{1}{c}{72.85}          & \multicolumn{1}{c|}{74.17} & \multicolumn{1}{c|}{70.20}                   & \multicolumn{1}{c}{\underline{74.83}} & \multicolumn{1}{c|}{74.17}             & \multicolumn{1}{c|}{73.51}                      & \multicolumn{1}{c}{70.20}             & \multicolumn{1}{c}{71.52}             \\
         \multicolumn{1}{c}{}                         & $\beta$   & \multicolumn{1}{c|}{71.52}                   & \multicolumn{1}{c}{\underline{74.83}}  & \multicolumn{1}{c|}{\underline{74.83}}  & \multicolumn{1}{c|}{70.86}                 & \multicolumn{1}{c}{70.86}                 & \multicolumn{1}{c|}{72.85}             & \multicolumn{1}{c|}{\underline{74.83}}         & \multicolumn{1}{c}{\textit{68.21}}    & \multicolumn{1}{c|}{70.20} & \multicolumn{1}{c|}{\underline{74.83}}         & \multicolumn{1}{c}{73.51}          & \multicolumn{1}{c|}{70.86} & \multicolumn{1}{c|}{73.51}                   & \multicolumn{1}{c}{\underline{74.83}} & \multicolumn{1}{c|}{72.19}             & \multicolumn{1}{c|}{\textbf{76.82}}             & \multicolumn{1}{c}{70.20}             & \multicolumn{1}{c}{\underline{74.83}} \\ \cline{3-20}
         \multicolumn{1}{c}{\multirow{2}{*}{SCARE}}   & $\alpha$  & \multicolumn{1}{c|}{\underline{85.61}}       & \multicolumn{1}{c}{82.58}              & \multicolumn{1}{c|}{83.71}              & \multicolumn{1}{c|}{83.33}                 & \multicolumn{1}{c}{83.71}                 & \multicolumn{1}{c|}{\underline{85.61}} & \multicolumn{1}{c|}{83.33}                     & \multicolumn{1}{c}{83.71}             & \multicolumn{1}{c|}{84.09} & \multicolumn{1}{c|}{84.09}                     & \multicolumn{1}{c}{\textit{81.44}} & \multicolumn{1}{c|}{84.47} & \multicolumn{1}{c|}{83.71}                   & \multicolumn{1}{c}{83.33}             & \multicolumn{1}{c|}{84.09}             & \multicolumn{1}{c|}{\textbf{85.98}}             & \multicolumn{1}{c}{84.09}             & \multicolumn{1}{c}{83.71}             \\
         \multicolumn{1}{c}{}                         & $\beta$   & \multicolumn{1}{c|}{\underline{86.36}}       & \multicolumn{1}{c}{84.85}              & \multicolumn{1}{c|}{84.47}              & \multicolumn{1}{c|}{85.23}                 & \multicolumn{1}{c}{85.23}                 & \multicolumn{1}{c|}{85.23}             & \multicolumn{1}{c|}{\textbf{86.74}}            & \multicolumn{1}{c}{85.98}             & \multicolumn{1}{c|}{85.23} & \multicolumn{1}{c|}{84.47}                     & \multicolumn{1}{c}{\textit{83.33}} & \multicolumn{1}{c|}{84.09} & \multicolumn{1}{c|}{\textbf{86.74}}          & \multicolumn{1}{c}{85.98}             & \multicolumn{1}{c|}{83.71}             & \multicolumn{1}{c|}{\underline{86.36}}          & \multicolumn{1}{c}{84.09}             & \multicolumn{1}{c}{85.23}             \\ \hline
         \multicolumn{1}{c}{\multirow{2}{*}{PAWS-X}}  & $\alpha$  & \multicolumn{1}{c|}{80.63}                   & \multicolumn{1}{c}{81.60}              & \multicolumn{1}{c|}{81.49}              & \multicolumn{1}{c|}{\textit{79.92}}        & \multicolumn{1}{c}{80.63}                 & \multicolumn{1}{c|}{82.51}             & \multicolumn{1}{c|}{81.19}                     & \multicolumn{1}{c}{80.78}             & \multicolumn{1}{c|}{80.07} & \multicolumn{1}{c|}{80.43}                     & \multicolumn{1}{c}{80.02}          & \multicolumn{1}{c|}{80.68} & \multicolumn{1}{c|}{82.26}                   & \multicolumn{1}{c}{82.77}             & \multicolumn{1}{c|}{82.77}             & \multicolumn{1}{c|}{82.82}                      & \multicolumn{1}{c}{\underline{82.87}} & \multicolumn{1}{c}{\textbf{83.53}}    \\
         \multicolumn{1}{c}{}                         & $\beta$   & \multicolumn{1}{g|}{87.49}                   & \multicolumn{1}{g}{\underline{88.05}}  & \multicolumn{1}{g|}{\textbf{88.21}}     & \multicolumn{1}{g|}{87.75}                 & \multicolumn{1}{g}{87.24}                 & \multicolumn{1}{g|}{88.00}             & \multicolumn{1}{g|}{86.83}                     & \multicolumn{1}{g}{87.39}             & \multicolumn{1}{g|}{87.09} & \multicolumn{1}{g|}{87.75}                     & \multicolumn{1}{g}{\textit{86.58}} & \multicolumn{1}{g|}{86.68} & \multicolumn{1}{g|}{87.60}                   & \multicolumn{1}{g}{87.60}             & \multicolumn{1}{g|}{87.90}             & \multicolumn{1}{g|}{88.00}                      & \multicolumn{1}{g}{88.00}             & \multicolumn{1}{g}{\underline{88.05}} \\ \cline{3-20}
         \multicolumn{1}{c}{\multirow{2}{*}{XNLI}}    & $\alpha$  & \multicolumn{1}{c|}{67.34}                   & \multicolumn{1}{c}{\textbf{67.52}}     & \multicolumn{1}{c|}{66.64}              & \multicolumn{1}{c|}{66.94}                 & \multicolumn{1}{c}{66.94}                 & \multicolumn{1}{c|}{\textit{66.26}}    & \multicolumn{1}{c|}{67.20}                     & \multicolumn{1}{c}{\underline{67.42}} & \multicolumn{1}{c|}{67.34} & \multicolumn{1}{c|}{66.38}                     & \multicolumn{1}{c}{67.08}          & \multicolumn{1}{c|}{66.92} & \multicolumn{1}{c|}{66.68}                   & \multicolumn{1}{c}{66.60}             & \multicolumn{1}{c|}{67.14}             & \multicolumn{1}{c|}{66.42}                      & \multicolumn{1}{c}{66.54}             & \multicolumn{1}{c}{66.26}             \\
         \multicolumn{1}{c}{}                         & $\beta$   & \multicolumn{1}{c|}{68.09}                   & \multicolumn{1}{c}{68.18}              & \multicolumn{1}{c|}{66.84}              & \multicolumn{1}{c|}{67.82}                 & \multicolumn{1}{c}{67.82}                 & \multicolumn{1}{c|}{\underline{68.36}} & \multicolumn{1}{c|}{66.31}                     & \multicolumn{1}{c}{65.60}             & \multicolumn{1}{c|}{66.40} & \multicolumn{1}{c|}{\textit{64.98}}            & \multicolumn{1}{c}{65.51}          & \multicolumn{1}{c|}{65.07} & \multicolumn{1}{c|}{66.84}                   & \multicolumn{1}{c}{67.82}             & \multicolumn{1}{c|}{67.02}             & \multicolumn{1}{c|}{67.64}                      & \multicolumn{1}{c}{66.31}             & \multicolumn{1}{c}{\textbf{68.53}}    \\ \hline\hline
         \multicolumn{1}{c}{Scores}                   &           & \multicolumn{1}{c|}{\underline{II}}          & \multicolumn{2}{c|}{\textbf{II} \underline{II}}                                  & \multicolumn{1}{c|}{}                      & \multicolumn{2}{c|}{\underline{II}}                                                & \multicolumn{1}{c|}{\textbf{I} \underline{I}}  & \multicolumn{2}{c|}{\textbf{I} \underline{I}}                      & \multicolumn{1}{c|}{\underline{I}}             & \multicolumn{2}{c|}{}                                           & \multicolumn{1}{c|}{\textbf{I}}              & \multicolumn{2}{c|}{\underline{II}}                                            & \multicolumn{1}{c|}{\textbf{II} \underline{I}}  & \multicolumn{2}{c}{\textbf{II} \underline{III}}                               \\ \cline{1-2}
         \multicolumn{1}{c}{+SRL}                     & \multicolumn{3}{l}{\textbf{5} \underline{10}} \\
         \multicolumn{1}{c}{$-$SRL}                   & \multicolumn{3}{l}{\textbf{4} \underline{5}}
      \end{tabular}
    }
  }{Results}

  \tab{tab:pawsx-beta}{The ``expanded'' PAWS-X $\beta$ results. The light gray line corresponds to the one in table \ref{tab:results}.
                       As can be seen, the fluctuations between single models are not too big, indicating that the architecture
                       produces fairly stable results. Ensembeling reliably leads to a 1.26 percentage points gain on average, with a standard deviation of 0.26\%.}{
    \scalebox{0.9}{
      \begin{tabular}{lccc|ccc|ccc|ccc|ccc|ccc}
                 & \multicolumn{18}{c}{\large \textbf{PAWS-X $\beta$}}  \\ \\
                 & \multicolumn{6}{c|}{\textbf{{[}CLS{]} Head}}                                                                                                                                                                                                                      & \multicolumn{6}{c|}{\textbf{FFNN Head}}                                                                                                                                                                                                         & \multicolumn{6}{c}{\textbf{GRU Head}}                                                                                                                                                                                                                           \\ \cline{2-19}
                 & \multicolumn{3}{c|}{subtokenized}                                                                                               & \multicolumn{3}{c|}{subtokens merged}                                                                                           & \multicolumn{3}{c|}{subtokenized}                                                                                            & \multicolumn{3}{c|}{subtokens merged}                                                                            & \multicolumn{3}{c|}{subtokenized}                                                                                             & \multicolumn{3}{c}{subtokens merged}                                                                                            \\ \cline{2-19}
                 & \multicolumn{1}{c|}{$-$SRL}                  & \multicolumn{2}{c|}{+SRL}                                                        & \multicolumn{1}{c|}{$-$SRL}                & \multicolumn{2}{c|}{+SRL}                                                          & \multicolumn{1}{c|}{$-$SRL}                    & \multicolumn{2}{c|}{+SRL}                                                   & \multicolumn{1}{c|}{$-$SRL}                    & \multicolumn{2}{c|}{+SRL}                                       & \multicolumn{1}{c|}{$-$SRL}                  & \multicolumn{2}{c|}{+SRL}                                                      & \multicolumn{1}{c|}{$-$SRL}                     & \multicolumn{2}{c}{+SRL}                                                      \\
                 & \multicolumn{1}{c|}{}                        & \multicolumn{1}{c}{zeros}              & \multicolumn{1}{c|}{dupl.}              & \multicolumn{1}{c|}{}                      & \multicolumn{1}{c}{zeros}                 & \multicolumn{1}{c|}{dupl.}             & \multicolumn{1}{c|}{}                          & \multicolumn{1}{c}{zeros}             & \multicolumn{1}{c|}{dupl.}          & \multicolumn{1}{c|}{}                          & \multicolumn{1}{c}{zeros}          & dupl.                      & \multicolumn{1}{c|}{}                        & \multicolumn{1}{c}{zeros}             & dupl.                                  & \multicolumn{1}{c|}{}                           & \multicolumn{1}{c}{zeros}             & dupl.                                 \\ \hline\hline
        Model 1  & \multicolumn{1}{c|}{85.41}                   & \multicolumn{1}{c}{85.36}              & \multicolumn{1}{c|}{86.02}              & \multicolumn{1}{c|}{86.43}                 & \multicolumn{1}{c}{86.53}                 & \multicolumn{1}{c|}{87.04}             & \multicolumn{1}{c|}{85.77}                     & \multicolumn{1}{c}{85.61}             & \multicolumn{1}{c|}{85.77}          & \multicolumn{1}{c|}{86.22}                     & \multicolumn{1}{c}{84.70}          & \multicolumn{1}{c|}{86.53} & \multicolumn{1}{c|}{87.54}                   & \multicolumn{1}{c}{87.49}             & \multicolumn{1}{c|}{86.43}             & \multicolumn{1}{c|}{85.51}                      & \multicolumn{1}{c}{86.93}             & \multicolumn{1}{c}{86.53}             \\
        Model 2  & \multicolumn{1}{c|}{86.07}                   & \multicolumn{1}{c}{86.83}              & \multicolumn{1}{c|}{86.99}              & \multicolumn{1}{c|}{85.87}                 & \multicolumn{1}{c}{86.32}                 & \multicolumn{1}{c|}{86.68}             & \multicolumn{1}{c|}{86.17}                     & \multicolumn{1}{c}{85.82}             & \multicolumn{1}{c|}{87.39}          & \multicolumn{1}{c|}{85.92}                     & \multicolumn{1}{c}{85.36}          & \multicolumn{1}{c|}{85.26} & \multicolumn{1}{c|}{87.04}                   & \multicolumn{1}{c}{86.99}             & \multicolumn{1}{c|}{85.87}             & \multicolumn{1}{c|}{87.19}                      & \multicolumn{1}{c}{86.07}             & \multicolumn{1}{c}{87.44}             \\
        Model 3  & \multicolumn{1}{c|}{86.07}                   & \multicolumn{1}{c}{87.49}              & \multicolumn{1}{c|}{86.99}              & \multicolumn{1}{c|}{87.14}                 & \multicolumn{1}{c}{85.26}                 & \multicolumn{1}{c|}{86.73}             & \multicolumn{1}{c|}{86.22}                     & \multicolumn{1}{c}{84.90}             & \multicolumn{1}{c|}{85.36}          & \multicolumn{1}{c|}{85.41}                     & \multicolumn{1}{c}{85.31}          & \multicolumn{1}{c|}{85.71} & \multicolumn{1}{c|}{86.12}                   & \multicolumn{1}{c}{85.66}             & \multicolumn{1}{c|}{86.83}             & \multicolumn{1}{c|}{86.48}                      & \multicolumn{1}{c}{87.09}             & \multicolumn{1}{c}{86.93}             \\
        Model 4  & \multicolumn{1}{c|}{87.39}                   & \multicolumn{1}{c}{86.32}              & \multicolumn{1}{c|}{86.58}              & \multicolumn{1}{c|}{86.38}                 & \multicolumn{1}{c}{84.04}                 & \multicolumn{1}{c|}{87.65}             & \multicolumn{1}{c|}{86.53}                     & \multicolumn{1}{c}{86.73}             & \multicolumn{1}{c|}{85.61}          & \multicolumn{1}{c|}{85.82}                     & \multicolumn{1}{c}{85.61}          & \multicolumn{1}{c|}{86.38} & \multicolumn{1}{c|}{84.99}                   & \multicolumn{1}{c}{86.02}             & \multicolumn{1}{c|}{87.70}             & \multicolumn{1}{c|}{86.99}                      & \multicolumn{1}{c}{86.68}             & \multicolumn{1}{c}{86.88}             \\
        Model 5  & \multicolumn{1}{c|}{86.63}                   & \multicolumn{1}{c}{86.43}              & \multicolumn{1}{c|}{86.58}              & \multicolumn{1}{c|}{86.99}                 & \multicolumn{1}{c}{85.26}                 & \multicolumn{1}{c|}{85.56}             & \multicolumn{1}{c|}{86.18}                     & \multicolumn{1}{c}{87.09}             & \multicolumn{1}{c|}{84.75}          & \multicolumn{1}{c|}{87.04}                     & \multicolumn{1}{c}{85.77}          & \multicolumn{1}{c|}{85.82} & \multicolumn{1}{c|}{86.12}                   & \multicolumn{1}{c}{85.82}             & \multicolumn{1}{c|}{86.73}             & \multicolumn{1}{c|}{87.34}                      & \multicolumn{1}{c}{86.73}             & \multicolumn{1}{c}{86.58}             \\ \hline\hline %\hhline{~==================}
        Average  & \multicolumn{1}{c|}{86.31}                   & \multicolumn{1}{c}{86.49}              & \multicolumn{1}{c|}{86.63}              & \multicolumn{1}{c|}{86.56}                 & \multicolumn{1}{c}{85.48}                 & \multicolumn{1}{c|}{\underline{86.81}} & \multicolumn{1}{c|}{86.22}                     & \multicolumn{1}{c}{86.03}             & \multicolumn{1}{c|}{\textit{85.78}} & \multicolumn{1}{c|}{86.08}                     & \multicolumn{1}{c}{85.35}          & \multicolumn{1}{c|}{85.94} & \multicolumn{1}{c|}{86.35}                   & \multicolumn{1}{c}{86.40}             & \multicolumn{1}{c|}{86.71}             & \multicolumn{1}{c|}{86.70}                      & \multicolumn{1}{c}{86.70}             & \multicolumn{1}{c}{\textbf{86.87}}    \\
        Ensemble & \multicolumn{1}{g|}{87.49}                   & \multicolumn{1}{g}{\underline{88.05}}  & \multicolumn{1}{g|}{\textbf{88.21}}     & \multicolumn{1}{g|}{87.75}                 & \multicolumn{1}{g}{87.24}                 & \multicolumn{1}{g|}{88.00}             & \multicolumn{1}{g|}{86.83}                     & \multicolumn{1}{g}{87.39}             & \multicolumn{1}{g|}{87.09}          & \multicolumn{1}{g|}{87.75}                     & \multicolumn{1}{g}{\textit{86.58}} & \multicolumn{1}{g|}{86.68} & \multicolumn{1}{g|}{87.60}                   & \multicolumn{1}{g}{87.60}             & \multicolumn{1}{g|}{87.90}             & \multicolumn{1}{g|}{88.00}                      & \multicolumn{1}{g}{88.00}             & \multicolumn{1}{g}{\underline{88.05}} \\ \cline{1-19}
        Gain     & \multicolumn{1}{c|}{1.18}                    & \multicolumn{1}{c}{1.56}               & \multicolumn{1}{c|}{1.58}               & \multicolumn{1}{c|}{1.19}                  & \multicolumn{1}{c}{1.76}                  & \multicolumn{1}{c|}{1.19}              & \multicolumn{1}{c|}{.65}                       & \multicolumn{1}{c}{1.36}              & \multicolumn{1}{c|}{1.31}           & \multicolumn{1}{c|}{1.67}                      & \multicolumn{1}{c}{1.23}           & \multicolumn{1}{c|}{.74}   & \multicolumn{1}{c|}{1.25}                    & \multicolumn{1}{c}{1.20}              & \multicolumn{1}{c|}{1.19}              & \multicolumn{1}{c|}{1.30}                       & \multicolumn{1}{c}{1.30}              & \multicolumn{1}{c}{1.18}              \\ \cline{1-1}
        Average  & 1.26 ($\sigma$ 0.28)
      \end{tabular}
    }
  }{Gains Ensemble vs Average}
  \vfill
\end{landscape}

To determine whether there is a tendency as to which SRL-BERT aligning strategy turns out
to be more effective, the results of the main table \ref{tab:results} are aggregated in table
\ref{tab:token-vs-merged}: For each head, the +SRL results are compared, and the differences
are reported in the table. For example, we look at the [CLS] head on deISEAR $\alpha$ and ask
us, which strategy --- merging the BERT subtokens back to ``normal'' tokens, or splitting the
SRLs up to align with the subtokenized BERT tokens --- worked better. For this, the difference
between both for +SRL zeros and +SRL duplicate is calculated and controlled for statistical
significance between both ensembles. The subtokenizing strategy (yellow) outperformed the
merging strategy (purple) 28 times, 15 times statistically significant, while merging was
better 20 times, only 15\% of these were statistically significant. Therefore, at least for
the GerGLUE classification datasets, splitting the SRLs up to align with the BERT subtokens
seems more effective than merging the BERT subtokens. This also makes sense intuitively ---
using the merging strategy of averaging the BERT embeddings of subtokens leads clearly to an
information loss or distortion that cannot be fully balanced by the added SRL information.


\tab{tab:token-vs-merged}{Performance of architectures when BERT \customcolorbox{subtokenized}{yellow} vs. \customcolorbox{merged}{purple}. Both SRL implementations were
                          compared pairwise. Clearly, the subtokenized ensembles showed better classification performances
                          than the merged ones.

                          Example case, upper left 4.64**: Comparison of deISEAR $\alpha$, +SRL zero
                          implementation [CLS] Head from table \ref{tab:results}: Subtokenized ensemble (72.19\% accuracy) and the merged ensemble (67.55\%
                          accuracy) --- the subtokenized ensemble performed 4.64\% better, apparently with quite high significance (p $<$ 5\%).}{
  \scalebox{0.90}{
    \begin{tabular}{llcc|cc|cc}
                                                 &           & \multicolumn{2}{c|}{\textbf{{[}CLS{]} Head}}                                                     & \multicolumn{2}{c|}{\textbf{FFNN Head}}                                                            & \multicolumn{2}{c}{\textbf{GRU Head}}                                                           \\ \cline{3-8}
                                                 &           & \multicolumn{1}{c|}{zeros}                      & \multicolumn{1}{c|}{dupl.}                     & \multicolumn{1}{c|}{zeros}                        & \multicolumn{1}{c|}{dupl.}                     & \multicolumn{1}{c|}{zeros}                    & dupl.                                           \\ \hline\hline
    \multicolumn{1}{c}{\multirow{2}{*}{deISEAR}} & $\alpha$  & \multicolumn{1}{c|}{\cellcolor{yellow} 4.64**}  & \multicolumn{1}{c|}{\cellcolor{purple} .67}    & \multicolumn{1}{c|}{\cellcolor{yellow} 4.63*}     & \multicolumn{1}{c|}{\cellcolor{purple} 1.32}   & \multicolumn{1}{c|}{\cellcolor{yellow} 4.63*} & \multicolumn{1}{c}{\cellcolor{yellow} 2.65}     \\
    \multicolumn{1}{c}{}                         & $\beta$   & \multicolumn{1}{c|}{\cellcolor{yellow} 3.97*}   & \multicolumn{1}{c|}{\cellcolor{yellow} 1.98}   & \multicolumn{1}{c|}{\cellcolor{purple} 5.30*}     & \multicolumn{1}{c|}{\cellcolor{purple} .66}    & \multicolumn{1}{c|}{\cellcolor{yellow} 4.63*} & \multicolumn{1}{c}{\cellcolor{purple} 2.64}     \\ \cline{3-8}
    \multicolumn{1}{c}{\multirow{2}{*}{SCARE}}   & $\alpha$  & \multicolumn{1}{c|}{\cellcolor{purple} 1.13}    & \multicolumn{1}{c|}{\cellcolor{purple} 1.90}   & \multicolumn{1}{c|}{\cellcolor{yellow} 2.27*}     & \multicolumn{1}{c|}{\cellcolor{purple} .38}    & \multicolumn{1}{c|}{\cellcolor{purple} .76}   & \multicolumn{1}{c}{\cellcolor{yellow} .38}      \\
    \multicolumn{1}{c}{}                         & $\beta$   & \multicolumn{1}{c|}{\cellcolor{purple} .38}     & \multicolumn{1}{c|}{\cellcolor{purple} .76}    & \multicolumn{1}{c|}{\cellcolor{yellow} 2.65**}    & \multicolumn{1}{c|}{\cellcolor{yellow} 1.14}   & \multicolumn{1}{c|}{\cellcolor{yellow} 1.89}  & \multicolumn{1}{c}{\cellcolor{purple} 1.52}     \\ \hline
    \multicolumn{1}{c}{\multirow{2}{*}{PAWS-X}}  & $\alpha$  & \multicolumn{1}{c|}{\cellcolor{yellow} .97*}    & \multicolumn{1}{c|}{\cellcolor{purple} 1.02**} & \multicolumn{1}{c|}{\cellcolor{yellow} .76}       & \multicolumn{1}{c|}{\cellcolor{purple} .61}    & \multicolumn{1}{c|}{\cellcolor{purple} .10}   & \multicolumn{1}{c}{\cellcolor{purple} .76}      \\
    \multicolumn{1}{c}{}                         & $\beta$   & \multicolumn{1}{c|}{\cellcolor{yellow} .81**}   & \multicolumn{1}{c|}{\cellcolor{yellow} .21}    & \multicolumn{1}{c|}{\cellcolor{yellow} .81*}      & \multicolumn{1}{c|}{\cellcolor{yellow} .41}    & \multicolumn{1}{c|}{\cellcolor{purple} .40}   & \multicolumn{1}{c}{\cellcolor{purple} .15}      \\ \cline{3-8}
    \multicolumn{1}{c}{\multirow{2}{*}{XNLI}}    & $\alpha$  & \multicolumn{1}{c|}{\cellcolor{yellow} .58**}   & \multicolumn{1}{c|}{\cellcolor{yellow} .38}    & \multicolumn{1}{c|}{\cellcolor{yellow} .34}       & \multicolumn{1}{c|}{\cellcolor{yellow} .42}    & \multicolumn{1}{c|}{\cellcolor{yellow} .06}   & \multicolumn{1}{c}{\cellcolor{yellow} .88**}    \\
    \multicolumn{1}{c}{}                         & $\beta$   & \multicolumn{1}{c|}{\cellcolor{yellow} .36}     & \multicolumn{1}{c|}{\cellcolor{purple} 1.52*}  & \multicolumn{1}{c|}{\cellcolor{yellow} .09}       & \multicolumn{1}{c|}{\cellcolor{yellow} 1.33*}  & \multicolumn{1}{c|}{\cellcolor{yellow} 1.51*} & \multicolumn{1}{c}{\cellcolor{purple} 1.51*}    \\
    \end{tabular}
  }
}{Tokenized vs. Merged wo QA}

% \tab{tab:configs}{The different hyperparameter configurations for each dataset.}{
%   \scalebox{0.9}{
%     \begin{tabular}{ll|ccccc}
%                                                    &             & \# of epochs                             & split set up                                & batch size                              & maximum length                 & SRL implementation                          \\ \hline
%     \multicolumn{1}{c}{\multirow{2}{*}{deISEAR}}   &  $\alpha$   & \multicolumn{1}{c}{\multirow{2}{*}{100}} & \multicolumn{1}{c}{\multirow{2}{*}{normal}} & \multicolumn{1}{c}{\multirow{2}{*}{16}} & 40                             & \multicolumn{1}{c}{\multirow{2}{*}{normal}} \\
%     \multicolumn{1}{c}{}                           &  $\beta$    &                                          &                                             &                                         & 200                            &                                             \\
%     \multicolumn{1}{c}{\multirow{2}{*}{SCARE}}     &  $\alpha$   & \multicolumn{1}{c}{\multirow{2}{*}{50}}  & \multicolumn{1}{c}{\multirow{2}{*}{normal}} & \multicolumn{1}{c}{\multirow{2}{*}{16}} & 50                             & \multicolumn{1}{c}{\multirow{2}{*}{normal}} \\
%     \multicolumn{1}{c}{}                           &  $\beta$    &                                          &                                             &                                         & 100                            &                                             \\ \hline
%     \multicolumn{1}{c}{\multirow{2}{*}{PAWS-X}}    &  $\alpha$   & 20                                       & normal                                      & 16                                      & 16                             & normal                                      \\
%     \multicolumn{1}{c}{}                           &  $\beta$    & 20                                       & normal                                      & 16                                      & 16                             & normal                                      \\
%     \multicolumn{1}{c}{\multirow{2}{*}{XNLI}}      &  $\alpha$   & 50                                       & normal                                      & 16                                      & 100                            & normal                                      \\
%     \multicolumn{1}{c}{}                           &  $\beta$    & 50                                       & re-split                                    & 16                                      & 100                            & normal                                      \\ \hline
%     \multicolumn{1}{c}{\multirow{2}{*}{MLQA}}      &  $\alpha$   & \colorbox{blue}{40}                      & \colorbox{blue}{200}                        & \colorbox{blue}{50}                     & \colorbox{blue}{100}           & normal                                      \\
%     \multicolumn{1}{c}{}                           &  $\beta$    & \colorbox{blue}{40}                      & \colorbox{blue}{200}                        & \colorbox{blue}{50}                     & \colorbox{blue}{100}           & normal                                      \\
%     \multicolumn{1}{c}{\multirow{2}{*}{XQuAD}}     &  $\alpha$   & \colorbox{blue}{40}                      & \colorbox{blue}{200}                        & \colorbox{blue}{50}                     & \colorbox{blue}{100}           & 100                                         \\
%     \multicolumn{1}{c}{}                           &  $\beta$    & \colorbox{blue}{40}                      & \colorbox{blue}{200}                        & \colorbox{blue}{50}                     & \colorbox{blue}{100}           & 100                                         \\
%     \end{tabular}
%   }
% }{Dataset specific Configs}

Another accumulating view on the main table \ref{tab:results} is constructed by not just
looking for the best results of a whole row, i.e. over all heads, merging strategies,
and SRL implementations, but instead by recording the superiority or inferiority of +SRL
compared to $-$SRL for each pairing: Take for example the deISEAR $\alpha$ subtokenized
[CLS] head setting: Without SRLs, the ensemble performance was 71.52\%; now the better
of the two +SRL implementations of this setting is taken into account, in this case
zeroing apparently was better. The difference, 0.67\% is tested for significance and
reported in the table. The coloring indicates which SRL implementation performed better
and was taken into account; sometimes both implementations performed equally, which is
also reflected in coloring. Negative numbers indicate, of course, that the head produced
better results when SRL information was \emph{not} added (and this was also controlled
for significance).
Looking at these accumulated results (table \ref{tab:gain-loss}), no clear picture emerges
at first glance; sometimes zeroing SRLs performed better than duplicating SRLs, sometimes not.
Sometimes +SRL clearly outperforms $-$SRL, even highly statistically significant, sometimes it is
the other way around and adding SRLs seems to create a disadvantage for the head.

\begin{figure}
  \begin{minipage}{0.45\linewidth}
  \vspace{0pt}
    \includegraphics[width=0.9\linewidth]{images/heatmap_subtokenized_eq.pdf}
  \end{minipage}
  \hfill
  \begin{minipage}{0.45\linewidth}
  \vspace{0pt}
    \includegraphics[width=0.9\linewidth]{images/heatmap_merged_eq.pdf}
  \end{minipage}
  \stepcounter{myfigure}
  \caption[Heatmaps subtokenizd and merged]{Gain/loss heat maps from the gain/loss values in table \ref{tab:gain-loss}. \textbf{Left}: Subtokenized. \textbf{Right}: Merged.}
  \label{fig:heatmap-gains}
\end{figure}

In figure \ref{fig:heatmap-gains} the absolute gains and losses are plotted in two heat maps:
In the heat map for the subtokenized settings, the pattern seems to be that for the single
sentence classification tasks there is more fluctuation; especially for deISEAR FFNN, where
for the $\alpha$ setting, there was the biggest gain in all subtokenized experiments, but
also the biggest loss in all subtokenized settings for deISEAR $\beta$. The SCARE [CLS] head
seems also clearly to suffer from subtokenizing SRLs tokens, while gining in other settings.
However, for the sentence pair tasks, subtokenizing mostly adds a smaller, but constant gain,
visualized by the uniformly distributed purple shades in this dataset rows.

For the merged settings, the correlation axis appears slightly to change: Before, positive or
negative influence of the BERT-SRL combining technique has shown some correlation with datasets
(tendency to lead to slight gains in sentence pair tasks, and irregular behavior on single
sentence tasks), now while this still holds to some degree, it seems that especially the FFNN and GRU
head are negatively affected by merging SRLs, while the [CLS] head shows a trend towards
gaining from it. Note also, that the subtokenized settings shows a greater range of gains and
losses (from $-$4.63 to + 6.62) compared to the merging approach ($-$1.99 to +2.59).

To add one more abstraction layer and see a more complete picture, this table is again
summarized in figure \ref{fig:classification-gains}. Doing this, the image
clarifies, allowing to stipulate that adding SRLs on GerGLUE classification tasks
leads to a measurable improvement. The total gains clearly outweigh the losses, and also
more statistically significant. Regarding the SRL implementation, there is no clear advantage
of one method over the other, only a slight tendency of duplicating being more effective
than zeroing.


\tab{tab:gain-loss}{Ensemble percentage points gains (positive numbers) / losses (negative numbers) for +SRL
                      over $-$SRL for each configuration from table \ref{tab:results}. The better of the +SRL configurations was taken into account:
                      \customcolorbox{zeros}{blue},
                      \customcolorbox{duplicate}{dark-blue}.
                      Light blue denotes that both architectures performed
                      \customcolorbox{equally}{llight-blue}
                      (in which case both ensembles were controlled for significance). One asterisk signifies
                      a $p$-value $<$ 10\%, two stand for $p <$ 5\% and three for $p <$ 1\%.

                      Example case upper left 0.67\%: The better +SRL ensemble for deISEAR $\alpha$ [CLS] head
                      was the zero implementation (72.19\%), surpassing the $-$SRL ensemble (71.52\%) by 0.67
                      percentage point; but apparently not statistically significant.}{
    \scalebox{1}{
      \begin{tabular}{llP{1.5cm}P{1.5cm}P{1.5cm}P{1.5cm}P{1.5cm}P{1.5cm}}
                                                   &           & \multicolumn{2}{c|}{\textbf{{[}CLS{]} Head}}                                                                 & \multicolumn{2}{c|}{\textbf{FFNN Head}}                                                                        & \multicolumn{2}{c}{\textbf{GRU Head}}                                                                     \\ \cline{3-8}
                                                   &           & \multicolumn{1}{c|}{subtok.}                          & \multicolumn{1}{c|}{merged}                          & \multicolumn{1}{c|}{subtok.}                          & \multicolumn{1}{c|}{merged}                            & \multicolumn{1}{c|}{subtok.}                        & \multicolumn{1}{c}{merged}                          \\ \hline\hline
      \multicolumn{1}{c}{\multirow{2}{*}{deISEAR}} & $\alpha$  & \multicolumn{1}{c|}{\cellcolor{blue} .67}             & \multicolumn{1}{c|}{\cellcolor{dark-blue} .00}       & \multicolumn{1}{c|}{\cellcolor{blue} 6.62**}          & \multicolumn{1}{c|}{\cellcolor{dark-blue} .00}         & \multicolumn{1}{c|}{\cellcolor{blue} 4.63**}        & \multicolumn{1}{c}{\cellcolor{dark-blue} $-1.99$}   \\
      \multicolumn{1}{c}{}                         & $\beta$   & \multicolumn{1}{c|}{\cellcolor{llight-blue} 3.31}     & \multicolumn{1}{c|}{\cellcolor{dark-blue} 1.99}      & \multicolumn{1}{c|}{\cellcolor{dark-blue} $-4.63$*}   & \multicolumn{1}{c|}{\cellcolor{blue} $-1.32$}          & \multicolumn{1}{c|}{\cellcolor{blue} 1.32}          & \multicolumn{1}{c}{\cellcolor{dark-blue} $-1.99$}   \\ \cline{3-8}
      \multicolumn{1}{c}{\multirow{2}{*}{SCARE}}   & $\alpha$  & \multicolumn{1}{c|}{\cellcolor{dark-blue} $-1.90$*}   & \multicolumn{1}{c|}{\cellcolor{dark-blue} 2.28*}     & \multicolumn{1}{c|}{\cellcolor{dark-blue} .76}        & \multicolumn{1}{c|}{\cellcolor{dark-blue} .38}         & \multicolumn{1}{c|}{\cellcolor{dark-blue} .38}      & \multicolumn{1}{c}{\cellcolor{blue} $-1.89$}        \\
      \multicolumn{1}{c}{}                         & $\beta$   & \multicolumn{1}{c|}{\cellcolor{blue} $-1.51$**}       & \multicolumn{1}{c|}{\cellcolor{llight-blue} .00}     & \multicolumn{1}{c|}{\cellcolor{blue} $-.76$}          & \multicolumn{1}{c|}{\cellcolor{dark-blue} $-.38$}      & \multicolumn{1}{c|}{\cellcolor{blue} .76}           & \multicolumn{1}{c}{\cellcolor{dark-blue} $-1.13$}   \\ \hline
      \multicolumn{1}{c}{\multirow{2}{*}{PAWS-X}}  & $\alpha$  & \multicolumn{1}{c|}{\cellcolor{blue} .97*}            & \multicolumn{1}{c|}{\cellcolor{dark-blue} 2.59***}   & \multicolumn{1}{c|}{\cellcolor{blue} $-.41$}          & \multicolumn{1}{c|}{\cellcolor{dark-blue} .25}         & \multicolumn{1}{c|}{\cellcolor{llight-blue} .51}     & \multicolumn{1}{c}{\cellcolor{dark-blue} .71}       \\
      \multicolumn{1}{c}{}                         & $\beta$   & \multicolumn{1}{c|}{\cellcolor{dark-blue} .72*}       & \multicolumn{1}{c|}{\cellcolor{dark-blue} .25}       & \multicolumn{1}{c|}{\cellcolor{blue} .56}             & \multicolumn{1}{c|}{\cellcolor{dark-blue} $-1.07$***}  & \multicolumn{1}{c|}{\cellcolor{dark-blue} .30}      & \multicolumn{1}{c}{\cellcolor{dark-blue} .05}       \\ \cline{3-8}
      \multicolumn{1}{c}{\multirow{2}{*}{XNLI}}    & $\alpha$  & \multicolumn{1}{c|}{\cellcolor{blue} .18}             & \multicolumn{1}{c|}{\cellcolor{blue} $-.22$}         & \multicolumn{1}{c|}{\cellcolor{blue} .22}             & \multicolumn{1}{c|}{\cellcolor{blue} .70*}             & \multicolumn{1}{c|}{\cellcolor{dark-blue} .46}      & \multicolumn{1}{c}{\cellcolor{dark-blue} .12}       \\
      \multicolumn{1}{c}{}                         & $\beta$   & \multicolumn{1}{c|}{\cellcolor{blue} .09}             & \multicolumn{1}{c|}{\cellcolor{dark-blue} .44}       & \multicolumn{1}{c|}{\cellcolor{dark-blue} .09}        & \multicolumn{1}{c|}{\cellcolor{blue} .53}              & \multicolumn{1}{c|}{\cellcolor{blue} .98}           & \multicolumn{1}{c}{\cellcolor{dark-blue} .89}       \\ % \hline\hline
      % & \multicolumn{2}{l}{Accumulation}                  \\ \cline{2-4}
      % & \multicolumn{2}{l}{\textbf{Gains} (significant)}  & 32 (5) \\
      % & \multicolumn{2}{l}{\textbf{Losses} (significant)} & 15 (3) \\
     \end{tabular}
    }
  }{Gain-Loss}



\begin{figure}
  \begin{minipage}{0.45\linewidth}
  \vspace{0pt}
    \includegraphics[width=0.9\linewidth]{images/gain_losses_accumulated.pdf}
  \end{minipage}
  \hfill
  \begin{minipage}{0.45\linewidth}
  \vspace{0pt}
    \includegraphics[width=0.9\linewidth]{images/gain_losses_SRL.pdf}
  \end{minipage}
  \stepcounter{myfigure}
  \caption[Accumulated Gains and Losses.]{Accumulated scores from table \ref{tab:gain-loss}. \textbf{Left}: Which SRL mode performed stronger out of a total of 48 settings. The bars indicate a slight outperforming of duplicating SRLs instead of adding zeros. \textbf{Right}: Counts for Gains and Losses in all 48 settings. Darker shades indicate significant results. The light green tip stands for settings where the gain equals 0.00.}
  \label{fig:classification-gains}
\end{figure}

As described in chapter \ref{chap:3_datasets}, the datasets compiled in GerGLUE are a
heterogeneous assembly: Some datasets like deISEAR and, especially, SCARE comprise rather
informal, colloquial texts, while PAWS-X and XNLI consists of more technical, standardized language.
Further, as depicted in figure \ref{fig:num-predarg-structs}, over 30\% of the sentences
in SCARE don't have any predicate-argument structures --- in other words there is no exploitable SRL
information for the model.

In figure \ref{fig:each-dataset-gains}, the same accumulation process as for the general overview
% in figure \ref{fig:classification-gains} is conducted for each dataset. The following observations
can be made dataset-wise: For each double row --- the $\alpha$ and $\beta$ runs ---, the gains and
losses are added together, and the significant ones are highlighted.

The positive effect of enriching BERT embeddings with SRLs is especially evident for
the sentence pair classification tasks: PAWS-X has the highest number of experiments where a significant
improvement over vanilla BERT embeddings could be measured, and has overall clearly gained from adding
SRLs. While XNLI has only one significant improved experiment, the overall number of experiments
where a positive effect was measured outweighs the negative one by far.

For the single sentence tasks, however, the picture somewhat darkens. While there were some
significant improvements for deISEAR, there were also quite some experiments where adding SRLs
seems to have weakened the model. Generally, deISEAR is the dataset that shows the strongest
accuracy fluctuations ranging from 67.55 to 77.48\% --- a range of staggering 9.93 percentage
points. It is not clear to me why especially deISEAR shows this behavior, but the relative
small amount of examples in the dataset could indicate that it is more prone to unlucky random
initializations which could be countered by more examples for the models to extrapolate from.
For SCARE, there is no clear positive effect measurable at all --- in contrary, the significant
deteriorations outnumber the significant gains. However, this is not too surprising taking into
account the afore mentioned fact of predicate scarceness in SCARE. Further, the register is
highly colloquial with very frequent deviations from standard orthography, elliptic structures,
etc. which all probably leads to a collapsing of the already not too stable DAMESRL system.


\fig{images/all_datasets_gain_dup.pdf}{fig:each-dataset-gains}{Accumulation of statistics for each classification
                                       dataset \textbf{Left}: Which SRL architecture performed better.
                                       \textbf{Right}: Comparison of accuray points gained/lost
                                        after adding SRLs. Darker Colors indicate significant runs; light green stands for a neutral run,
                                        i.e. where adding SRLs information performed equally to the vanilla architecture.}{15}{Results
                                        Accumulation for each Dataset}

In figure \ref{fig:each-head-gains} the accumulation of gains and losses is done head-wise,; i.e.
for both BERT-SRL combination settings --- subtokenized and merged ---, the gains and losses
are added together and the significant ones are indicated. The [CLS] head amasses the most
significant gains, while at the same time having led to two significant losses.
Clearly, the FFNN head shows the weakest performance of the tested heads; the overall losses are the heighest
of all the heads and the significant gains and losses are on par. The GRU head accumulated the
most gains but with only one of them being significant.

\fig{images/all_heads_gain_dup.pdf}{fig:each-head-gains}{Accumulation of statistics for each head. \textbf{Left}: Which SRL architecture performed better.
                                       \textbf{Right}: Comparison of accuray points gained/lost after adding SRLs. Darker Colors indicate significant runs; light green stands for a neutral run, i.e. where adding SRLs information performed equally compared to the vanilla architecture}{15}{Results Accumulation for each Dataset}

Still, the sheer number of experiments where adding SRL structure compared to the four experiments
where it had a negative impact makes the GRU head the most realible extracting task-supporting
features out of SRL-enriched BERT embeddings.

\section{Question Answering Dataset Results}
\label{sec:qa-results}

The reportings of the results for the question answering datasets follows the line that was taken
for the classification sets: A general table with the ensemble test set results is reported in
\ref{tab:results-qa}. Because there is only one head for the question answering task, the table is
much smaller than for the classification task; however, the same facts are highlighted: The best,
second best, and worst ensemble results in one hyperparameter setting ($\alpha$ or $\beta$) over
all other configurations (merged/subtokenized, +SRL/$-$SRL, etc.) are highlighted.

\begin{figure}
  \begin{minipage}{0.45\linewidth}
  \vspace{0pt}
    \includegraphics[width=0.9\linewidth]{images/QA_gain_losses_accumulated.pdf}
  \end{minipage}
  \hfill
  \begin{minipage}{0.45\linewidth}
  \vspace{0pt}
    \includegraphics[width=0.9\linewidth]{images/QA_gain_losses_SRL2.pdf}
  \end{minipage}
  \stepcounter{myfigure}
  \caption[Accumulated Gains and Losses.]{Accumulated scores of the question answering experiments from table \ref{tab:qa-gain-loss-token-merged}. \textbf{Left}: Which SRL mode performed stronger out of a total of 8 settings. The bars indicate a slight outperforming of duplicating SRLs instead of adding zeros. \textbf{Right}: Counts for Gains and Losses in all 8 settings. Darker shades indicate significant results. The light green tip stands for settings where the gain equals 0.00.}
  \label{fig:qa-tot-gains}
\end{figure}

In contrast to the classification task restuls, there was no measurable
positive effect of adding SRLs to the German BERT embeddings: Three
out of four best results were achieved by models implementing vanilla
BERT embeddings and three out of four times the worst performance was
measured for +SRL settings. Only in the XQuAD $\beta$ setting (regular
SRL sequence encoding, see table \ref{tab:alpha-beta}), a +SRL setting
performed best --- however, when compared to the corresponding $-$SRL
setting, this 0.29\% gain is not statistically significant (see table
\ref{tab:qa-gain-loss-token-merged}).

\tab{tab:results-qa}{Test set accuracy ensemble results (per 5 models) on question answering tasks. \textbf{Bold} font marks the best result per line, \underline{underline} the second best, and \textit{italics} the poorest.}{
  \scalebox{1}{
    \begin{tabular}{llccc|ccc}
                                               &           & \multicolumn{6}{c}{\large \textbf{Question Answering Datasets}}  \\ \\
                                               &           & \multicolumn{6}{c}{\textbf{Span Prediction Head}}                                                                                                                                                                                                \\ \cline{3-8}
                                               &           & \multicolumn{3}{c|}{subtokenized}                                                                                                & \multicolumn{3}{c}{subtokens merged}                                                                          \\ \cline{3-8}
                                               &           & \multicolumn{1}{c|}{$-$SRL}                     & \multicolumn{2}{c|}{+SRL}                                                      & \multicolumn{1}{c|}{$-$SRL}         & \multicolumn{2}{c}{+SRL}                                                \\ %\cline{3-4}\cline{6-7}
                                               &           & \multicolumn{1}{c|}{}                           & \multicolumn{1}{c}{zeros}             & \multicolumn{1}{c|}{dupl.}             & \multicolumn{1}{c|}{}               & \multicolumn{1}{c}{zeros}          & dupl.                              \\ \hline\hline
    \multicolumn{1}{c}{\multirow{2}{*}{MLQA}}  & $\alpha$  & \multicolumn{1}{c|}{\textbf{30.69}}             & \multicolumn{1}{c}{\underline{29.68}} & \multicolumn{1}{c|}{\underline{29.68}} & \multicolumn{1}{c|}{21.92}          & \multicolumn{1}{c}{21.92}          & \multicolumn{1}{c}{\textit{21.81}} \\
    \multicolumn{1}{c}{}                       & $\beta$   & \multicolumn{1}{c|}{\textbf{44.75}}             & \multicolumn{1}{c}{\underline{44.55}} & \multicolumn{1}{c|}{43.41}             & \multicolumn{1}{c|}{\textit{41.66}} & \multicolumn{1}{c}{41.86}          & \multicolumn{1}{c}{41.79}          \\ \cline{3-8}
    \multicolumn{1}{c}{\multirow{2}{*}{XQuAD}} & $\alpha$  & \multicolumn{1}{c|}{\textbf{42.01}}             & \multicolumn{1}{c}{\underline{41.42}} & \multicolumn{1}{c|}{41.12}             & \multicolumn{1}{c|}{37.87}          & \multicolumn{1}{c}{36.98}          & \multicolumn{1}{c}{\textit{35.50}} \\
    \multicolumn{1}{c}{}                       & $\beta$   & \multicolumn{1}{c|}{\underline{46.57}}          & \multicolumn{1}{c}{45.43}             & \multicolumn{1}{c|}{\textbf{46.86}}    & \multicolumn{1}{c|}{37.43}          & \multicolumn{1}{c}{\textit{37.14}} & \multicolumn{1}{c}{39.14}          \\ \hline\hline
    \multicolumn{1}{c}{Scores}                 &           & \multicolumn{1}{c|}{\textbf{III} \underline{I}} & \multicolumn{2}{c|}{\textbf{I} \underline{III}}                                & \multicolumn{1}{c|}{}               & \multicolumn{2}{c}{}                                                    \\ \cline{1-2}
    \multicolumn{1}{c}{+SRL}                   & \multicolumn{3}{l}{\textbf{1} \underline{3}} \\
    \multicolumn{1}{c}{$-$SRL}                 & \multicolumn{3}{l}{\textbf{3} \underline{1}}

    \end{tabular}
  }
}{Results-QA}

When accumulated for subtokenized vs. merged patterns, there emerges a surprisingly
clear tendency (see left side of table \ref{tab:qa-gain-loss-token-merged}): Splitting SRLs according to
the BERT subtokenization always showed stronger results than BERT merging. 5 times out of the
total 8 settings the differences were even highly significant with $p < 1\%$ and one time
moderately significant with $p < 5\%$.

Also for the question answering datasets, I compare setting-wise the performance of $-$SRL against the better +SRL experiment
(see right side of table \ref{tab:qa-gain-loss-token-merged}): The only significant result is the MLQA $\alpha$
subtokenized setting, where the SRL-enriching led to a loss of 1.01\%. The other results are mixed, with $-$SRL
performing 3 times better than +SRL.
The only dataset hyperparameter configuration where for both SRL combination methods the +SRL implementation
performed better was XQuAD $\beta$ --- however in both cases insignificant.


\tab{tab:qa-gain-loss-token-merged}{\textbf{Left part}: Ensemble percentage points gains (positive numbers) / losses (negative numbers) for +SRL
                       over $-$SRL for the Span Prediction Head from table \ref{tab:results-qa}. The better of the +SRL configurations was taken into
                       account: \customcolorbox{zeros}{blue},
                       \customcolorbox{duplicate}{dark-blue}.
                       Light blue denotes that both architectures performed \customcolorbox{equally}{llight-blue}
                       (in which case both ensembles were tested for significance).
                       One asterisk signifies a $p$-value $<$ 10\%, two stand for $p <$ 5\% and three for $p <$ 1\%.
                       \textbf{Right part}: Performance of architectures when BERT \customcolorbox{subtokenized}{yellow} vs. \customcolorbox{merged}{purple}. Both SRL implementations were
                          compared pairwise..}{
  \scalebox{1}{
    \begin{tabular}{llP{2cm}P{2cm}cP{2cm}P{2cm}}
                                                 &           & \multicolumn{5}{c}{\textbf{Span Prediction Head}}                                                                                                                                                                   \\
                                                 &           & \multicolumn{2}{c}{Gains/Losses}                                                                            &  & \multicolumn{2}{c}{subtokenized/merged}                                                            \\ \cline{3-4} \cline{6-7}
                                                 &           & \multicolumn{1}{P{1.5cm}|}{subtok.}                            & \multicolumn{1}{P{1.5cm}}{merged}                        &  & \multicolumn{1}{P{1.5cm}|}{zeros}                       & \multicolumn{1}{P{1.5cm}}{dupl.}                       \\ \hline\hline
    \multicolumn{1}{P{1.5cm}}{\multirow{2}{*}{MLQA}}    & $\alpha$  & \multicolumn{1}{P{1.5cm}|}{\cellcolor{llight-blue} $-$1.01***} & \multicolumn{1}{P{1.5cm}}{\cellcolor{blue} .00}          &  & \multicolumn{1}{P{1.5cm}|}{\cellcolor{yellow} 7.76***}  & \multicolumn{1}{P{1.5cm}}{\cellcolor{yellow} 7.86***}  \\
    \multicolumn{1}{P{1.5cm}}{}                         & $\beta$   & \multicolumn{1}{P{1.5cm}|}{\cellcolor{blue} $-$.20}            & \multicolumn{1}{P{1.5cm}}{\cellcolor{blue} .20}          &  & \multicolumn{1}{P{1.5cm}|}{\cellcolor{yellow} 2.69}     & \multicolumn{1}{P{1.5cm}}{\cellcolor{yellow} 1.62}     \\ \cline{3-4} \cline{6-7}
    \multicolumn{1}{P{1.5cm}}{\multirow{2}{*}{XQuAD}}   & $\alpha$  & \multicolumn{1}{P{1.5cm}|}{\cellcolor{blue} $-$.59}            & \multicolumn{1}{P{1.5cm}}{\cellcolor{blue} $-$.89}         &  & \multicolumn{1}{P{1.5cm}|}{\cellcolor{yellow} 4.44**}   & \multicolumn{1}{P{1.5cm}}{\cellcolor{yellow} 5.62***}  \\
    \multicolumn{1}{P{1.5cm}}{}                         & $\beta$   & \multicolumn{1}{P{1.5cm}|}{\cellcolor{dark-blue} .29}          & \multicolumn{1}{P{1.5cm}}{\cellcolor{dark-blue} 1.71}    &  & \multicolumn{1}{P{1.5cm}|}{\cellcolor{yellow} 8.29***}  & \multicolumn{1}{P{1.5cm}}{\cellcolor{yellow} 7.72***}  \\
   \end{tabular}
  }
}{QA Gain-Loss / QA Tokenized vs. merged}


Therefore, the overall aggregation in figure \ref{fig:qa-tot-gains} does not look too promising for the
research question of this thesis: For the two question answering datasets in GerGLUE, the effect of enriching
BERT embeddings with SRL information seems to rather harm the model instead of adding helpful information.
Interestingly, this contrasts with the findings of \citeauthor{zhang2019semantics}, who reported a gain
of 4.3\% in accuracy on the SQuAD 2.0 set for their SemBERT implementation.\myfootnote{However, a ``synthetic
self training'' technique leading to these results is mentioned, but which is unfortunately not elaborated upon furthermore. For their
``regular'' SemBERT large implementation the reported gain is 1.9\% --- which would still stand
for a positive influence of SRL enrichment, which is not reproduced through my experiments.}


% Main conclusions:

% \begin{itemize}
%   \item SRLs in tendency helpful
%   \item not for Q\&A
%   \item results somewhat unclear
% \end{itemize}

