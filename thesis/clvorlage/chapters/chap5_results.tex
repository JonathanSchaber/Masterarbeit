\newchap{Results}
\label{chap:5_results}

% \epigraph{Yada, yada, yada}{\textit{Someone, somewhere}}

In this chapter I report the outcomings of my experiments on the GerGLUE dataset. Because there
were several different implementations of various parameters tested against each other (SRLs duplicated vs.
SRLs zeroed, BERT tokens merged vs. SRLs splitted, etc.), I provide several aggregated ``views'' on the
plain numerical results, where one or more of these implementations is paid attention to. Further, I
document the ourcomings of a human assessment of the quality of the DAMESRL produced SRLs to be able to
make propositions about the usefulness of those and their effect on the results. Similar to this, the
general data quality of random samples of two datasets is manually reviewed, for the same reasons.
Eventually, I carry out a small ablation study, controlling the effect of ghosting out different
SRL structures to show that the actual, semanticity providing power of SRLs lies in the combination
of these structures.

For each dataset, two
rounds of experiments where computed with one hyperparameter difference, resulting in
an $\alpha$ and $\beta$ for each dataset. The hyperparameters which were kept stable
over all experiments are listen in table \ref{tab:hyper-configs}.

\tab{tab:hyper-configs}{Stable hyperparameter configurations.}{
  \scalebox{0.9}{
    \begin{tabular}{ll}
    learning rate              & 2e-05                         \\
    SRL embedding dimensions   & 20                            \\
    SRL GRU hidden size        & 32                            \\
    SRL number of layers       & 2                             \\
    SRL bias                   & True                          \\
    SRL bidirectional          & True                          \\
    SRL dropout                & 0.1                           \\
    \# of Epochs               & 50 (deISEAR, SCARE, XNLI, MLQA, XQuAD), 20 (PAWS-X) \\
    Batch Size                 & 16 (classification datasets) 8 (Q\&A datasets) \\
    maximum sequence length    & 100 (PAWS-X, XNLI) 512 (MLQA, XQuAD)
    \end{tabular}
  }
}{Stable Hyperparameter Configs}

In table \ref{tab:alpha-beta}
the different hyperparameter for the two runs is listed for each dataset.

\tab{tab:alpha-beta}{Differences betwenn $\alpha$ and $\beta$ runs for each dataset.}{
  \scalebox{0.9}{
    \begin{tabular}{lll}
                               & $\alpha$                   & $\beta$                    \\ \hline
    deISEAR                    & maximum length: 40         & maximum length: 200        \\
    SCARE                      & maximum length: 50         & maximum length: 100        \\
    PAWS-X                     & Examples train set: 8,000  & Examples train set: 48,977 \\
    XNLI                       & Predefined splits          & Resplitted 70:15:15        \\
    MLQA                       & Predefined splits          & Resplitted 70:15:15        \\
    XQuAD                      & SRLs encoded sentence wise & Regular SRL encoding       \\
    \end{tabular}
  }
}{$\alpha$-$\beta$ differences}

Since the different dataset types were tested with different heads, I group the them
the following way: Results for classification datasets (deISEAR, SCARE, PAWS-X, XNLI)
are reported together, and the results for question answering datasets (MLQA, XQuAD),
are grouped together. Since the latter group consists of only two datasets addressing
the same task, and bearing in mind that there were only experiments for one head in
this group, the Span Prediction Head, the discussion analyzing these results will not
be as interesting and substantial as for the first group. For space reasons, and for
the sake of straightforwardness, only test set results are reported in the tables.
However, model selection was always based on development set results as is visualized
in figure \ref{fig:acc-loss}:

Based on the progression of the accuracy value on the development set the epoch with
the highest accuracy on the development set was chosen and the corresponding test set
accuracy was taken as best performance for this experiment on the dataset. The vertical
line in those three example experiments\myfootnote{\textbf{deISEAR} $\beta$ +SRL normal
duplicate [CLS] head, \textbf{SCARE} $\beta$ +SRL merge zeros GRU head, \textbf{XNLI}
$\alpha$ +SRL merge zeros [CLS] head.} accuracy and loss progression on development and
test results compared). marks this epoch, the horizontal dran through line marks the
development accuarcy (the ``peak'' of the red curve), and the horizontal dotted line
marks the correpsonding test accuracy. On the right the corresponding losses for the
same epoch are marked in the same fashion.

\begin{figure}[H]
  \begin{center}
    \includegraphics[width=11cm]{images/acc_loss-3_sets.pdf}
  \end{center}
  \stepcounter{myfigure}
  \caption[Accuracy/Loss plots of three experiments]{Accuracy and loss plots for three
experiments (\textcolor{std-blue}{training}, \textcolor{std-red}{development}, and
\textcolor{std-green}{test} set): The vertical line marks the epoch with the highest
development set accuracy; the test set accurracy for this epoch is taken as peak performance
of the model.}
  \label{fig:acc-loss}
\end{figure}

In the deISEAR plot, we see that early stopping
was triggered after the loss on the development set increased for four contiguous
epochs --- indicating an overfitting of the model on the training set. Both SCARE
and XNLI experiments ran until the maximum number of epochs were reached without
triggering early stopping. However, the SCARE accuracy and loss progressions show
much more fluctuations and erratic behavior than the XNLI ones, indicating the
noisiness of the data and the randomness of the function that needs to be learned
by the model.


% (see figure
% \ref{fig:acc-loss} for visualizations of example\myfootnote{\textbf{deISEAR} $\beta$
% +SRL normal duplicate [CLS] head, \textbf{SCARE} $\beta$ +SRL merge zeros GRU head,
% \textbf{XNLI} $\alpha$ +SRL merge zeros [CLS] head.} accuracy and loss progression on
% development and test results compared).


\section{Controlling for Statistical Significance}

Before reporting the actual results, I briefly elaborate on my strategy for
assessing statistical significance, since this is a crucial aspect of empirical
analysis in general, and in attributing superiority of one model over the other
on the basis of measured performance on a dataset in particular:
``if we rely on empirical evaluation to validate our hypotheses and reveal the
correct language processing mechanisms, we better be sure that our results are
not coincidental'' \citep{dror2018hitchhiker}. In a formalized way, if we have a
performance measure $M$ --- in my case accuracy --- to evaluate the predictions
of a Model $A$ on a dataset $X$, we want to assess, if this performance was better
than the one of a model $B$:

$\delta(X) = M(A, X) - M(B, X)$

where $\delta(X)$ refers to the test statistic, which will be shortly explained.
Thus, the null-hypothesis which needs to be rejected, and the corresponding
alternative hypothesis (that model $A$ \emph{is} in fact truly superior to model
$B$) are formulated:

$H_0:\delta(X) \leq 0$

$H_1:\delta(X) > 0$

However, it is normally not the case that one is able to answer this question --- is the observed
performance measurement due to true model improvement or simply due to chance --- by simply looking
at the pure results; ``[b]ut with statistics, one can answer the following proxy question: if the
new technique was actually no different than the old technique (the null hypothesis), what is the
probability that the results on the test set would be at least this skewed in the new techniqueâ€™s
favor?'' \citep{yeh2000more}

In other words, to be able to recet the above defined null-hypothesis, usually a $p$-value test is
computed, where $p$ is defined ``as the probability, under the null hypothesis $H_0$, of obtaining
a result equal to or more extreme than what was actually observe'' \citep[p.~1384]{dror2018hitchhiker}:

$Pr(\delta(X) \geq \delta_{observed} | H_0)$.

There are different approaches as to how this probability can
be computed: \citep{koehn2004statistical}, e.g., proposes a
``paired bootstrap resampling'' algorithm he applies to machine
translation systems. I follow here \cite{morgan2005statistical} who
argues in favour of an ``approximate randomization '' approach,
which makes less assumptions about the distribution of the sample
distributions.\myfootnote{ \citeauthor{koehn2004statistical}
presupposes that the results to be compared stem from different
subsamples of the original test set which get re-drawn repeatedly
with replacement. In my case, this does not hold since my results
are not obtained from subsamples but the whole test set.}

% ``It is important to have a method at hand that gives us assurances that the
% observed increase in the test score on a test set reflects true improvement in system
% quality.'' \citep{koehn2004statistical}

% \citet{koehn2004statistical} focus strongly on significance testing in the context of evaluating
% on a sub-sample of the test set --- due to expensiveness of testing on the whole set ---
% and making statements about the reliability of this subset sample:

% ``Given a test result of \emph{m} BLEU, we want to compute with a confidence \emph{q} (or
% p-level P = 1 - \emph{q}) that the rue BLEU score lies in an interval [\emph{m} - \emph{d},
% \emph{m} + \emph{d}].'' \citep{koehn2004statistical}

% Since the systems under review here predict on the exact same test set, the assumed independence
% of the predictions of the two models holds no longer. \citet{morgan2005statistical} propose
% the following algorithm for testing difference significance:

% ``When the results are better with the new technique, a question arises as to whether
% these result differences are due to the new technique actually being better or just due to
% chance. Unfortunately, one usually cannot directly answer the question â€œwhat is the probability
% that the new technique is better given the results on the test data set'' \citep{yeh2000more}

% ``But with statistics, one can answer the following proxy question: if the new technique was
% actually no different than the old technique (the null hypothesis), what is the probability
% that the results on the test set would be at least this skewed in the new techniqueâ€™s
% favor?'' \citep{yeh2000more}

% Many evaluation metrics ``have a tendency to underestimate the significance of the results'',
% due to their inherent assumption that the compared systems ``produce independent results''
% when in reality, they tend to produce ``positively correlated results''. \citep{yeh2000more}

The basic idea is to go through the paired predictions of two models (on
the same test set) and
randomly (i.e. with a 50\% chance) flip the predictions between them. If
this has been completed for the whole test set, the difference $\delta_{perm}$ of the
evaluation metric between the recomputed performance of them is calculated again
and it is checked whether $\delta_{perm} \geq \delta_{orig}$. This process is repeated
$R$ times and if the number of times $\delta_{perm}$ was greater/equal is small
enough, in statistics, normally 5\%, for a large enough number $R$, we can reject
the null-hypothesis with good confidence. See the following algorithm \ref{alg:alg:approximate-randomization}
for a more detailed formulation of this process:

\begin{algorithm}
\caption{Approximate Randomization Algorithm}
\label{alg:approximate-randomization}
	\begin{algorithmic}[1]
    \STATE $p(M,x) =$ prediction of model $M$ on example $x$
    \STATE $A, B =$ Two different models
    \STATE $O = \{x_1, \dotsc, x_n\} =$ test set
    \STATE $O_A = \{p(A,x_1), \dotsc, p(A,x_n)\}$
    \STATE $O_B = \{p(B,x_1), \dotsc, p(B,x_n)\}$
    \STATE $O_{gold} =$ gold labels for $\{x_1, \dotsc, x_n\}$
    \STATE $e(\hat{Y},Y) =$ evaluation function for gold labels $\hat{Y}$ and predictions $Y$
    \STATE $t_{original} =\ \mid e(O_{gold},O_A) - e(O_{gold},O_B) \mid$
    \STATE $rand() =$ returns $0$ or $1$, randomly
    \STATE $swap(x,y) =$ exchanges elements $x \in A,y \in B$ such that $y \in A, x \in B$
    \STATE $r \leftarrow 0$
    \STATE $R \leftarrow 0$
    \STATE $threshold \leftarrow 1,000$
    \STATE $p \leftarrow 0.05$
    \WHILE{$R < threshold$}
      \FORALL{$(a_i, b_i) \in O_A \times O_B \mid i \in I$}
        \IF{$rand() = 0$}
          \STATE $swap(a_i,b_i)$
        \ENDIF
      \ENDFOR
      \STATE $t_{permute} =\ \mid e(O_{gold},O_A') - e(O_{gold},O_B') \mid$
      \IF{$t_{permute} \geq t_{original}$}
        \STATE $r \mathrel{+}= 1$
      \ENDIF
      \STATE $R \mathrel{+}= 1$
    \ENDWHILE
    \IF{$\frac{r+1}{R+1} < p$}
      \STATE system $A$ truly better than system $B$
    \ENDIF
  \end{algorithmic}
\end{algorithm}


% \subsubsection{Example Case for XNLI}

% Let's consider the case for the non-merged subtokens setting in the resampled XNLI dataset. The test
% set contains 1,125 sentence pairs for which textual entailment must be predicted. From these 1,125
% sentence pairs, 398 bear the gold label \emph{contradiction}, 357 are labeled \emph{entailment},
% and 370 are \emph{neutral}; so, the class distribution of the set is fairly balanced.

% I trained and optimized five systems for two architectures on the training and development set
% of XNLI: One architecture is the plain ``vanilla'' GRU classifier described in section XXX, the
% other is the same GRU architecture enriched with embedded SRLs (implementing the duplication
% approach, described in section XXX). The ``vanilla'' system ensemble achieved an accuracy of
% 66,58\% on the XNLI test set, while the SRL enriched ensemble scored a 68,27\% --- in other
% words, the SRL enchriched ensemble performed 1,69\% better than the ``vanilla'' ensemble.

% To check if this difference truly measures the supremacy of the latter model over the first, I
% apply the above described algorithm \ref{alg:approximate-randomization} for testing significance
% by permuting the actual ensemble predictions. Note that both ensemble models were equally
% right or wrong in 1,018 cases out of 1,125.  From this follows, in consequence, that in 90,49\%
% of the cases the flipping of predictions between the ensemble models will have no effect.

% Result p = 4.80\%

% In contrary, if we compare this results to the zero implementation of SRLs, we observe something
% different: The accuracy of this ensemble was slightly lower than the duplicate architecture;
% namely 67,73\% or, speaking in differences, 1.15\% better than the vanilly ensemble. The
% number of equally right or wrong examples was also slightly lower --- 1,010 examples were
% equally wrong or correct predicted by the systems.

% Result zeros p = 14.09\%

% In summary it is safe to say that although there is a positive effect of injecting
% SRL information during training over all datasets and architectures, this effect is
% arguably quite small and unsteady. {\color{red} this is especially in yontrast to
% \cite{zhang2019semantics}, who report more stable and higher effects} In the next
% sections I will try to give an answer as to what are the reasons for these, honestly
% spoken, moderate results. Concretely, I will argue that this is mainly due to noise,
% present in differing intensities and at various levels in the data I acquired, that
% the model has to cope with:

% Afterwards, statistical signifance is indicated by appending one, two, or three asterisks
% to a result that was compared with one or several others: * stands for $p < 10\%$, ** for
% $p < 5\%$, and *** indicates very high significance of $p < 1\%$.



\section{Classification Dataset Results}
\label{sec:classification-results}

To obtain as stable results as possible, I decided to train five models for each architecture
and configuration, all initialized with different random seeds. Additionally, I ensembled the
five models, achieving a performance gain of several percentage points (see example of PAWS-X,
table \ref{tab:pawsx-beta}).\myfootnote{For ensembling, a straightforward majority vote of the five models
is implemented.} In table \ref{tab:results} below, the test set ensemble results for
each architecture on the classification datasets are reported.

The best, second best and worst performace over all three heads and SRL configurations
are marekd. In the last row, Best and second best results per head and +/-SRL for each
subtokenization settings are accumulated. As the Scores indicate, taken the results without
controlling for significance,\myfootnote{ Here I don't control for statistical significance
because it is not clear, against what model(s) should a peak performance be controlled
for: Take the 77.45\% accuracy of the deISEAR $\alpha$, achieved by the +SRL, BERT tokens
subtokenized, FFNN head: Against what should this results be compared? To all other -SRL
results? Only to the corresponding -SRL results for the same head?} the +SRL configurations
achieved 5 times the best, and 10 times the second best results --- compared to 4 best and
5 second best results for -SRL. This is a first indicator that adding SRL information seems
to be able to support plain BERT embeddings in classification tasks.

The accumulations in roman numerals also indicate a further, observable trend: The GRU head
accumulated 5 best and 6 second best results, independent of with or without SRLs, appears to be better
suited for classification tasks than the [CLS] (2 best, 6 second bset) and FFNN (2 best, 3 second bset) heads.
Supporting this, the GRU head never was responsible for the worst performance, which were all
achieved by the FFNN and [CLS] heads.

In the following table \ref{tab:pawsx-beta}, the ensembling of each of the results in the main table is
exemplary disaggregated. Each head-SRL configuration is randomly initialized 5 times, resulting in
5 models achieving a certain performance on the dataset. By ensembling them, a steady gain of, in this case,
1.26\% compared to the average of the 5 models is achieved. Similar ensembling improvements where
obseverd in all other datasets.

% In a first step, I will discuss on the overall performance of models when the SRLs are added,
% compared to the same architectures without.

\begin{landscape}\centering
  % \vspace*{\fill}
  \tab{tab:results}{Test set accuracy ensemble results (per 5 models) on single sentence and sentence pair tasks.
                    \textbf{Bold} font marks the best result per line, \underline{underline} the second best, and \textit{italics} the poorest.
                    In the \emph{Scores} row, the afore mentioned positive extremes are accumulated for $-$SRL and $+$SRL;
                    note that if both +SRL configurations of an architecture achieved an extreme, it is only counted once.

                    The line marked with light gray --- PAWS-X $\beta$ --- is ``expanded'' in table \ref{tab:pawsx-beta} to illustrate
                    that each result in this table is actually a majority voting out of an ensembeling of fice models.}{
    \scalebox{0.90}{
      \begin{tabular}{llccc|ccc|ccc|ccc|ccc|ccc}
                                                      &           & \multicolumn{18}{c}{\large \textbf{Classification Datasets}}  \\ \\
                                                      &           & \multicolumn{6}{c|}{\textbf{{[}CLS{]} Head}}                                                                                                                                                                                                                      & \multicolumn{6}{c|}{\textbf{FFNN Head}}                                                                                                                                                                                                & \multicolumn{6}{c}{\textbf{GRU Head}}                                                                                                                                                                                                                           \\ \cline{3-20}
                                                      &           & \multicolumn{3}{c|}{subtokenized}                                                                                               & \multicolumn{3}{c|}{subtokens merged}                                                                                           & \multicolumn{3}{c|}{subtokenized}                                                                                   & \multicolumn{3}{c|}{subtokens merged}                                                                            & \multicolumn{3}{c|}{subtokenized}                                                                                             & \multicolumn{3}{c}{subtokens merged}                                                                                            \\ \cline{3-20}
                                                      &           & \multicolumn{1}{c|}{$-$SRL}                  & \multicolumn{2}{c|}{+SRL}                                                        & \multicolumn{1}{c|}{$-$SRL}                & \multicolumn{2}{c|}{+SRL}                                                          & \multicolumn{1}{c|}{$-$SRL}                    & \multicolumn{2}{c|}{+SRL}                                          & \multicolumn{1}{c|}{$-$SRL}                    & \multicolumn{2}{c|}{+SRL}                                       & \multicolumn{1}{c|}{$-$SRL}                  & \multicolumn{2}{c|}{+SRL}                                                      & \multicolumn{1}{c|}{$-$SRL}                     & \multicolumn{2}{c}{+SRL}                                                      \\ % \cline{3-4}\cline{6-7}\cline{9-10}\cline{12-13}\cline{15-16}\cline{18-19}
                                                      &           & \multicolumn{1}{c|}{}                        & \multicolumn{1}{c}{zeros}              & \multicolumn{1}{c|}{dupl.}              & \multicolumn{1}{c|}{}                      & \multicolumn{1}{c}{zeros}                 & \multicolumn{1}{c|}{dupl.}             & \multicolumn{1}{c|}{}                          & \multicolumn{1}{c}{zeros}             & \multicolumn{1}{c|}{dupl.} & \multicolumn{1}{c|}{}                          & \multicolumn{1}{c}{zeros}          & dupl.                      & \multicolumn{1}{c|}{}                        & \multicolumn{1}{c}{zeros}             & dupl.                                  & \multicolumn{1}{c|}{}                           & \multicolumn{1}{c}{zeros}             & dupl.                                 \\ \hline\hline
         \multicolumn{1}{c}{\multirow{2}{*}{deISEAR}} & $\alpha$  & \multicolumn{1}{c|}{71.52}                   & \multicolumn{1}{c}{72.19}              & \multicolumn{1}{c|}{71.52}              & \multicolumn{1}{c|}{72.19}                 & \multicolumn{1}{c}{\textit{67.55}}        & \multicolumn{1}{c|}{72.19}             & \multicolumn{1}{c|}{70.86}                     & \multicolumn{1}{c}{\textbf{77.48}}    & \multicolumn{1}{c|}{72.85} & \multicolumn{1}{c|}{74.17}                     & \multicolumn{1}{c}{72.85}          & \multicolumn{1}{c|}{74.17} & \multicolumn{1}{c|}{70.20}                   & \multicolumn{1}{c}{\underline{74.83}} & \multicolumn{1}{c|}{74.17}             & \multicolumn{1}{c|}{73.51}                      & \multicolumn{1}{c}{70.20}             & \multicolumn{1}{c}{71.52}             \\
         \multicolumn{1}{c}{}                         & $\beta$   & \multicolumn{1}{c|}{71.52}                   & \multicolumn{1}{c}{\underline{74.83}}  & \multicolumn{1}{c|}{\underline{74.83}}  & \multicolumn{1}{c|}{70.86}                 & \multicolumn{1}{c}{70.86}                 & \multicolumn{1}{c|}{72.85}             & \multicolumn{1}{c|}{\underline{74.83}}         & \multicolumn{1}{c}{\textit{68.21}}    & \multicolumn{1}{c|}{70.20} & \multicolumn{1}{c|}{\underline{74.83}}         & \multicolumn{1}{c}{73.51}          & \multicolumn{1}{c|}{70.86} & \multicolumn{1}{c|}{73.51}                   & \multicolumn{1}{c}{\underline{74.83}} & \multicolumn{1}{c|}{72.19}             & \multicolumn{1}{c|}{\textbf{76.82}}             & \multicolumn{1}{c}{70.20}             & \multicolumn{1}{c}{\underline{74.83}} \\ \cline{3-20}
         \multicolumn{1}{c}{\multirow{2}{*}{SCARE}}   & $\alpha$  & \multicolumn{1}{c|}{\underline{85.61}}       & \multicolumn{1}{c}{82.58}              & \multicolumn{1}{c|}{83.71}              & \multicolumn{1}{c|}{83.33}                 & \multicolumn{1}{c}{83.71}                 & \multicolumn{1}{c|}{\underline{85.61}} & \multicolumn{1}{c|}{83.33}                     & \multicolumn{1}{c}{83.71}             & \multicolumn{1}{c|}{84.09} & \multicolumn{1}{c|}{84.09}                     & \multicolumn{1}{c}{\textit{81.44}} & \multicolumn{1}{c|}{84.47} & \multicolumn{1}{c|}{83.71}                   & \multicolumn{1}{c}{83.33}             & \multicolumn{1}{c|}{84.09}             & \multicolumn{1}{c|}{\textbf{85.98}}             & \multicolumn{1}{c}{84.09}             & \multicolumn{1}{c}{83.71}             \\
         \multicolumn{1}{c}{}                         & $\beta$   & \multicolumn{1}{c|}{\underline{86.36}}       & \multicolumn{1}{c}{84.85}              & \multicolumn{1}{c|}{84.47}              & \multicolumn{1}{c|}{85.23}                 & \multicolumn{1}{c}{85.23}                 & \multicolumn{1}{c|}{85.23}             & \multicolumn{1}{c|}{\textbf{86.74}}            & \multicolumn{1}{c}{85.98}             & \multicolumn{1}{c|}{85.23} & \multicolumn{1}{c|}{84.47}                     & \multicolumn{1}{c}{\textit{83.33}} & \multicolumn{1}{c|}{84.09} & \multicolumn{1}{c|}{\textbf{86.74}}          & \multicolumn{1}{c}{85.98}             & \multicolumn{1}{c|}{83.71}             & \multicolumn{1}{c|}{\underline{86.36}}          & \multicolumn{1}{c}{84.09}             & \multicolumn{1}{c}{85.23}             \\ \hline
         \multicolumn{1}{c}{\multirow{2}{*}{PAWS-X}}  & $\alpha$  & \multicolumn{1}{c|}{80.63}                   & \multicolumn{1}{c}{81.60}              & \multicolumn{1}{c|}{81.49}              & \multicolumn{1}{c|}{\textit{79.92}}        & \multicolumn{1}{c}{80.63}                 & \multicolumn{1}{c|}{82.51}             & \multicolumn{1}{c|}{81.19}                     & \multicolumn{1}{c}{80.78}             & \multicolumn{1}{c|}{80.07} & \multicolumn{1}{c|}{80.43}                     & \multicolumn{1}{c}{80.02}          & \multicolumn{1}{c|}{80.68} & \multicolumn{1}{c|}{82.26}                   & \multicolumn{1}{c}{82.77}             & \multicolumn{1}{c|}{82.77}             & \multicolumn{1}{c|}{82.82}                      & \multicolumn{1}{c}{\underline{82.87}} & \multicolumn{1}{c}{\textbf{83.53}}    \\
         \multicolumn{1}{c}{}                         & $\beta$   & \multicolumn{1}{g|}{87.49}                   & \multicolumn{1}{g}{\underline{88.05}}  & \multicolumn{1}{g|}{\textbf{88.21}}     & \multicolumn{1}{g|}{87.75}                 & \multicolumn{1}{g}{87.24}                 & \multicolumn{1}{g|}{88.00}             & \multicolumn{1}{g|}{86.83}                     & \multicolumn{1}{g}{87.39}             & \multicolumn{1}{g|}{87.09} & \multicolumn{1}{g|}{87.75}                     & \multicolumn{1}{g}{\textit{86.58}} & \multicolumn{1}{g|}{86.68} & \multicolumn{1}{g|}{87.60}                   & \multicolumn{1}{g}{87.60}             & \multicolumn{1}{g|}{87.90}             & \multicolumn{1}{g|}{88.00}                      & \multicolumn{1}{g}{88.00}             & \multicolumn{1}{g}{\underline{88.05}} \\ \cline{3-20}
         \multicolumn{1}{c}{\multirow{2}{*}{XNLI}}    & $\alpha$  & \multicolumn{1}{c|}{67.34}                   & \multicolumn{1}{c}{\textbf{67.52}}     & \multicolumn{1}{c|}{66.64}              & \multicolumn{1}{c|}{66.94}                 & \multicolumn{1}{c}{66.94}                 & \multicolumn{1}{c|}{\textit{66.26}}    & \multicolumn{1}{c|}{67.20}                     & \multicolumn{1}{c}{\underline{67.42}} & \multicolumn{1}{c|}{67.34} & \multicolumn{1}{c|}{66.38}                     & \multicolumn{1}{c}{67.08}          & \multicolumn{1}{c|}{66.92} & \multicolumn{1}{c|}{66.68}                   & \multicolumn{1}{c}{66.60}             & \multicolumn{1}{c|}{67.14}             & \multicolumn{1}{c|}{66.42}                      & \multicolumn{1}{c}{66.54}             & \multicolumn{1}{c}{66.26}             \\
         \multicolumn{1}{c}{}                         & $\beta$   & \multicolumn{1}{c|}{68.09}                   & \multicolumn{1}{c}{68.18}              & \multicolumn{1}{c|}{66.84}              & \multicolumn{1}{c|}{67.82}                 & \multicolumn{1}{c}{67.82}                 & \multicolumn{1}{c|}{\underline{68.36}} & \multicolumn{1}{c|}{66.31}                     & \multicolumn{1}{c}{65.60}             & \multicolumn{1}{c|}{66.40} & \multicolumn{1}{c|}{\textit{64.98}}            & \multicolumn{1}{c}{65.51}          & \multicolumn{1}{c|}{65.07} & \multicolumn{1}{c|}{66.84}                   & \multicolumn{1}{c}{67.82}             & \multicolumn{1}{c|}{67.02}             & \multicolumn{1}{c|}{67.64}                      & \multicolumn{1}{c}{66.31}             & \multicolumn{1}{c}{\textbf{68.53}}    \\ \hline\hline
         \multicolumn{1}{c}{Scores}                   &           & \multicolumn{1}{c|}{\underline{II}}          & \multicolumn{2}{c|}{\textbf{II} \underline{II}}                                  & \multicolumn{1}{c|}{}                      & \multicolumn{2}{c|}{\underline{II}}                                                & \multicolumn{1}{c|}{\textbf{I} \underline{I}}  & \multicolumn{2}{c|}{\textbf{I} \underline{I}}                      & \multicolumn{1}{c|}{\underline{I}}             & \multicolumn{2}{c|}{}                                           & \multicolumn{1}{c|}{\textbf{I}}              & \multicolumn{2}{c|}{\underline{II}}                                            & \multicolumn{1}{c|}{\textbf{II} \underline{I}}  & \multicolumn{2}{c}{\textbf{II} \underline{III}}                               \\ \cline{1-2}
         \multicolumn{1}{c}{+SRL}                     & \multicolumn{3}{l}{\textbf{5} \underline{10}} \\
         \multicolumn{1}{c}{$-$SRL}                   & \multicolumn{3}{l}{\textbf{4} \underline{5}}
      \end{tabular}
    }
  }{Results}

  \tab{tab:pawsx-beta}{The ``expanded'' PAWS-X $\beta$ results. The light gray line corresponds to the one in table \ref{tab:results}.
                       As can be seen, the fluctuations between single models is not too big, {\textcolor{red} which is an indicator that the architecture
                       is fairly stable.} Ensembeling reliably leads to a 1.26 percentage points gain on average, with a standard deviation of 0.26\%.}{
    \scalebox{0.9}{
      \begin{tabular}{lccc|ccc|ccc|ccc|ccc|ccc}
                 & \multicolumn{18}{c}{\large \textbf{PAWS-X $\beta$}}  \\ \\
                 & \multicolumn{6}{c|}{\textbf{{[}CLS{]} Head}}                                                                                                                                                                                                                      & \multicolumn{6}{c|}{\textbf{FFNN Head}}                                                                                                                                                                                                         & \multicolumn{6}{c}{\textbf{GRU Head}}                                                                                                                                                                                                                           \\ \cline{2-19}
                 & \multicolumn{3}{c|}{subtokenized}                                                                                               & \multicolumn{3}{c|}{subtokens merged}                                                                                           & \multicolumn{3}{c|}{subtokenized}                                                                                            & \multicolumn{3}{c|}{subtokens merged}                                                                            & \multicolumn{3}{c|}{subtokenized}                                                                                             & \multicolumn{3}{c}{subtokens merged}                                                                                            \\ \cline{2-19}
                 & \multicolumn{1}{c|}{$-$SRL}                  & \multicolumn{2}{c|}{+SRL}                                                        & \multicolumn{1}{c|}{$-$SRL}                & \multicolumn{2}{c|}{+SRL}                                                          & \multicolumn{1}{c|}{$-$SRL}                    & \multicolumn{2}{c|}{+SRL}                                                   & \multicolumn{1}{c|}{$-$SRL}                    & \multicolumn{2}{c|}{+SRL}                                       & \multicolumn{1}{c|}{$-$SRL}                  & \multicolumn{2}{c|}{+SRL}                                                      & \multicolumn{1}{c|}{$-$SRL}                     & \multicolumn{2}{c}{+SRL}                                                      \\
                 & \multicolumn{1}{c|}{}                        & \multicolumn{1}{c}{zeros}              & \multicolumn{1}{c|}{dupl.}              & \multicolumn{1}{c|}{}                      & \multicolumn{1}{c}{zeros}                 & \multicolumn{1}{c|}{dupl.}             & \multicolumn{1}{c|}{}                          & \multicolumn{1}{c}{zeros}             & \multicolumn{1}{c|}{dupl.}          & \multicolumn{1}{c|}{}                          & \multicolumn{1}{c}{zeros}          & dupl.                      & \multicolumn{1}{c|}{}                        & \multicolumn{1}{c}{zeros}             & dupl.                                  & \multicolumn{1}{c|}{}                           & \multicolumn{1}{c}{zeros}             & dupl.                                 \\ \hline\hline
        Model 1  & \multicolumn{1}{c|}{85.41}                   & \multicolumn{1}{c}{85.36}              & \multicolumn{1}{c|}{86.02}              & \multicolumn{1}{c|}{86.43}                 & \multicolumn{1}{c}{86.53}                 & \multicolumn{1}{c|}{87.04}             & \multicolumn{1}{c|}{85.77}                     & \multicolumn{1}{c}{85.61}             & \multicolumn{1}{c|}{85.77}          & \multicolumn{1}{c|}{86.22}                     & \multicolumn{1}{c}{84.70}          & \multicolumn{1}{c|}{86.53} & \multicolumn{1}{c|}{87.54}                   & \multicolumn{1}{c}{87.49}             & \multicolumn{1}{c|}{86.43}             & \multicolumn{1}{c|}{85.51}                      & \multicolumn{1}{c}{86.93}             & \multicolumn{1}{c}{86.53}             \\
        Model 2  & \multicolumn{1}{c|}{86.07}                   & \multicolumn{1}{c}{86.83}              & \multicolumn{1}{c|}{86.99}              & \multicolumn{1}{c|}{85.87}                 & \multicolumn{1}{c}{86.32}                 & \multicolumn{1}{c|}{86.68}             & \multicolumn{1}{c|}{86.17}                     & \multicolumn{1}{c}{85.82}             & \multicolumn{1}{c|}{87.39}          & \multicolumn{1}{c|}{85.92}                     & \multicolumn{1}{c}{85.36}          & \multicolumn{1}{c|}{85.26} & \multicolumn{1}{c|}{87.04}                   & \multicolumn{1}{c}{86.99}             & \multicolumn{1}{c|}{85.87}             & \multicolumn{1}{c|}{87.19}                      & \multicolumn{1}{c}{86.07}             & \multicolumn{1}{c}{87.44}             \\
        Model 3  & \multicolumn{1}{c|}{86.07}                   & \multicolumn{1}{c}{87.49}              & \multicolumn{1}{c|}{86.99}              & \multicolumn{1}{c|}{87.14}                 & \multicolumn{1}{c}{85.26}                 & \multicolumn{1}{c|}{86.73}             & \multicolumn{1}{c|}{86.22}                     & \multicolumn{1}{c}{84.90}             & \multicolumn{1}{c|}{85.36}          & \multicolumn{1}{c|}{85.41}                     & \multicolumn{1}{c}{85.31}          & \multicolumn{1}{c|}{85.71} & \multicolumn{1}{c|}{86.12}                   & \multicolumn{1}{c}{85.66}             & \multicolumn{1}{c|}{86.83}             & \multicolumn{1}{c|}{86.48}                      & \multicolumn{1}{c}{87.09}             & \multicolumn{1}{c}{86.93}             \\
        Model 4  & \multicolumn{1}{c|}{87.39}                   & \multicolumn{1}{c}{86.32}              & \multicolumn{1}{c|}{86.58}              & \multicolumn{1}{c|}{86.38}                 & \multicolumn{1}{c}{84.04}                 & \multicolumn{1}{c|}{87.65}             & \multicolumn{1}{c|}{86.53}                     & \multicolumn{1}{c}{86.73}             & \multicolumn{1}{c|}{85.61}          & \multicolumn{1}{c|}{85.82}                     & \multicolumn{1}{c}{85.61}          & \multicolumn{1}{c|}{86.38} & \multicolumn{1}{c|}{84.99}                   & \multicolumn{1}{c}{86.02}             & \multicolumn{1}{c|}{87.70}             & \multicolumn{1}{c|}{86.99}                      & \multicolumn{1}{c}{86.68}             & \multicolumn{1}{c}{86.88}             \\
        Model 5  & \multicolumn{1}{c|}{86.63}                   & \multicolumn{1}{c}{86.43}              & \multicolumn{1}{c|}{86.58}              & \multicolumn{1}{c|}{86.99}                 & \multicolumn{1}{c}{85.26}                 & \multicolumn{1}{c|}{85.56}             & \multicolumn{1}{c|}{86.18}                     & \multicolumn{1}{c}{87.09}             & \multicolumn{1}{c|}{84.75}          & \multicolumn{1}{c|}{87.04}                     & \multicolumn{1}{c}{85.77}          & \multicolumn{1}{c|}{85.82} & \multicolumn{1}{c|}{86.12}                   & \multicolumn{1}{c}{85.82}             & \multicolumn{1}{c|}{86.73}             & \multicolumn{1}{c|}{87.34}                      & \multicolumn{1}{c}{86.73}             & \multicolumn{1}{c}{86.58}             \\ \hline\hline %\hhline{~==================}
        Average  & \multicolumn{1}{c|}{86.31}                   & \multicolumn{1}{c}{86.49}              & \multicolumn{1}{c|}{86.63}              & \multicolumn{1}{c|}{86.56}                 & \multicolumn{1}{c}{85.48}                 & \multicolumn{1}{c|}{\underline{86.81}} & \multicolumn{1}{c|}{86.22}                     & \multicolumn{1}{c}{86.03}             & \multicolumn{1}{c|}{\textit{85.78}} & \multicolumn{1}{c|}{86.08}                     & \multicolumn{1}{c}{85.35}          & \multicolumn{1}{c|}{85.94} & \multicolumn{1}{c|}{86.35}                   & \multicolumn{1}{c}{86.40}             & \multicolumn{1}{c|}{86.71}             & \multicolumn{1}{c|}{86.70}                      & \multicolumn{1}{c}{86.70}             & \multicolumn{1}{c}{\textbf{86.87}}    \\
        Ensemble & \multicolumn{1}{g|}{87.49}                   & \multicolumn{1}{g}{\underline{88.05}}  & \multicolumn{1}{g|}{\textbf{88.21}}     & \multicolumn{1}{g|}{87.75}                 & \multicolumn{1}{g}{87.24}                 & \multicolumn{1}{g|}{88.00}             & \multicolumn{1}{g|}{86.83}                     & \multicolumn{1}{g}{87.39}             & \multicolumn{1}{g|}{87.09}          & \multicolumn{1}{g|}{87.75}                     & \multicolumn{1}{g}{\textit{86.58}} & \multicolumn{1}{g|}{86.68} & \multicolumn{1}{g|}{87.60}                   & \multicolumn{1}{g}{87.60}             & \multicolumn{1}{g|}{87.90}             & \multicolumn{1}{g|}{88.00}                      & \multicolumn{1}{g}{88.00}             & \multicolumn{1}{g}{\underline{88.05}} \\ \cline{1-19}
        Gain     & \multicolumn{1}{c|}{1.18}                    & \multicolumn{1}{c}{1.56}               & \multicolumn{1}{c|}{1.58}               & \multicolumn{1}{c|}{1.19}                  & \multicolumn{1}{c}{1.76}                  & \multicolumn{1}{c|}{1.19}              & \multicolumn{1}{c|}{.65}                       & \multicolumn{1}{c}{1.36}              & \multicolumn{1}{c|}{1.31}           & \multicolumn{1}{c|}{1.67}                      & \multicolumn{1}{c}{1.23}           & \multicolumn{1}{c|}{.74}   & \multicolumn{1}{c|}{1.25}                    & \multicolumn{1}{c}{1.20}              & \multicolumn{1}{c|}{1.19}              & \multicolumn{1}{c|}{1.30}                       & \multicolumn{1}{c}{1.30}              & \multicolumn{1}{c}{1.18}              \\ \cline{1-1}
        Average  & 1.26 ($\sigma$ 0.28)
      \end{tabular}
    }
  }{Gains Ensemble vs Average}
  \vfill
\end{landscape}

To see whether there can be seen a tendency as to which SRL-BERT aligning strategy turns out
to be more effective, the results of the main table \ref{tab:results} are aggregated in table
\ref{tab:token-vs-merged}: For each head, the +SRL results are compared, and the differences are
reported in the table. For example, we look at the [CLS] head on deISEAR $\alpha$ and ask us, which
strategy --- merging the BERT subtokens back ot ``normal'' tokens, or splitting the SRLs up to
align with the subtokenized BERT tokens --- worked better. For this, the difference between both
for +SRL zeros and +SRL duplicate is calculated and controlled for statistical significance between
both ensembles. The subtokenizing strategy (yellow) outperformed the merging strategy (purple) 28
times, 15 times of it statistically significant, while merging was better 20 times, only 15\% of
it significantly. Therefore, at least for the GerGLUE classification datasets, splitting the SRLs
up to align with the BERT subtokens seems more effective than merging the BERT subtokens. This
also makes sense intuitively --- using the merging strategy of averaging the BERT embeddings of
subtokens leads clearly to an information loss or distortion that cannot be fully balanced by the
added SRL information.


\tab{tab:token-vs-merged}{Performance of architectures when BERT \customcolorbox{subtokenized}{yellow} vs. \customcolorbox{merged}{purple}. Both SRL implementations were
                          compared pairwise. Clearly, the subtokenized ensembles showed better classification performances
                          than the merged ones.

                          Example case, upper left 4.64**: Comparison of deISEAR $\alpha$, +SRL zero
                          implementation [CLS] Head from table \ref{tab:results}: Subtokenized ensemble (72.19\% accuracy) and the merged ensemble (67.55\%
                          accuracy) --- the subtokenized ensemble performed 4.64\% better, apparently with quite high significance (p $<$ 5\%).}{
  \scalebox{0.90}{
    \begin{tabular}{llcc|cc|cc}
                                                 &           & \multicolumn{2}{c|}{\textbf{{[}CLS{]} Head}}                                                     & \multicolumn{2}{c|}{\textbf{FFNN Head}}                                                            & \multicolumn{2}{c}{\textbf{GRU Head}}                                                           \\ \cline{3-8}
                                                 &           & \multicolumn{1}{c|}{zeros}                      & \multicolumn{1}{c|}{dupl.}                     & \multicolumn{1}{c|}{zeros}                        & \multicolumn{1}{c|}{dupl.}                     & \multicolumn{1}{c|}{zeros}                    & dupl.                                           \\ \hline\hline
    \multicolumn{1}{c}{\multirow{2}{*}{deISEAR}} & $\alpha$  & \multicolumn{1}{c|}{\cellcolor{yellow} 4.64**}  & \multicolumn{1}{c|}{\cellcolor{purple} .67}    & \multicolumn{1}{c|}{\cellcolor{yellow} 4.63*}     & \multicolumn{1}{c|}{\cellcolor{purple} 1.32}   & \multicolumn{1}{c|}{\cellcolor{yellow} 4.63*} & \multicolumn{1}{c}{\cellcolor{yellow} 2.65}     \\
    \multicolumn{1}{c}{}                         & $\beta$   & \multicolumn{1}{c|}{\cellcolor{yellow} 3.97*}   & \multicolumn{1}{c|}{\cellcolor{yellow} 1.98}   & \multicolumn{1}{c|}{\cellcolor{purple} 5.30*}     & \multicolumn{1}{c|}{\cellcolor{purple} .66}    & \multicolumn{1}{c|}{\cellcolor{yellow} 4.63*} & \multicolumn{1}{c}{\cellcolor{purple} 2.64}     \\ \cline{3-8}
    \multicolumn{1}{c}{\multirow{2}{*}{SCARE}}   & $\alpha$  & \multicolumn{1}{c|}{\cellcolor{purple} 1.13}    & \multicolumn{1}{c|}{\cellcolor{purple} 1.90}   & \multicolumn{1}{c|}{\cellcolor{yellow} 2.27*}     & \multicolumn{1}{c|}{\cellcolor{purple} .38}    & \multicolumn{1}{c|}{\cellcolor{purple} .76}   & \multicolumn{1}{c}{\cellcolor{yellow} .38}      \\
    \multicolumn{1}{c}{}                         & $\beta$   & \multicolumn{1}{c|}{\cellcolor{purple} .38}     & \multicolumn{1}{c|}{\cellcolor{purple} .76}    & \multicolumn{1}{c|}{\cellcolor{yellow} 2.65**}    & \multicolumn{1}{c|}{\cellcolor{yellow} 1.14}   & \multicolumn{1}{c|}{\cellcolor{yellow} 1.89}  & \multicolumn{1}{c}{\cellcolor{purple} 1.52}     \\ \hline
    \multicolumn{1}{c}{\multirow{2}{*}{PAWS-X}}  & $\alpha$  & \multicolumn{1}{c|}{\cellcolor{yellow} .97*}    & \multicolumn{1}{c|}{\cellcolor{purple} 1.02**} & \multicolumn{1}{c|}{\cellcolor{yellow} .76}       & \multicolumn{1}{c|}{\cellcolor{purple} .61}    & \multicolumn{1}{c|}{\cellcolor{purple} .10}   & \multicolumn{1}{c}{\cellcolor{purple} .76}      \\
    \multicolumn{1}{c}{}                         & $\beta$   & \multicolumn{1}{c|}{\cellcolor{yellow} .81**}   & \multicolumn{1}{c|}{\cellcolor{yellow} .21}    & \multicolumn{1}{c|}{\cellcolor{yellow} .81*}      & \multicolumn{1}{c|}{\cellcolor{yellow} .41}    & \multicolumn{1}{c|}{\cellcolor{purple} .40}   & \multicolumn{1}{c}{\cellcolor{purple} .15}      \\ \cline{3-8}
    \multicolumn{1}{c}{\multirow{2}{*}{XNLI}}    & $\alpha$  & \multicolumn{1}{c|}{\cellcolor{yellow} .58**}   & \multicolumn{1}{c|}{\cellcolor{yellow} .38}    & \multicolumn{1}{c|}{\cellcolor{yellow} .34}       & \multicolumn{1}{c|}{\cellcolor{yellow} .42}    & \multicolumn{1}{c|}{\cellcolor{yellow}Â .06}   & \multicolumn{1}{c}{\cellcolor{yellow} .88**}    \\
    \multicolumn{1}{c}{}                         & $\beta$   & \multicolumn{1}{c|}{\cellcolor{yellow} .36}     & \multicolumn{1}{c|}{\cellcolor{purple} 1.52*}  & \multicolumn{1}{c|}{\cellcolor{yellow} .09}       & \multicolumn{1}{c|}{\cellcolor{yellow} 1.33*}  & \multicolumn{1}{c|}{\cellcolor{yellow} 1.51*} & \multicolumn{1}{c}{\cellcolor{purple} 1.51*}    \\
    \end{tabular}
  }
}{Tokenized vs. Merged wo QA}

% \tab{tab:configs}{The different hyperparameter configurations for each dataset.}{
%   \scalebox{0.9}{
%     \begin{tabular}{ll|ccccc}
%                                                    &             & \# of epochs                             & split set up                                & batch size                              & maximum length                 & SRL implementation                          \\ \hline
%     \multicolumn{1}{c}{\multirow{2}{*}{deISEAR}}   &  $\alpha$   & \multicolumn{1}{c}{\multirow{2}{*}{100}} & \multicolumn{1}{c}{\multirow{2}{*}{normal}} & \multicolumn{1}{c}{\multirow{2}{*}{16}} & 40                             & \multicolumn{1}{c}{\multirow{2}{*}{normal}} \\
%     \multicolumn{1}{c}{}                           &  $\beta$    &                                          &                                             &                                         & 200                            &                                             \\
%     \multicolumn{1}{c}{\multirow{2}{*}{SCARE}}     &  $\alpha$   & \multicolumn{1}{c}{\multirow{2}{*}{50}}  & \multicolumn{1}{c}{\multirow{2}{*}{normal}} & \multicolumn{1}{c}{\multirow{2}{*}{16}} & 50                             & \multicolumn{1}{c}{\multirow{2}{*}{normal}} \\
%     \multicolumn{1}{c}{}                           &  $\beta$    &                                          &                                             &                                         & 100                            &                                             \\ \hline
%     \multicolumn{1}{c}{\multirow{2}{*}{PAWS-X}}    &  $\alpha$   & 20                                       & normal                                      & 16                                      & 16                             & normal                                      \\
%     \multicolumn{1}{c}{}                           &  $\beta$    & 20                                       & normal                                      & 16                                      & 16                             & normal                                      \\
%     \multicolumn{1}{c}{\multirow{2}{*}{XNLI}}      &  $\alpha$   & 50                                       & normal                                      & 16                                      & 100                            & normal                                      \\
%     \multicolumn{1}{c}{}                           &  $\beta$    & 50                                       & re-split                                    & 16                                      & 100                            & normal                                      \\ \hline
%     \multicolumn{1}{c}{\multirow{2}{*}{MLQA}}      &  $\alpha$   & \colorbox{blue}{40}                      & \colorbox{blue}{200}                        & \colorbox{blue}{50}                     & \colorbox{blue}{100}           & normal                                      \\
%     \multicolumn{1}{c}{}                           &  $\beta$    & \colorbox{blue}{40}                      & \colorbox{blue}{200}                        & \colorbox{blue}{50}                     & \colorbox{blue}{100}           & normal                                      \\
%     \multicolumn{1}{c}{\multirow{2}{*}{XQuAD}}     &  $\alpha$   & \colorbox{blue}{40}                      & \colorbox{blue}{200}                        & \colorbox{blue}{50}                     & \colorbox{blue}{100}           & 100                                         \\
%     \multicolumn{1}{c}{}                           &  $\beta$    & \colorbox{blue}{40}                      & \colorbox{blue}{200}                        & \colorbox{blue}{50}                     & \colorbox{blue}{100}           & 100                                         \\
%     \end{tabular}
%   }
% }{Dataset specific Configs}

Another accumulating view on the main table \ref{tab:results} is constructed by not just
looking for the best results of a whole row, i.e. over all heads, merging strategies,
and SRL implementations, but instead record the superiority or inferiority of +SRL
compared to $-$SRL for each pairing: Take for example the deISEAR $\alpha$ subtokenized
[CLS] head setting: Without SRLs, the ensemble performance was 71.52\%; now the better
of the two +SRL implementations of this setting is taken into account, in this case
zeroing apparently was better. The difference, .67\% is controlled for significance and
reported in the table. The coloring indicates which SRL implementation performed better
and was taken into account; sometimes both implementations performed equally, which is
also reflected in coloring. Negative numbers indicate, of course, that the head produced
better results when SRL information was \emph{not} added (and this was also controlled
for significance).
Looking at the such accumulated results (table \ref{gain-loss}), no clear picture emerges
at first glance; sometimes zeroing SRLs performed better than duplicating SRLs, sometimes not,
sometimes +SRLs clearly outperforms $-$SRL, even highly statistically, sometimes it is
the other way around adding SRLs seems to create a disadvantage for the head.

\begin{figure}
  \begin{minipage}{0.45\linewidth}
  \vspace{0pt}
    \includegraphics[width=0.9\linewidth]{images/heatmap_subtokenized_eq.pdf}
  \end{minipage}
  \hfill
  \begin{minipage}{0.45\linewidth}
  \vspace{0pt}
    \includegraphics[width=0.9\linewidth]{images/heatmap_merged_eq.pdf}
  \end{minipage}
  \stepcounter{myfigure}
  \caption[Heatmaps subtokenizd and merged]{Gain/loss heat maps from the gain/loss values in table \ref{tab:gain-loss}. \textbf{Left}: Subtokenized. \textbf{Right}: Merged.}
  \label{fig:heatmap-gains}
\end{figure}

In figure \ref{fig:heatmap-gains} the absolute gains and losses are plotted in two heat maps:
In the heat map for the subtokenized settings, the pattern seems to be that for the single
sentence classification tasks there is more fluctuation; especially for deISEAR FFNN, where
for the $\alpha$ setting, there was the biggest gain in all subtokenized experiments, but
also the biggest loss in all subtokenized settings for deISEAR $\beta$. The SCARE [CLS] head
seems also clearly to suffer from subtokenizing SRLs tokens, while gining in other settings.
However, for the sentence pair tasks, subtokenizing mostly adds a smaller, but constant gain,
vizualized by the uniformly distributed purple shades in this dataset rows.

For the merged settings, the correlation axis appears slightly to change: Before, positive or
negative influence of the BERT-SRL combining technique has shown some correlation with datasets
(tendency to lead to slight gains in sentence pair tasks, and irregular behaviour on single
sentence tasks), now while this holds still slightly, it seems that especially the FFNN and GRU
head seem to negativaly affected by merging SRLs, while the [CLS] head shows a trend towards
gaining from it. Note also, that the subtokenized settings showes a greater range of gains and
losses (from $-$4.63 to + 6.62) compared to the merging approach ($-$1.99 to +2.59).

To add one more abstraction layer and see a more complete picture, this table is again
summarized in figure \ref{fig:classification-gains}. Doing this, the image
clarifies, allowing to stipulate that adding SRLs on GerGLUE classification tasks
leads to a measurable improvement. The total gains clearly outweigh the losses, and also
more statistically significant. Regarding the SRL implementation, there is no clear advantage
of one method over the other, only a slight tendency of duplicating being more effective
than zeroing.


\tab{tab:gain-loss}{Ensemble percentage points gains (positive numbers) / losses (negative numbers) for +SRL
                      over $-$SRL for each configuration from table \ref{tab:results}. The better of the +SRL configurations was taken into account:
                      \customcolorbox{zeros}{blue},
                      \customcolorbox{duplicate}{dark-blue}.
                      Light blue denotes that both architectures performed
                      \customcolorbox{equally}{llight-blue}
                      (in which case both ensembles were controlled for significance). One asterisk signifies
                      a $p$-value $<$ 10\%, two stand for $p <$ 5\% and three for $p <$ 1\%.

                      Example case upper left 0.67\%: The better +SRL ensemble for deISEAR $\alpha$ [CLS] head
                      was the zero implementation (72.19\%), surpassing the $-$SRL ensemble (71.52\%) by 0.67
                      percentage point; but apparently not statistically significant.}{
    \scalebox{1}{
      \begin{tabular}{llP{1.5cm}P{1.5cm}P{1.5cm}P{1.5cm}P{1.5cm}P{1.5cm}}
                                                   &           & \multicolumn{2}{c|}{\textbf{{[}CLS{]} Head}}                                                                 & \multicolumn{2}{c|}{\textbf{FFNN Head}}                                                                        & \multicolumn{2}{c}{\textbf{GRU Head}}                                                                     \\ \cline{3-8}
                                                   &           & \multicolumn{1}{c|}{subtok.}                          & \multicolumn{1}{c|}{merged}                          & \multicolumn{1}{c|}{subtok.}                          & \multicolumn{1}{c|}{merged}                            & \multicolumn{1}{c|}{subtok.}                        & \multicolumn{1}{c}{merged}                          \\ \hline\hline
      \multicolumn{1}{c}{\multirow{2}{*}{deISEAR}} & $\alpha$  & \multicolumn{1}{c|}{\cellcolor{blue} .67}             & \multicolumn{1}{c|}{\cellcolor{dark-blue} .00}       & \multicolumn{1}{c|}{\cellcolor{blue} 6.62**}          & \multicolumn{1}{c|}{\cellcolor{dark-blue} .00}         & \multicolumn{1}{c|}{\cellcolor{blue} 4.63**}        & \multicolumn{1}{c}{\cellcolor{dark-blue} $-1.99$}   \\
      \multicolumn{1}{c}{}                         & $\beta$   & \multicolumn{1}{c|}{\cellcolor{llight-blue} 3.31}     & \multicolumn{1}{c|}{\cellcolor{dark-blue} 1.99}      & \multicolumn{1}{c|}{\cellcolor{dark-blue} $-4.63$*}   & \multicolumn{1}{c|}{\cellcolor{blue} $-1.32$}          & \multicolumn{1}{c|}{\cellcolor{blue} 1.32}          & \multicolumn{1}{c}{\cellcolor{dark-blue} $-1.99$}   \\ \cline{3-8}
      \multicolumn{1}{c}{\multirow{2}{*}{SCARE}}   & $\alpha$  & \multicolumn{1}{c|}{\cellcolor{dark-blue} $-1.90$*}   & \multicolumn{1}{c|}{\cellcolor{dark-blue} 2.28*}     & \multicolumn{1}{c|}{\cellcolor{dark-blue} .76}        & \multicolumn{1}{c|}{\cellcolor{dark-blue} .38}         & \multicolumn{1}{c|}{\cellcolor{dark-blue} .38}      & \multicolumn{1}{c}{\cellcolor{blue} $-1.89$}        \\
      \multicolumn{1}{c}{}                         & $\beta$   & \multicolumn{1}{c|}{\cellcolor{blue} $-1.51$**}       & \multicolumn{1}{c|}{\cellcolor{llight-blue} .00}     & \multicolumn{1}{c|}{\cellcolor{blue} $-.76$}          & \multicolumn{1}{c|}{\cellcolor{dark-blue} $-.38$}      & \multicolumn{1}{c|}{\cellcolor{blue} .76}           & \multicolumn{1}{c}{\cellcolor{dark-blue} $-1.13$}   \\ \hline
      \multicolumn{1}{c}{\multirow{2}{*}{PAWS-X}}  & $\alpha$  & \multicolumn{1}{c|}{\cellcolor{blue} .97*}            & \multicolumn{1}{c|}{\cellcolor{dark-blue} 2.59***}   & \multicolumn{1}{c|}{\cellcolor{blue} $-.41$}          & \multicolumn{1}{c|}{\cellcolor{dark-blue} .25}         & \multicolumn{1}{c|}{\cellcolor{llight-blue} .51}     & \multicolumn{1}{c}{\cellcolor{dark-blue} .71}       \\
      \multicolumn{1}{c}{}                         & $\beta$   & \multicolumn{1}{c|}{\cellcolor{dark-blue} .72*}       & \multicolumn{1}{c|}{\cellcolor{dark-blue} .25}       & \multicolumn{1}{c|}{\cellcolor{blue} .56}             & \multicolumn{1}{c|}{\cellcolor{dark-blue} $-1.07$***}  & \multicolumn{1}{c|}{\cellcolor{dark-blue} .30}      & \multicolumn{1}{c}{\cellcolor{dark-blue} .05}       \\ \cline{3-8}
      \multicolumn{1}{c}{\multirow{2}{*}{XNLI}}    & $\alpha$  & \multicolumn{1}{c|}{\cellcolor{blue} .18}             & \multicolumn{1}{c|}{\cellcolor{blue} $-.22$}         & \multicolumn{1}{c|}{\cellcolor{blue} .22}             & \multicolumn{1}{c|}{\cellcolor{blue} .70*}             & \multicolumn{1}{c|}{\cellcolor{dark-blue} .46}      & \multicolumn{1}{c}{\cellcolor{dark-blue} .12}       \\
      \multicolumn{1}{c}{}                         & $\beta$   & \multicolumn{1}{c|}{\cellcolor{blue} .09}             & \multicolumn{1}{c|}{\cellcolor{dark-blue} .44}       & \multicolumn{1}{c|}{\cellcolor{dark-blue} .09}        & \multicolumn{1}{c|}{\cellcolor{blue} .53}              & \multicolumn{1}{c|}{\cellcolor{blue} .98}           & \multicolumn{1}{c}{\cellcolor{dark-blue} .89}       \\ % \hline\hline
      % & \multicolumn{2}{l}{Accumulation}                  \\ \cline{2-4}
      % & \multicolumn{2}{l}{\textbf{Gains} (significant)}  & 32 (5) \\
      % & \multicolumn{2}{l}{\textbf{Losses} (significant)} & 15 (3) \\
     \end{tabular}
    }
  }{Gain-Loss}



\begin{figure}
  \begin{minipage}{0.45\linewidth}
  \vspace{0pt}
    \includegraphics[width=0.9\linewidth]{images/gain_losses_accumulated.pdf}
  \end{minipage}
  \hfill
  \begin{minipage}{0.45\linewidth}
  \vspace{0pt}
    \includegraphics[width=0.9\linewidth]{images/gain_losses_SRL.pdf}
  \end{minipage}
  \stepcounter{myfigure}
  \caption[Accumulated Gains and Losses.]{Accumulated scores from table \ref{tab:gain-loss}. \textbf{Left}: Which SRL mode performed stronger out of a total of 48 settings. The bars indicate a slight outperforming of duplicating SRLs instead of adding zeros. \textbf{Right}: Counts for Gains and Losses in all 48 settings. Darker shades indicate significant results. The light green tip stands for settings where the gain equals 0.00.}
  \label{fig:classification-gains}
\end{figure}

As described in chapter \ref{chap:3_datasets}, the datasets compiled in GerGLUE are a quite
heterogeneous assemble: Some datasets like deISEAR and, especially, SCARE comprise rather
informal, colloquial tests, while PAWS-X and XNLI consists of more technical, standardized text.
Further, as depicted in figure \ref{fig:num_predarg_structs}, over 30\% of the sentences
in SCARE don't have any predicate-argument structures --- in other words there is no exploitable SRL
information for the model.

In figure \ref{fig:each-dataset-gains}, the same accumulation process as for the general overview
% in figure \ref{fig:classification-gains} is conducted for each dataset. The following observations
can be made dataset-wise: For each double row --- the $\alpha$ and $\beta$ runs ---, the gains and
losses are added togehter, and the significant ones are highlighted.

The positive effect of enriching BERT embeddings with SRLs is especially evident for
the sentence pair classification tasks: PAWS-X has the highest number of experiments where a significant
improvement over vanilla BERT embeddings could be measured, and has overall clearly gained from adding
SRLs. While XNLI has only one significant improved experiment, the overall number of experiments
where a positive effect was measured outweighs the negative one by far.

For the single sentence tasks, however, the picture somewhat darkens. While there were some
significant improvements for deISEAR, there were also quite some experiments where adding SRLs
seems to have weakend the model. Generally, deISEAR is the dataset that shows the strongest
fluctuations ranging from 67.55 to 77.48\% --- a range of staggering 9.93\%. It is not clear
to me why especially deISEAR shows this behaviour, but the relative small amount of examples
in the dataset could indicate that it is more prone to unlucky random initializations which
could be countered with more examples for the models to extrapolate from.
For SCARE, there is no clear positive effect measurable at all
--- in contrary, the significant deteriorations outnumber the significant gains. However, this
is not too surprising taking into account the afore mentioned fact of predicate scarceness in
SCARE (or at least, ParZu couldn't detect any). Further, the register is highly colloquial with
very frequent deviations from standard orthography etc. which all probably leads to a collapsing
of the already not too stable DAMESRL system.


\fig{images/all_datasets_gain_dup.pdf}{fig:each-dataset-gains}{Accumulation of statistics for each classification
                                       dataset \textbf{Left}: Which SRL architecture performed better.
                                       \textbf{Right}: Comparison of accuray points gained/lost after adding SRLs. Darker Colors indicate significant runs; light green stands for a neutral run, i.e. where adding SRLs information performed equally to the vanilla architecture}{15}{Results Accumulation for each Dataset}

In figure \ref{fig:each-head-gains} the accumulation of gains and losses is done head-wise; i.e.
for both BERT-SRL combination settings --- subtokenized and merged ---, the gains and losses
are added together and the significant ones are indicated. The [CLS] head amasses the most
significant gains, while at the same time having led to 2 significant losses.
Clearly, the FFNN head shows the weakest performance of the tested heads; the overall losses are the heighest
of all the heads and the significant gains and losses are on par. The GRU head accumulated the
most gains but only one of them being significant.

\fig{images/all_heads_gain_dup.pdf}{fig:each-head-gains}{Accumulation of statistics for each head. \textbf{Left}: Which SRL architecture performed better.
                                       \textbf{Right}: Comparison of accuray points gained/lost after adding SRLs. Darker Colors indicate significant runs; light green stands for a neutral run, i.e. where adding SRLs information performed equally to the vanilla architecture}{15}{Results Accumulation for each Dataset}

One might ask if a simple accuracy is the right measure for all datasets, since
it is often common to compute other metrics, especially if class imbalance is
a known property of datasets. There are two main reasons as to why I chose to
stay with simple accuracy: (1) I am not interested in building a classifier
which explicitely handles such a problem, e.g. spam detection tasks. In such
cases, an F1-score of course is more meaningful because it is more sensibel to
predictions on underrepresented but task-important classes (such as spam). (2)
As reported in chapter \ref{chap:3_datasets}, most of my datasets do not show a
drastic class impbalance. To further substantiate this, I compute some metrics
for the most significantly imbalanced dataset, namely SCARE: A brute force
``stupid'' prediction of the majority class on the test set would results in an
accuracy of 59.09\% and a macro F1 of 24.76. The example confusion of an actual
experiment is shown in table \ref{tab:confusion-scare}: Altough the algorithm
overprefers the majority (``Positive'') class, it also predicts to some extent
the other ones. The measures resulting from this experiment are an accuracy of
83.71\% compared to the macro F1 of 73.52. Therefore, although the accuracy may
distort the picture to some extent, this is negligible in the context of this
thesis.


\tab{tab:confusion-scare}{Confusion matrix for SCARE $\alpha$ +SRL merged duplicated [CLS] head.}{
  \begin{tabular}{|ll|ccc|}
    \hline
                                                    &           & \multicolumn{3}{c|}{Predicted} \\
                                                    &           & Positive & Neutral & Negative  \\ \hline
    \multirow{3}{*}{\rotatebox[origin=c]{90}{True}} & Positive  & 149      & 12      & 5         \\
                                                    & Neutral   & 4        & 11      & 6         \\
                                                    & Negative  & 3        & 8       & 66        \\ \hline
  \end{tabular}
}{Confusion matrix for one SCARE +SRL ensemble}


\section{Question Answering Dataset Results}
\label{sec:qa-results}

The reportings of the results for the question answering data sets follows
the line that was taken for the classification sets: A general table with the
ensemble test set results is reported in \ref{tab:results-qa}. But the verdict is far more
devastating: 3 out of 4 best results were achieved by models implementing vanilla BERT
embeddings and 3 out of 4 times the worst performance was measured for +SRL settings.

\tab{tab:results-qa}{Test set accuracy ensemble results (per 5 models) on question answering tasks. \textbf{Bold} font marks the best result per line, \underline{underline} the second best, and \textit{italics} the poorest.}{
  \scalebox{1}{
    \begin{tabular}{llccc|ccc}
                                               &           & \multicolumn{6}{c}{\large \textbf{Q\&A Datasets}}  \\ \\
                                               &           & \multicolumn{6}{c}{\textbf{Span Prediction Head}}                                                                                                                                                                                                \\ \cline{3-8}
                                               &           & \multicolumn{3}{c|}{subtokenized}                                                                                                & \multicolumn{3}{c}{subtokens merged}                                                                          \\ \cline{3-8}
                                               &           & \multicolumn{1}{c|}{$-$SRL}                     & \multicolumn{2}{c|}{+SRL}                                                      & \multicolumn{1}{c|}{$-$SRL}         & \multicolumn{2}{c}{+SRL}                                                \\ %\cline{3-4}\cline{6-7}
                                               &           & \multicolumn{1}{c|}{}                           & \multicolumn{1}{c}{zeros}             & \multicolumn{1}{c|}{dupl.}             & \multicolumn{1}{c|}{}               & \multicolumn{1}{c}{zeros}          & dupl.                              \\ \hline\hline
    \multicolumn{1}{c}{\multirow{2}{*}{MLQA}}  & $\alpha$  & \multicolumn{1}{c|}{\textbf{30.69}}             & \multicolumn{1}{c}{\underline{29.68}} & \multicolumn{1}{c|}{\underline{29.68}} & \multicolumn{1}{c|}{21.92}          & \multicolumn{1}{c}{21.92}          & \multicolumn{1}{c}{\textit{21.81}} \\
    \multicolumn{1}{c}{}                       & $\beta$   & \multicolumn{1}{c|}{\textbf{44.75}}             & \multicolumn{1}{c}{\underline{44.55}} & \multicolumn{1}{c|}{43.41}             & \multicolumn{1}{c|}{\textit{41.66}} & \multicolumn{1}{c}{41.86}          & \multicolumn{1}{c}{41.79}          \\ \cline{3-8}
    \multicolumn{1}{c}{\multirow{2}{*}{XQuAD}} & $\alpha$  & \multicolumn{1}{c|}{\textbf{42.01}}             & \multicolumn{1}{c}{\underline{41.42}} & \multicolumn{1}{c|}{41.12}             & \multicolumn{1}{c|}{37.87}          & \multicolumn{1}{c}{36.98}          & \multicolumn{1}{c}{\textit{35.50}} \\
    \multicolumn{1}{c}{}                       & $\beta$   & \multicolumn{1}{c|}{\underline{46.57}}          & \multicolumn{1}{c}{45.43}             & \multicolumn{1}{c|}{\textbf{46.86}}    & \multicolumn{1}{c|}{37.43}          & \multicolumn{1}{c}{\textit{37.14}} & \multicolumn{1}{c}{39.14}          \\ \hline\hline
    \multicolumn{1}{c}{Scores}                 &           & \multicolumn{1}{c|}{\textbf{III} \underline{I}} & \multicolumn{2}{c|}{\textbf{I} \underline{III}}                                & \multicolumn{1}{c|}{}               & \multicolumn{2}{c}{}                                                    \\ \cline{1-2}
    \multicolumn{1}{c}{+SRL}                   & \multicolumn{3}{l}{\textbf{1} \underline{3}} \\
    \multicolumn{1}{c}{$-$SRL}                 & \multicolumn{3}{l}{\textbf{3} \underline{1}}

    \end{tabular}
  }
}{Results-QA}

When accumulated for unfolding subtokenized vs. merged patterns, there emerges a surprisingly
clear tendency (see left side of table \ref{tab:qa-gain-loss-token-merged}): Splitting SRLs according to
the BERT subtokenization always showed stronger results than BERT merging. 5 times out of the
total 8 settings the differences were even highly significant with $p < 1\%$ and one time
moderately significant with $p < 5\%$. The reason for this rather {\color{red} drastic pattern
remains unclear to me}; despite tediously controlling the merging algorithm and validating the
re-computing of start and end indices, I could not make out any error.

Also for the Q\&A datasets, I compare setting-wise the performance of $-$SRL against the better +SRL experiment
(see right side of table \ref{tab:qa-gain-loss-token-merged}): The only significant result is the MLQA $\alpha$
subtokenized setting, where the SRL-enriching led to a loss of 1.01\%. The other results are mixed, with $-$SRL
performing 3 times better than +SRL.
The only dataset hyperparameter configuration where for both SRL combination methods the +SRL implementation
performed better was XQuAD $\beta$ --- however in both cases unsignificant.


\tab{tab:qa-gain-loss-token-merged}{\textbf{Left part}: Ensemble percentage points gains (positive numbers) / losses (negative numbers) for +SRL
                       over $-$SRL for the Span Prediction Head from table \ref{tab:results-qa}. The better of the +SRL configurations was taken into
                       account: \customcolorbox{zeros}{blue},
                       \customcolorbox{duplicate}{dark-blue}.
                       Light blue denotes that both architectures performed \customcolorbox{equally}{llight-blue}
                       (in which case both ensembles were controlled for significance).
                       One asterisk signifies a $p$-value $<$ 10\%, two stand for $p <$ 5\% and three for $p <$ 1\%.
                       \textbf{Right part}: Performance of architectures when BERT \customcolorbox{subtokenized}{yellow} vs. \customcolorbox{merged}{purple}. Both SRL implementations were
                          compared pairwise..}{
  \scalebox{1}{
    \begin{tabular}{llP{2cm}P{2cm}cP{2cm}P{2cm}}
                                                 &           & \multicolumn{5}{c}{\textbf{Span Prediction Head}}                                                                                                                                                                   \\
                                                 &           & \multicolumn{2}{c}{Gains/Losses}                                                                            &  & \multicolumn{2}{c}{subtokenized/merged}                                                            \\ \cline{3-4} \cline{6-7}
                                                 &           & \multicolumn{1}{P{1.5cm}|}{subtok.}                            & \multicolumn{1}{P{1.5cm}}{merged}                        &  & \multicolumn{1}{P{1.5cm}|}{zeros}                       & \multicolumn{1}{P{1.5cm}}{dupl.}                       \\ \hline\hline
    \multicolumn{1}{P{1.5cm}}{\multirow{2}{*}{MLQA}}    & $\alpha$  & \multicolumn{1}{P{1.5cm}|}{\cellcolor{llight-blue} $-$1.01***} & \multicolumn{1}{P{1.5cm}}{\cellcolor{blue} .00}          &  & \multicolumn{1}{P{1.5cm}|}{\cellcolor{yellow} 7.76***}  & \multicolumn{1}{P{1.5cm}}{\cellcolor{yellow} 7.86***}  \\
    \multicolumn{1}{P{1.5cm}}{}                         & $\beta$   & \multicolumn{1}{P{1.5cm}|}{\cellcolor{blue} $-$.20}            & \multicolumn{1}{P{1.5cm}}{\cellcolor{blue} .20}          &  & \multicolumn{1}{P{1.5cm}|}{\cellcolor{yellow} 2.69}     & \multicolumn{1}{P{1.5cm}}{\cellcolor{yellow} 1.62}     \\ \cline{3-4} \cline{6-7}
    \multicolumn{1}{P{1.5cm}}{\multirow{2}{*}{XQuAD}}   & $\alpha$  & \multicolumn{1}{P{1.5cm}|}{\cellcolor{blue} $-$.59}            & \multicolumn{1}{P{1.5cm}}{\cellcolor{blue} $-$.89}         &  & \multicolumn{1}{P{1.5cm}|}{\cellcolor{yellow} 4.44**}   & \multicolumn{1}{P{1.5cm}}{\cellcolor{yellow} 5.62***}  \\
    \multicolumn{1}{P{1.5cm}}{}                         & $\beta$   & \multicolumn{1}{P{1.5cm}|}{\cellcolor{dark-blue} .29}          & \multicolumn{1}{P{1.5cm}}{\cellcolor{dark-blue} 1.71}    &  & \multicolumn{1}{P{1.5cm}|}{\cellcolor{yellow} 8.29***}  & \multicolumn{1}{P{1.5cm}}{\cellcolor{yellow} 7.72***}  \\
   \end{tabular}
  }
}{QA Gain-Loss / QA Tokenized vs. merged}


Therefore, the overall aggregation in figure \ref{fig:qa-tot-gains} does not look too promising for the
research question of this thesis: For the two question answering datasets in GerGLUE, the effect of enriching
BERT embeddings with SRL information seems to rather harm the model instead of adding helpful information.
Interestingly, this contrasts with the findings of \citeauthor{zhang2019semantics}, which reported a gain
of 4.3\% in accuracy on the SQuAD 2.0 set for their SemBERT implementation.\myfootnote{However, a ``synthetic
self training'' technique for this results is mentioned which is not elaborated upon any more. For their
``regular'' SemBERT large implementation the reported gain is 1.9\% --- which still would stand
for a positive influence of SRL enriching which is not reflected in my experiments.}

\begin{figure}
  \begin{minipage}{0.45\linewidth}
  \vspace{0pt}
    \includegraphics[width=0.9\linewidth]{images/QA_gain_losses_accumulated.pdf}
  \end{minipage}
  \hfill
  \begin{minipage}{0.45\linewidth}
  \vspace{0pt}
    \includegraphics[width=0.9\linewidth]{images/QA_gain_losses_SRL2.pdf}
  \end{minipage}
  \stepcounter{myfigure}
  \caption[Accumulated Gains and Losses.]{Accumulated scores from table \ref{tab:qa-gain-loss}. \textbf{Left}: Which SRL mode performed stronger out of a total of 8 settings. The bars indicate a slight outperforming of duplicating SRLs instead of adding zeros. \textbf{Right}: Counts for Gains and Losses in all 8 settings. Darker shades indicate significant results. The light green tip stands for settings where the gain equals 0.00.}
  \label{fig:qa-tot-gains}
\end{figure}

One of the main differences between the classification and question answering datasets is the size of
the examples: While for the classifications task the model needs to extract semantic
information out of a handful sentences, for the question answering tasks thi is rather the exception ---
normally the context from which the answer span must be extracted is multiple sentences long. My suspicion
is that SRL information can highlight some crucial semantic relations present in a sequence which would be
hard for the vanilla BERT to extract; mabye because it is in marked passive voice, not obiously negated etc.
However, for extracting answer spans out of a long text sequence consisting of several sentences, each having
potentially 3 predicate-argument structures present, the SRL information may not display a helpful signal to the
head, which then relies mor on information present in the vanilla BERT subtokens. This hypothesis is further
substantiated by the analyis of BERT-SRL combination strategy superiority, depicted in table \ref{tab:qa-gain-loss-token-merged}:
Maintaining the BERT inherent subtokenization structure outperforms the merging approach by far, indicating that
the head relies heavily on information encoded in the BERT subtokens.






Main conclusions:

\begin{itemize}
  \item SRLs in tendency helpful
  \item not for Q\&A
  \item results somewhat unclear
\end{itemize}


\section{GliBERT Noise Nuisance Analysis}
\label{sec:glinoise}


After reporting the general results, aggregations and the somewhat disenchanting conclusion, I
now try to investigate the reasons for this mediocre effect. After investigating the data and
SRL quality, I found that noise on several levels is present which probably led to too much
randomness in the complex functions the models needed to approximate. I classify the observed
data irregularites into the following \textbf{noise} categories:


\begin{itemize}
  \item[\textbf{register}] The textual styles vary greatly from utilizing complex,
                           hypotactic sentence structures (e.g. XQuAD), to highly
                           informal, elliptic --- even erratic --- structures
                           (e.g. SCARE).
 \item[\textbf{data set}] Many of my datasets were constructed either automatically
                         (e.g. scrambling text automatically to create paraphrase pairs)
                         or employing crowd-sourcing techniques. Either way, the process
                         is prone to erros. There are, e.g., 84 sentence pairs in the trainig
                         set of PAWS-X that are 100\% identical, yet labelled as non-paraphrases.
 \item[\textbf{translation}] Due to the mostly employed semi-automatic translation
                             approach for creating the various datasets, errors
                             have been introduced into the data ranging from typical
                             translation errors (e.g. English ``bishop'' in the clerical
                             context translated to the German chess figure counterpart
                             ``LÃ¤ufer'', not ``Bischof'') to eventually wrongly copied
                             labels, since the overall meaning changed during
                             the tranlsation process (e.g. a sentence pair is no more
                             contradictive but neutral), thus creating label noise.
 \item[\textbf{SRL}] The SRLs obtained from DAMESRL are, conservatively formulated,
                     questionable in their quality (e.g. modifiers are completely missing).
\end{itemize}

While it is normal for data in NLP to possess noise in some way or the other, this usually does
not impact the overall performance of a model trainied on it in a severe way, since information
in language is encoded redundantly, and noise present in one channel can be compensated for by
uncorrupted information from another channel.\myfootnote{For example, information about which
constituent takes the syntactic function of the subject is often not marked by using only one
possibility, e.g. though a case ending. Often, several strategies are present, as in German: The
subject is marked morphologically through case endigs (which are not always alient), positionally
(in unmarked contexts), and structurally (in unmarked contexts, normally the proto-agent is
realized as subject) (cf. \cite{bussmann2006routledge,jaeger2010redundancy})} However, if noise
is present on too many channels at the same time, reliably recovering structural information may
break down to some extent. In short --- the good old GIGO concept from informatics holds mutatis
mutandis also in NLU, and NLP in general.



\subsection{Register Noise}
\label{sec:register-noise}

German BERT, according to deepset, was pretrained on the German Wikipedia dump, the OpenLegalData dump and
news articles. All of this corpora comprise grammatical, to the standards of orthography adhering text
belonging to a rather formal, ``professional'' register. Three of the six datasets in GerGLUE, namely
deISEAR, SCARE, and XNLI, however, can be described as representing a rather colloquial register of
language, even displaying feautures of transcribed speech as e.g. in XNLI:

\begin{examples}
  \item Ja und wir haben es irgendwie behalten, Ã¤h irgendwie Ã¤h, es war eine Ãœberraschungs-Geburtstagsparty fÃ¼r sie, das war es Sie liebte die Ãœberraschungsfeier.
\end{examples}

Naturally, since the German BERT module probably wasn't confronted with texts coming from such a register,
the BERT embeddings will probably conatin some noise regarding the classification task at hand. Also, ParZu
is aimed at parsing ``clean'', correct sentences, showing of course problems at processing examples as the
one above, which leads to the known error propagation and amplification through DAMESRL.

In other words, it is expectable for GliBERT to show difficulties on the aforesaid datasets, which
was also observed during the analyses of the results in section \ref{sec:classification-results}.

Interestingly, the discrepancy between the German BERT pretraining text register and the text
present in some GerGLUE datasets, the analysis of the German BERT tokenizetion statistics shows
that also in those problematic datasets, there are very little out of vocabulary items, which
would be represented by the catch-all [UNK] BERT token:

\fig{images/all_data_sets_token_stats.pdf}{fig:all-data-sets-token-stats}{Percentages of token-types in all datasets.
                                                                          \#\# Subtokens represent the amount of tokens that
                                                                          get re-merged in the merged settings (e.g. ``Master''
                                                                          ``\#\#arbeit'' $\rightarrow$ ``Masterarbeit''. The amount
                                                                          of tokens that lie outside of the German BERT vocabulary is
                                                                          in all datasets extremely small (for deISEAR and XNLI
                                                                          there are no [UNK]s at all); the largest shares of such tokens
                                                                          are present in MLQA and XQuAD with .79\% and .73\%,
                                                                          respectively.}{14}{Token Types all Datasets}



\subsection{Data Set Noise}
\label{sec:label-noise}

As \citep{caswell2021quality} point out, apparently there is often a lack of
prudence observable when multilingual datasets get compiled --- especially for
low resoursce languages they report devastating discoveries of ungrammatical
and even non-sense texts for these.


However, especiall for sentence pair datasets, one of the sentences needs to be constructed. This
can happen by automaticall construct these sentences (PAWS-X) or let humans come up with those
sentences (as for XNLI). In both cases, there might happen errors, and the label assigned to a
sentence pair may be actually wrong. This is obvious for PAWS-X, where e.g. in the training set,
there are 84 identical sentence pairs are labelled as non-paraphrases, which is obviously false,
since the other 3,125 exact identical sentence pairs are labelled as paraphrases. E.g. sentence pair
number 45061 (in the original English PAWS and the German PAWS-X datasets), both labelled as
non-paraphrases:

\begin{examples}
  \item Riverton was a parliamentary electorate in the New Zealand region of Southland .\\
        Riverton was a parliamentary electorate in the New Zealand region of Southland .

  \item Riverton war ein ParlamentswÃ¤hler in der neuseelÃ¤ndischen Region Southland.\\
        Riverton war ein ParlamentswÃ¤hler in der neuseelÃ¤ndischen Region Southland.
\end{examples}

But also for human created sentence pairs, there
is a chance that the gold labels are not that clear. Further, thorugh the creation of the German
datasets by means of semi-automatically translating English datasets, additional tranlsation
noise might additionally pollute the data.

To examine this more systematically for XNLI and PAWS-X, I conduct a relabelling of 20 random
examples each  where the predictions of the best model contradicted the gold labels in the dataset.


\subsubsection{Re-annotation}

\paragraph*{PAWS-X}

For the 20 re-annotated examples in PAWS-X the Fleiss' $\kappa$ of 0.68 indicates ``substantial'' agreement
between the two human annotators; however, if the coefficient is computed between the two human annotators
and the gold labels of those 20 examples, the value drops to 0.35, indicating substantive disagreement
as to what sentence pairs are to be considered paraphrases and which don't.

Even more interestingly, in 30\% of the considered cases, the model and the human annotators unanimously
disagree with the gold labels present in PAWS-X:

% 2 human annotators re-label 20 examples of PAWS-X where gold != predicted.
% Fleiss' $\kappa$ between 2 annotators: 0.68
% Fleiss' $\kappa$ between 2 annotators and gold: 0.3541
% Fleiss' 4\kappa$ between all $-$0.07

% 6 examples where both annotators agree with predictions, disagree with gold:

\begin{examples}
  \item Der NVIDIA TITAN V wurde von Nvidia am 7. Dezember 2017 offiziell angekÃ¼ndigt.\\
        Am 07. Dezember 2017, verkÃ¼ndete NVIDIA offiziell Nvidia TITAN V.

        humans \& model: False, Gold: True
  \item Die SchÃ¤fte sind sehr kurz oder oft nicht vorhanden.\\
        Es sind entweder wenig Landschaften vorhanden oder sie fehlen in den meisten FÃ¤llen.

        humans \& model: False, Gold: True
  \item 1963 trat Roy der Kommunistischen Partei Indiens bei und leitete Gewerkschaftsbewegungen in Bansdroni in Kalkutta.\\
        Roy trat 1963 der Kommunistischen Partei Indiens bei und leitete Gewerkschaftsbewegungen im Kolkata-Gebiet von Bansdroni.

        humans \& model: True, Gold:False
  \item Der Kanal ist einer der Ã¤ltesten schiffbaren KanÃ¤le Europas und sogar Belgiens.\\
        Der Kanal ist einer der Ã¤ltesten befahrbaren KanÃ¤le in Belgien und Europa.

        humans \& model: True, Gold:False
  \item Propilidium pelseneeri ist eine Art der Meeresschnecken, eine wahre Napfschnecke und Gastropoden-Mollusk in der Familie der Lepetidae.\\
        Propilidium pelseneeri ist eine Art der Meeresschnecken, eine wahre Napfschnecke und Meeres-Gastropoden-Mollusk der Familie der Lepetidae.

        humans \& model: True, Gold:False
  \item Die Chicago Bears sanken auf die Giants 27:21, und verloren 0:6 zum ersten Mal seit 1976.\\
        Die Chicago Bears verloren 21:27 gegen die Bears und standen erstmals seit 1976 bei 0:6.

        humans \& model: False, Gold:True
\end{examples}

In all of this cases it seems to be the case that the given gold labels actually are false,
leading to the conclusion that the evaluation and measured performance on such a dataset must
be taken with caution, and not just blindly accepted as ultimate truth.


{% PAWS-X; different repair-strategies $\rightarrow$ different labels (gold: false)

% Sawyers autorisierte Biografie wurde 2014 von Huston Smith verÃ¶ffentlicht.
% Im Jahr 2014 wurde Huston Smith eine autorisierte Biographie von Sawyer verÃ¶ffentlicht.


% Im Jahr 2014 wurde {\color{red} Â«}Huston Smith{\color{red} Â», } eine autorisierte Biographie von Sawyer{\color{red} ,} verÃ¶ffentlicht.

% Im Jahr 2014 wurde {\color{red} von|fÃ¼r|durch|trotz|wegen} Huston Smith eine autorisierte Biographie von Sawyer verÃ¶ffentlicht.

% Im Jahr 2014 wurde Huston Smith eine autorisierte Biographie von Sawyer verÃ¶ffentlicht.


\paragraph*{XNLI}

For XNLI, where in contrast to PAWS-X each example can have one of three labels instead of
binary classification, the findings differ to some extent: While the agreement only between
the two re-annotators is measured as a low Fleiss' $\kappa$ of 0.36, it raises to 0.48
when the gold labels are also taken into account, indicating ``moderate agreement''. For
one example the humans and model were on the same page regarding the label but disagreed
with the gold label:

% 2 human annotators re-label 20 examples of XNLI where gold != predicted.
% Fleiss' $\kappa$ between 2 annotators: 0.36
% Fleiss' $\kappa$ between 2 annotators and gold: 0.4787
% Fleiss' 4\kappa$ between all 0.08

% 1 example where 2 humans == model and 2 humans != Gold

\begin{examples}
  \item Bato ist ein Jahrhunderte altes Wort, das man als Kerl oder Kumpel Ã¼bersetzen kann.\\
        Bato (oder Vato) ist ein spanisches Wort, das Typ oder Typ bedeutet.

        humans \& model: neutral, Gold: Entailment
\end{examples}

% 1 example where 2 humans != model and 2 humans != Gold
Since the notion of ``spanisch'' is only encountered in the hypothesis but missing from the premise,
I would argue that the humans and the model are right about the ``neutral'' label and the gold label
is false.
In another example, all three ``agents'' --- the two humans, the model, and the gold labels --- disagreed:

\begin{examples}
  \item Oh, ich sehe oh der Staat braucht es nicht gut, das ist eher das, das ist eher ungewÃ¶hnlich, nicht wahr?\\
        Das macht Sinn, dass der Staat es benÃ¶tigt.

        humans: neutral, model: entailment, Gold: contradiction
\end{examples}

The latter is also a further exemplification of the register diversity in XNLI: The premise above seems as if it
was a transcription of an utterance torn out of the context an actual conversation; because of that for a human
reader there is essential information missing, making it a rather strange and vague case for attributing
a specific entailment regarding any hypothesis.


Both re-labellings have led to the finding that the quality of semi-automatically
created datasets is often doubtful: (1) As the many ``neutral'' re-labellings of
XNLI examples by the human annotators indicate, often there seems not be a clear
preference towards a certain label --- rather, as in the examples above, there is
too much ambiguity and lacking of needed information around to make a clear decision
of the relation between those sentences. (2) Further, the predefined labels in such
datasets are by no means to be taken as undoubtedly indicating true classifications.



\subsection{Translation Noise}
\label{sec:translation-noise}

Automatic translation has come a long way and systems nowadays produces often qualitatively high results ---
at least for translation between high resource languages, such as English and German.
But still, especially for delicate, subtle expressions, errors can happen and a (partial) mistranslation
is produced. While this potentially leads to tranlation artifacts, which may confuse the model, there
also exists the possibility of additionally introducing label noise.

Take for example the following XNLI English original premise-hypothesis pair and its automaticall
translated German counterparts:

\begin{examples}
  \item and that's a lot of it is due to the fact that the mothers are on drugs\\
        The mothers take drugs.

  \item Und vieles davon liegt daran, dass die MÃ¼tter Medikamente nehmen.\\
        Die MÃ¼tter nehmen Drogen.
\end{examples}

The english term ``drug'' indeed is an ambigous word, which either can refer to a pharmaceutical
product that is used as medicine or to an abusively taken substance (which may be of the first
type).\myfootnote{\url{https://dictionary.cambridge.org/dictionary/english/drug}} However, since
the label of the original sentence pair is \emph{entailment}, the term would have needed
to be translated in the same meaning both times. However, this did not happen, and thus
a wrongly labelled training instance was created.



\subsection{SRL Noise}
\label{sec:srl-noise}

A major question arising in the context of using automatically assigned Semantic Roles in
downstream tasks, is how precise these predicted Semantic Role are. Since there is no gold standard
available for Semantic Role Labels for the datasets I use in my experiments, there is no
straight-forward way to evaluate their quality {\color{red} automatically}. In contrast to
other tagging tasks like POS prediction or NER, Semantic Roles are not as black and white:
While it is relatively easy to decide if a predicted POS tag is correct or incorrect, it
{\color{red} is more a scale} concerning SRLs.

As for the data quality / label noise assessment, I conduct an human evaluation study
for determining the SRL quality: Three people evaluated the SRL quality of examples regarding
whether they estimate them to be helpful, neutral, or harmful to the model for solving
the task at hand.\myfootnote{For single sentence tasks, 20 examples were randomly sampled, for
sentence pair tasks 10 examples, and for question answering tasks 2 examples (since the context
comprises often umpteen sentences.}

\fig{images/SRL_assessment.pdf}{fig:SRL-assessment}{Independent evaluation of SRL quality by three people. Regardless of the label attributed to each example, it is obvious, that the total amount of sentences for which the annotators evaluated the corresponding semanti roles as \emph{helpful}, is relatively stable.}{11}{SRL assessment}

In figure \ref{fig:SRL-assessment} the aggregated evaluations per human are summed up:
Evidently, the predicted SRLs were mostly estimated to be whether of great help to the model
nor severely distract it. It has to be noted, however, that the inter-annotator agreement between
the the three people was very low, the computed Fleiss' $\kappa$ was only 0.20 --- indicating
an agreement slightly above randomness.

If the annotations are summed up over each dataset, as in figure \ref{fig:data-set-SRL-assessment}, this
impression gets tightend: Only a small subset of the usefulness attributions were unisono (indicated
by darker color shades).

\fig{images/SRL_assessment_data_set.pdf}{fig:data-set-SRL-assessment}{Estimated quality of SRLs per dataset.}{15}{SRL assessment per datasets}

Interestingly, for PAWS-X was comparatively high agreement between the annotators that the SRLs would
rather harm the model, however, as is hown in the right figure of \ref{fig:classification-gains}, PAWS-X
is one of the datasets were the performance gain from adding SRL information was rather clear.



% Fleiss' $\kappa$ = 0.2048 --- this slightly above the threshold of Â«fair agreementÂ», as defined by \citep{landis1977measurement} (0.20).

% The $\kappa$ for helpful vs. other is even worse: 0.1944

% for individual datasets:

% deISEAR: 0.0814

% SCARE: 0.2401

% PAWS-X: 0.1245

% XNLI: 0.2475

% MLQA: -0.3636

% XQuAD: -0.5



\cite{do2018flexible}


\section{Ablation study}
\label{sec:ablation}

To be able to make substantial claims about the positive influence about a new algorithm over an
established one, it is common ground to conduct an ablation study. In such a study, one tries to
determine which aspects of the proposed architecture contribute how much to the overall performance
gain (or loss, respectively).

\begin{minipage}{1.0\linewidth}
  \begin{srl}
  \centering
    \begin{BVerbatim}[commandchars=\\\{\}, fontsize=\footnotesize]
      Ich        \colorbox{llight-blue}{B-A0}         \colorbox{white}{O}
      weiÃŸ       \colorbox{blue}{B-V}          \colorbox{white}{O}
      nicht      \colorbox{white}{O}            \colorbox{white}{O}
      ob         \colorbox{llight-blue}{B-A1}         \colorbox{white}{O}
      er         \colorbox{llight-blue}{I-A1}         \colorbox{llight-blue}{B-A0}
      danach     \colorbox{llight-blue}{I-A1}         \colorbox{white}{O}
      in         \colorbox{llight-blue}{I-A1}         \colorbox{llight-blue}{B-A1}
      Augusta    \colorbox{llight-blue}{I-A1}         \colorbox{llight-blue}{I-A1}
      geblieben  \colorbox{llight-blue}{I-A1}         \colorbox{blue}{B-V}
      ist        \colorbox{llight-blue}{I-A1}         \colorbox{white}{O}
      .          \colorbox{white}{O}            \colorbox{white}{O}
      ==============================
      Er         \colorbox{llight-blue}{B-A0}
      wohnte     \colorbox{blue}{B-V}
      weiterhin  \colorbox{white}{O}
      in         \colorbox{llight-blue}{B-A1}
      Augusta    \colorbox{llight-blue}{I-A1}
      .          \colorbox{white}{O}
    \end{BVerbatim}
    \caption{Normal SRLs.}
    \label{srl:normal}
  \end{srl}
\end{minipage}

In my case, i.e. the attempt to improve the performance of BERT regarding NLU tasks, the
following question would need some ablation experiments to be answered: What part of the SRLs
is most responsible for the performance boost? To be able to formulate this in a matter which
can be experimentally tested, I identify two easily separatable and testable aspects of SRLs,
take for example the sentence depicted in figure \ref{srl:normal}: Firstly, the information
what parts of a sentence are the predicates. The intuition behind this is that maybe the
head relies mostly on the information as to which tokens carry information about the events
that happen in a given sentence. To test this, I drop the information about all SRLs,
except the information that a token is a predicate (left side of figure \ref{srl:ablation}).
In the second case, the hypothesis is reversed: Maybe the head is able to get the most useful
hints about the information which inidcates what role certain token groups play in a given
sentence. To test for this all information about predicates is dropped and only information
about arguments is preserved (right side of figure \ref{srl:ablation}).

\begingroup
\begin{srl}[!h]
\centering
  \begin{minipage}{0.45\linewidth}
  \vspace{0pt}
    \begin{BVerbatim}[commandchars=\\\{\}, fontsize=\footnotesize]
    Ich        \colorbox{white}{O}           \colorbox{white}{O}
    weiÃŸ       \colorbox{blue}{B-V}         \colorbox{white}{O}
    nicht      \colorbox{white}{O}           \colorbox{white}{O}
    ob         \colorbox{white}{O}           \colorbox{white}{O}
    er         \colorbox{white}{O}           \colorbox{white}{O}
    danach     \colorbox{white}{O}           \colorbox{white}{O}
    in         \colorbox{white}{O}           \colorbox{white}{O}
    Augusta    \colorbox{white}{O}           \colorbox{white}{O}
    geblieben  \colorbox{white}{O}           \colorbox{blue}{B-V}
    ist        \colorbox{white}{O}           \colorbox{white}{O}
    .          \colorbox{white}{O}           \colorbox{white}{O}
    ==============================
    Er         \colorbox{white}{O}
    wohnte     \colorbox{blue}{B-V}
    weiterhin  \colorbox{white}{O}
    in         \colorbox{white}{O}
    Augusta    \colorbox{white}{O}
    .          \colorbox{white}{O}
    \end{BVerbatim}
  \end{minipage}
  \hfill
  \begin{minipage}{0.45\linewidth}
  \vspace{0pt}
    \begin{BVerbatim}[commandchars=\\\{\}, fontsize=\footnotesize]
      Ich        \colorbox{llight-blue}{B-A0}         \colorbox{white}{O}
      weiÃŸ       \colorbox{white}{O}            \colorbox{white}{O}
      nicht      \colorbox{white}{O}            \colorbox{white}{O}
      ob         \colorbox{llight-blue}{B-A1}         \colorbox{white}{O}
      er         \colorbox{llight-blue}{I-A1}         \colorbox{llight-blue}{B-A0}
      danach     \colorbox{llight-blue}{I-A1}         \colorbox{white}{O}
      in         \colorbox{llight-blue}{I-A1}         \colorbox{llight-blue}{B-A1}
      Augusta    \colorbox{llight-blue}{I-A1}         \colorbox{llight-blue}{I-A1}
      geblieben  \colorbox{llight-blue}{I-A1}         \colorbox{white}{O}
      ist        \colorbox{llight-blue}{I-A1}         \colorbox{white}{O}
      .          \colorbox{white}{O}            \colorbox{white}{O}
      ==============================
      Er         \colorbox{llight-blue}{B-A0}
      wohnte     \colorbox{white}{O}
      weiterhin  \colorbox{white}{O}
      in         \colorbox{llight-blue}{B-A1}
      Augusta    \colorbox{llight-blue}{I-A1}
      .          \colorbox{white}{O}
    \end{BVerbatim}
  \end{minipage}
\end{srl}
\label{srl:ablation}
\captionof{srl}{\textbf{Left}: Only predicateÂ SRLs. \textbf{Right}:  Only argument SRLs.}
\endgroup

To see the influence of these SRL ghosting techniques, for each classification dataset, I take a
configuration from table \ref{tab:results} where +SRL showed positive influence and recomputed 5
models for each configuration (preserving only predicate information vs. only argument information).
In table \ref{tab;srl-abl} I report the results from this ablation expermints.


\tab{tab:srl-abla}{Ablation on effect of PREDs and ARGs information isolated. Note that mostly opnly the combination of both information structures leads to a significant improvement over vanilly BERT embeddings ($-$SRL)}{
  \scalebox{0.9}{
    \begin{tabular}{llcccc}
                     &                               & \multicolumn{1}{c|}{$-$SRL}                  & \multicolumn{3}{c}{+SRL}                                                                                                 \\
                     &                               & \multicolumn{1}{c|}{}                        & \multicolumn{1}{c}{only PREDs}         & \multicolumn{1}{c}{only ARGs}           & \multicolumn{1}{c}{normal}            \\ \cline{3-6}
    deISEAR $\alpha$ & FFNN Head subtok. zeros       & \multicolumn{1}{c|}{\textit{70.86}}          & \multicolumn{1}{c}{72.19}              & \multicolumn{1}{c}{\underline{75.50**}} & \multicolumn{1}{c}{\textbf{77.48**}}  \\
    SCARE $\alpha$   & [CLS] Head merged duplicate   & \multicolumn{1}{c|}{\textit{83.33}}          & \multicolumn{1}{c}{84.47}              & \multicolumn{1}{c}{\underline{85.23}}   & \multicolumn{1}{c}{\textbf{85.61*}}   \\
    PAWS-X $\beta$   & [CLS] Head merged duplicate   & \multicolumn{1}{c|}{\textit{79.92}}          & \multicolumn{1}{c}{80.53}              & \multicolumn{1}{c}{\underline{80.68}}   & \multicolumn{1}{c}{\textbf{82.51***}} \\
    XNLI $\beta$     & GRU Head subtok. zeros        & \multicolumn{1}{c|}{\textit{66.84}}          & \multicolumn{1}{c}{67.02}              & \multicolumn{1}{c}{\textbf{68.00}}      & \multicolumn{1}{c}{\underline{67.82}} \\
    \end{tabular}
  }
}{Ablation Study}


As can be seen, SRL information only displays its full positive effect when both information parts are combined: Providing
the model only with predicate information already leads to a slight, but insignificant improvement in all experiments.
By telling the model exclusively about where arguments are located in sentences leads to a greater advancement, but
mostly also insignificant. For three out of four datasets, informing the model about both SRL information structures
in combination leads to a further significant increase of accuracy.

Apparently, the crucial aspect of information the GliBERT heads improve from having knowledge
about semantic roles lies in its relational character: It is not so much an ``annotation''
of certain parts of sentences --- as for e.g. named entities ---, but the indication of the
relationship in which those sequences stand to each other.


